#!/usr/bin/env python3
"""
çŸ¥è¯†åº“å…¥åº“è„šæœ¬
Textbook Knowledge Ingestion Script for Homework Pal RAG System

ç”¨äºå¤„ç†äººæ•™ç‰ˆæ•™ææ–‡æ¡£ï¼Œç”Ÿæˆå‘é‡åµŒå…¥å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
"""

import sys
import os
from pathlib import Path
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from homeworkpal.database.connection import engine, get_db
from homeworkpal.database.models import TextbookChunk
from homeworkpal.llm.siliconflow import create_siliconflow_client
from sqlalchemy.orm import sessionmaker

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def load_textbook_documents(data_dir: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½æ•™ææ–‡æ¡£

    Args:
        data_dir: æ•™ææ–‡æ¡£ç›®å½•è·¯å¾„

    Returns:
        æ–‡æ¡£åˆ—è¡¨
    """
    print_status(f"åŠ è½½æ•™ææ–‡æ¡£: {data_dir}", "ğŸ“š")

    documents = []
    data_path = Path(data_dir)

    if not data_path.exists():
        print_status(f"æ•™æç›®å½•ä¸å­˜åœ¨: {data_dir}", "âŒ")
        return documents

    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    supported_extensions = ['.md', '.txt']

    for file_path in data_path.rglob('*'):
        if file_path.suffix.lower() in supported_extensions:
            try:
                # ä½¿ç”¨LangChainåŠ è½½æ–‡æ¡£
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()

                for doc in docs:
                    documents.append({
                        'content': doc.page_content,
                        'source': str(file_path),
                        'file_name': file_path.name,
                        'file_type': file_path.suffix.lower()
                    })

                print(f"  âœ… å·²åŠ è½½: {file_path.name}")

            except Exception as e:
                print(f"  âŒ åŠ è½½å¤±è´¥ {file_path.name}: {e}")

    print_status(f"å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£", "ğŸ“š")
    return documents


def split_documents(documents: List[Dict[str, Any]],
                   chunk_size: int = 1000,
                   chunk_overlap: int = 200) -> List[Dict[str, Any]]:
    """
    åˆ†å‰²æ–‡æ¡£ä¸ºå°ç‰‡æ®µ

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        chunk_size: ç‰‡æ®µå¤§å°
        chunk_overlap: ç‰‡æ®µé‡å å¤§å°

    Returns:
        åˆ†å‰²åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"åˆ†å‰²æ–‡æ¡£ (ç‰‡æ®µå¤§å°: {chunk_size}, é‡å : {chunk_overlap})", "ğŸ”")

    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
    )

    chunks = []

    for doc in documents:
        try:
            # åˆ†å‰²æ–‡æ¡£
            split_docs = text_splitter.create_documents([doc['content']])

            for i, chunk in enumerate(split_docs):
                # æå–å…ƒæ•°æ®
                metadata = extract_metadata(chunk.page_content, doc)

                chunks.append({
                    'content': chunk.page_content,
                    'metadata': metadata,
                    'source_file': doc['source'],
                    'file_name': doc['file_name'],
                    'chunk_index': i,
                    'total_chunks': len(split_docs)
                })

            print(f"  âœ… {doc['file_name']}: {len(split_docs)} ä¸ªç‰‡æ®µ")

        except Exception as e:
            print(f"  âŒ åˆ†å‰²å¤±è´¥ {doc['file_name']}: {e}")

    print_status(f"å…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ", "ğŸ”")
    return chunks


def extract_metadata(content: str, doc: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»æ–‡æ¡£å†…å®¹ä¸­æå–å…ƒæ•°æ®

    Args:
        content: æ–‡æ¡£å†…å®¹
        doc: æ–‡æ¡£ä¿¡æ¯

    Returns:
        å…ƒæ•°æ®å­—å…¸
    """
    metadata = {
        'file_name': doc['file_name'],
        'file_type': doc['file_type'],
        'source': doc['source'],
        'subject': 'æ•°å­¦',  # é»˜è®¤å­¦ç§‘
        'grade': 'ä¸‰å¹´çº§',  # é»˜è®¤å¹´çº§
        'processed_date': datetime.now().isoformat(),
        'content_length': len(content),
        'content_hash': hashlib.md5(content.encode('utf-8')).hexdigest()
    }

    # ä»æ–‡ä»¶åæ¨æ–­ä¿¡æ¯
    file_name = doc['file_name'].lower()

    if 'æ•°å­¦' in file_name or 'math' in file_name:
        metadata['subject'] = 'æ•°å­¦'
    elif 'è¯­æ–‡' in file_name or 'chinese' in file_name:
        metadata['subject'] = 'è¯­æ–‡'
    elif 'è‹±è¯­' in file_name or 'english' in file_name:
        metadata['subject'] = 'è‹±è¯­'

    if 'ä¸‰å¹´çº§' in file_name or 'grade3' in file_name or '3' in file_name:
        metadata['grade'] = 'ä¸‰å¹´çº§'
    elif 'äºŒå¹´çº§' in file_name or 'grade2' in file_name or '2' in file_name:
        metadata['grade'] = 'äºŒå¹´çº§'
    elif 'å››å¹´çº§' in file_name or 'grade4' in file_name or '4' in file_name:
        metadata['grade'] = 'å››å¹´çº§'

    # ä»å†…å®¹ä¸­æå–å•å…ƒä¿¡æ¯
    content_lower = content.lower()
    if 'ç¬¬' in content and 'å•å…ƒ' in content:
        import re
        unit_pattern = r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+å•å…ƒ'
        matches = re.findall(unit_pattern, content)
        if matches:
            metadata['unit'] = matches[0]

    # ä»å†…å®¹ä¸­æå–ä¸»é¢˜ä¿¡æ¯
    themes = []
    theme_keywords = {
        'åŠ æ³•': ['åŠ æ³•', 'æ±‚å’Œ', 'ç›¸åŠ '],
        'å‡æ³•': ['å‡æ³•', 'æ±‚å·®', 'ç›¸å‡'],
        'ä¹˜æ³•': ['ä¹˜æ³•', 'æ±‚ç§¯', 'ç›¸ä¹˜'],
        'é™¤æ³•': ['é™¤æ³•', 'æ±‚å•†', 'ç›¸é™¤'],
        'æ—¶é—´': ['æ—¶é—´', 'å°æ—¶', 'åˆ†é’Ÿ', 'ç§’'],
        'è´¨é‡': ['åƒå…‹', 'å…‹', 'é‡é‡', 'è´¨é‡'],
        'é•¿åº¦': ['ç±³', 'å˜ç±³', 'æ¯«ç±³', 'é•¿åº¦'],
        'å‡ ä½•': 'å›¾å½¢ æ­£æ–¹å½¢ é•¿æ–¹å½¢ åœ†å½¢ ä¸‰è§’å½¢'.split()
    }

    for theme, keywords in theme_keywords.items():
        for keyword in keywords:
            if keyword in content:
                themes.append(theme)
                break

    if themes:
        metadata['themes'] = list(set(themes))

    return metadata


def generate_embeddings(chunks: List[Dict[str, Any]],
                       embedding_client) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆæ–‡æ¡£ç‰‡æ®µçš„å‘é‡åµŒå…¥

    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        embedding_client: åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯

    Returns:
        åŒ…å«åµŒå…¥å‘é‡çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µçš„å‘é‡åµŒå…¥", "ğŸ”")

    # æå–æ–‡æœ¬å†…å®¹
    texts = [chunk['content'] for chunk in chunks]

    try:
        # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = embedding_client.embed_documents(texts)

        # éªŒè¯åµŒå…¥å‘é‡æ•°é‡å’Œç»´åº¦
        if len(embeddings) != len(chunks):
            raise ValueError(f"åµŒå…¥å‘é‡æ•°é‡({len(embeddings)})ä¸ç‰‡æ®µæ•°é‡({len(chunks)})ä¸åŒ¹é…")

        expected_dim = 1024  # BGE-M3çš„ç»´åº¦
        for i, embedding in enumerate(embeddings):
            if len(embedding) != expected_dim:
                print(f"  âš ï¸ ç‰‡æ®µ {i} å‘é‡ç»´åº¦ä¸æ­£ç¡®: {len(embedding)} (æœŸæœ›: {expected_dim})")

        # å°†åµŒå…¥å‘é‡æ·»åŠ åˆ°ç‰‡æ®µä¸­
        for i, chunk in enumerate(chunks):
            chunk['embedding'] = embeddings[i]

        print_status(f"æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡åµŒå…¥", "âœ…")
        return chunks

    except Exception as e:
        print_status(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}", "âŒ")
        return []


def save_to_database(chunks: List[Dict[str, Any]],
                     batch_size: int = 10) -> bool:
    """
    å°†æ–‡æ¡£ç‰‡æ®µä¿å­˜åˆ°æ•°æ®åº“

    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    print_status(f"ä¿å­˜ {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°æ•°æ®åº“", "ğŸ’¾")

    try:
        # åˆ›å»ºæ•°æ®åº“ä¼šè¯
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # æ‰¹é‡ä¿å­˜
        saved_count = 0
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]

            for chunk in batch:
                # åˆ›å»ºTextbookChunkå¯¹è±¡
                textbook_chunk = TextbookChunk(
                    content=chunk['content'],
                    embedding=chunk['embedding'],
                    metadata_json=chunk['metadata'],
                    source_file=chunk['source_file'],
                    chunk_index=chunk['chunk_index']
                )

                session.add(textbook_chunk)
                saved_count += 1

            # æäº¤æ‰¹æ¬¡
            session.commit()
            print(f"  âœ… å·²ä¿å­˜ {min(i + batch_size, len(chunks))}/{len(chunks)} ä¸ªç‰‡æ®µ")

        session.close()

        print_status(f"æˆåŠŸä¿å­˜ {saved_count} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°æ•°æ®åº“", "âœ…")
        return True

    except Exception as e:
        print_status(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}", "âŒ")
        if 'session' in locals():
            session.rollback()
            session.close()
        return False


def verify_ingestion():
    """
    éªŒè¯å…¥åº“ç»“æœ
    """
    print_status("éªŒè¯å…¥åº“ç»“æœ", "ğŸ”")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # æŸ¥è¯¢æ€»è®°å½•æ•°
        total_count = session.query(TextbookChunk).count()
        print(f"  ğŸ“Š æ•°æ®åº“ä¸­æ€»è®°å½•æ•°: {total_count}")

        if total_count > 0:
            # æŸ¥è¯¢ç¤ºä¾‹è®°å½•
            sample_chunk = session.query(TextbookChunk).first()
            print(f"  ğŸ“ ç¤ºä¾‹å†…å®¹é•¿åº¦: {len(sample_chunk.content)} å­—ç¬¦")
            print(f"  ğŸ”¢ å‘é‡ç»´åº¦: {len(sample_chunk.embedding) if sample_chunk.embedding else 0}")
            print(f"  ğŸ“„ æºæ–‡ä»¶: {sample_chunk.source_file}")
            print(f"  ğŸ“‹ å…ƒæ•°æ®: {json.dumps(sample_chunk.metadata_json, ensure_ascii=False, indent=2)}")

        session.close()
        print_status("å…¥åº“éªŒè¯å®Œæˆ", "âœ…")
        return True

    except Exception as e:
        print_status(f"éªŒè¯å¤±è´¥: {e}", "âŒ")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ä½œä¸šæ­å­ RAG ç³»ç»Ÿ - çŸ¥è¯†åº“å…¥åº“è„šæœ¬")
    print("=" * 60)
    print()

    # é…ç½®å‚æ•°
    data_dir = os.getenv("TEXTBOOK_DIR", "data/textbooks")
    chunk_size = int(os.getenv("CHUNK_SIZE", "1000"))
    chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "200"))

    print(f"ğŸ“‚ æ•™æç›®å½•: {data_dir}")
    print(f"ğŸ“ ç‰‡æ®µå¤§å°: {chunk_size}")
    print(f"ğŸ”„ ç‰‡æ®µé‡å : {chunk_overlap}")
    print()

    # æ­¥éª¤æ£€æŸ¥
    steps = [
        ("åŠ è½½æ•™ææ–‡æ¡£", lambda: load_textbook_documents(data_dir)),
        ("åˆ†å‰²æ–‡æ¡£ç‰‡æ®µ", lambda: split_documents(load_textbook_documents(data_dir), chunk_size, chunk_overlap)),
        ("åˆå§‹åŒ–åµŒå…¥æ¨¡å‹", lambda: initialize_embedding_model()),
        ("ç”Ÿæˆå‘é‡åµŒå…¥", lambda: generate_embeddings(
            split_documents(load_textbook_documents(data_dir), chunk_size, chunk_overlap),
            create_siliconflow_client()
        )),
        ("ä¿å­˜åˆ°æ•°æ®åº“", lambda: save_to_database(
            generate_embeddings(
                split_documents(load_textbook_documents(data_dir), chunk_size, chunk_overlap),
                create_siliconflow_client()
            )
        )),
        ("éªŒè¯å…¥åº“ç»“æœ", verify_ingestion)
    ]

    passed = 0
    total = len(steps)

    for name, step_func in steps:
        try:
            result = step_func()
            if result or result is None:  # Noneä¹Ÿè¡¨ç¤ºæˆåŠŸ
                passed += 1
                print(f"âœ… {name} - é€šè¿‡")
            else:
                print(f"âŒ {name} - å¤±è´¥")
                break
        except Exception as e:
            print(f"âŒ {name} - å¤±è´¥: {e}")
            break

    print("\n" + "=" * 60)
    print(f"ğŸ“Š å…¥åº“ç»“æœ: {passed}/{total} æ­¥éª¤å®Œæˆ")

    if passed == total:
        print("ğŸ‰ çŸ¥è¯†åº“å…¥åº“å®Œæˆ!")
        print("âœ… äººæ•™ç‰ˆæ•™æå·²æˆåŠŸå‘é‡åŒ–å¹¶å­˜å‚¨åˆ°æ•°æ®åº“")
        print("ğŸ” ç°åœ¨å¯ä»¥è¿›è¡Œè¯­ä¹‰æ£€ç´¢å’Œé—®ç­”")
        return 0
    else:
        print("âš ï¸ çŸ¥è¯†åº“å…¥åº“æœªå®Œæˆï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1


def initialize_embedding_model():
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
    try:
        client = create_siliconflow_client()

        # æµ‹è¯•è¿æ¥
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        embedding = client.embed_query(test_text)

        if len(embedding) != 1024:
            raise ValueError(f"å‘é‡ç»´åº¦é”™è¯¯: {len(embedding)} (æœŸæœ›: 1024)")

        print_status("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ", "âœ…")
        return True

    except Exception as e:
        print_status(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}", "âŒ")
        return False


if __name__ == "__main__":
    sys.exit(main())