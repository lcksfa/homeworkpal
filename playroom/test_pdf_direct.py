#!/usr/bin/env python3
"""
ç›´æ¥å¯¼å…¥PDFå¤„ç†æµ‹è¯•
Direct PDF Processing Test
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

# ç›´æ¥å¯¼å…¥æ¨¡å—
from homeworkpal.document.pdf_processor import create_pdf_processor
from simple_text_splitter import create_simple_splitter

def test_pdf_processing():
    """æµ‹è¯•PDFå¤„ç†åŠŸèƒ½"""
    print("ğŸ”§ æµ‹è¯•PDFå¤„ç†åŠŸèƒ½")
    print("=" * 40)

    # åˆ›å»ºå¤„ç†å™¨
    pdf_processor = create_pdf_processor()
    text_splitter = create_simple_splitter(chunk_size=1500, chunk_overlap=200)

    # æŸ¥æ‰¾PDFæ–‡ä»¶
    data_dir = Path("data/textbooks")
    if not data_dir.exists():
        print(f"âŒ æ•™æç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return False

    pdf_files = list(data_dir.glob("*.pdf"))
    print(f"ğŸ“„ å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

    if not pdf_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
        return False

    # å¤„ç†ç¬¬ä¸€ä¸ªPDFæ–‡ä»¶
    pdf_file = pdf_files[0]
    print(f"ğŸ”„ æ­£åœ¨å¤„ç†: {pdf_file.name}")
    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {pdf_file.stat().st_size / 1024 / 1024:.1f} MB")

    try:
        # æå–PDFå†…å®¹
        print("ğŸ“– å¼€å§‹æå–PDFå†…å®¹...")
        pdf_result = pdf_processor.extract_text_from_pdf(str(pdf_file))
        print(f"âœ… PDFæå–æˆåŠŸ")
        print(f"  - æ€»é¡µæ•°: {len(pdf_result.get('pages', []))}")
        print(f"  - å­¦ç§‘: {pdf_result['education_metadata'].get('subject', 'æœªè¯†åˆ«')}")
        print(f"  - å¹´çº§: {pdf_result['education_metadata'].get('grade', 'æœªè¯†åˆ«')}")
        print(f"  - å¤„ç†å™¨: {pdf_result.get('processor_type', 'unknown')}")

        # åˆ†å‰²å†…å®¹
        print("ğŸ”ª å¼€å§‹åˆ†å‰²æ–‡æœ¬...")
        chunks = text_splitter.split_pdf_content(pdf_result)
        print(f"âœ… æ–‡æ¡£åˆ†å‰²æˆåŠŸï¼Œç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if chunks:
            high_quality = [c for c in chunks if c['quality_score'] > 0.5]
            avg_length = sum(c['text_length'] for c in chunks) / len(chunks)
            print(f"  - é«˜è´¨é‡ç‰‡æ®µ: {len(high_quality)}")
            print(f"  - å¹³å‡ç‰‡æ®µé•¿åº¦: {avg_length:.1f}")

            # æ˜¾ç¤ºå‰3ä¸ªç‰‡æ®µçš„é¢„è§ˆ
            for i, chunk in enumerate(chunks[:3]):
                print(f"\n--- ç‰‡æ®µ {i+1} é¢„è§ˆ ---")
                print(f"ID: {chunk['id']}")
                print(f"é¡µé¢: {chunk['page_number']}")
                print(f"é•¿åº¦: {chunk['text_length']} å­—ç¬¦")
                print(f"è´¨é‡è¯„åˆ†: {chunk['quality_score']:.2f}")
                print(f"å†…å®¹ç±»å‹: {chunk['metadata']['content_type']}")
                preview = chunk['content'][:200] + '...' if len(chunk['content']) > 200 else chunk['content']
                print(f"å†…å®¹: {preview}")
        else:
            print("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•ç‰‡æ®µ")

        return True

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_pdf_processing()
    if success:
        print("\nğŸ‰ PDFå¤„ç†æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ PDFå¤„ç†æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)