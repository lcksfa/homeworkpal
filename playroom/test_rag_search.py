#!/usr/bin/env python3
"""
RAGæ£€ç´¢æµ‹è¯•
RAG Search Test

æµ‹è¯•å‘é‡æ£€ç´¢å’ŒçŸ¥è¯†åº“æŸ¥è¯¢åŠŸèƒ½
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

from homeworkpal.database.connection import engine
from sqlalchemy.orm import sessionmaker
from homeworkpal.database.models import TextbookChunk
import numpy as np
import json

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def test_database_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    print_status("æµ‹è¯•æ•°æ®åº“è¿æ¥", "ğŸ”§")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # ç®€å•æŸ¥è¯¢
        count = session.query(TextbookChunk).count()
        print(f"ğŸ“Š æ•°æ®åº“ä¸­çŸ¥è¯†ç‰‡æ®µæ•°é‡: {count}")

        session.close()
        print_status("æ•°æ®åº“è¿æ¥æˆåŠŸ", "âœ…")
        return True

    except Exception as e:
        print_status(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}", "âŒ")
        return False


def test_vector_similarity():
    """æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦è®¡ç®—"""
    print_status("æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦è®¡ç®—", "ğŸ”")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # è·å–æ‰€æœ‰çŸ¥è¯†ç‰‡æ®µ
        chunks = session.query(TextbookChunk).all()

        if len(chunks) < 2:
            print("âš ï¸ éœ€è¦è‡³å°‘2ä¸ªçŸ¥è¯†ç‰‡æ®µæ¥æµ‹è¯•ç›¸ä¼¼åº¦")
            session.close()
            return False

        print(f"ğŸ“Š æ‰¾åˆ° {len(chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

        # æ˜¾ç¤ºå‰å‡ ä¸ªç‰‡æ®µçš„ä¿¡æ¯
        for i, chunk in enumerate(chunks[:2]):
            print(f"\n--- çŸ¥è¯†ç‰‡æ®µ {i+1} ---")
            print(f"æ–‡ä»¶: {chunk.source_file}")
            print(f"å­¦ç§‘: {chunk.metadata_json.get('subject', 'æœªçŸ¥')}")
            print(f"å¹´çº§: {chunk.metadata_json.get('grade', 'æœªçŸ¥')}")
            print(f"å†…å®¹é•¿åº¦: {len(chunk.content)} å­—ç¬¦")
            print(f"å†…å®¹é¢„è§ˆ: {chunk.content[:100]}...")

            if chunk.embedding is not None:
                print(f"å‘é‡ç»´åº¦: {len(chunk.embedding)}")

        # è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        if chunks[0].embedding is not None and chunks[1].embedding is not None:
            vec1 = np.array(chunks[0].embedding)
            vec2 = np.array(chunks[1].embedding)

            # ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)

            if norm1 > 0 and norm2 > 0:
                cosine_similarity = dot_product / (norm1 * norm2)
                print(f"\nğŸ”— å‘é‡ç›¸ä¼¼åº¦: {cosine_similarity:.4f}")
            else:
                print(f"\nâš ï¸ æ— æ³•è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå‘é‡ä¸ºé›¶ï¼‰")

        session.close()
        print_status("å‘é‡ç›¸ä¼¼åº¦æµ‹è¯•å®Œæˆ", "âœ…")
        return True

    except Exception as e:
        print_status(f"å‘é‡ç›¸ä¼¼åº¦æµ‹è¯•å¤±è´¥: {e}", "âŒ")
        return False


def test_mock_query():
    """æµ‹è¯•æ¨¡æ‹ŸæŸ¥è¯¢"""
    print_status("æµ‹è¯•æ¨¡æ‹ŸæŸ¥è¯¢åŠŸèƒ½", "ğŸ”")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # æ¨¡æ‹ŸæŸ¥è¯¢ï¼š"ä¸‰å¹´çº§æ•°å­¦æ—¶é—´"
        query_text = "ä¸‰å¹´çº§æ•°å­¦æ—¶é—´"
        print(f"ğŸ” æ¨¡æ‹ŸæŸ¥è¯¢: {query_text}")

        # è·å–æ‰€æœ‰çŸ¥è¯†ç‰‡æ®µ
        chunks = session.query(TextbookChunk).all()

        if not chunks:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰çŸ¥è¯†ç‰‡æ®µ")
            session.close()
            return False

        # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰
        relevant_chunks = []
        for chunk in chunks:
            content_lower = chunk.content.lower()
            if any(keyword in content_lower for keyword in ['æ—¶é—´', 'æ•°å­¦', 'é’Ÿè¡¨']):
                relevant_chunks.append(chunk)

        print(f"ğŸ“Š æ‰¾åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")

        # æ˜¾ç¤ºç›¸å…³ç‰‡æ®µ
        for i, chunk in enumerate(relevant_chunks):
            print(f"\n--- ç›¸å…³ç‰‡æ®µ {i+1} ---")
            print(f"æ–‡ä»¶: {chunk.source_file}")
            print(f"å­¦ç§‘: {chunk.metadata_json.get('subject', 'æœªçŸ¥')}")
            print(f"å¹´çº§: {chunk.metadata_json.get('grade', 'æœªçŸ¥')}")
            print(f"è´¨é‡è¯„åˆ†: {chunk.quality_score}")
            print(f"å†…å®¹: {chunk.content}")

        session.close()
        print_status("æ¨¡æ‹ŸæŸ¥è¯¢æµ‹è¯•å®Œæˆ", "âœ…")
        return True

    except Exception as e:
        print_status(f"æ¨¡æ‹ŸæŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}", "âŒ")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ ä½œä¸šæ­å­ RAG ç³»ç»Ÿ - æ£€ç´¢åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    tests = [
        ("æ•°æ®åº“è¿æ¥", test_database_connection),
        ("å‘é‡ç›¸ä¼¼åº¦", test_vector_similarity),
        ("æ¨¡æ‹ŸæŸ¥è¯¢", test_mock_query)
    ]

    passed = 0
    total = len(tests)

    for name, test_func in tests:
        try:
            print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {name}")
            if test_func():
                print(f"âœ… {name} - é€šè¿‡")
                passed += 1
            else:
                print(f"âŒ {name} - å¤±è´¥")
        except Exception as e:
            print(f"âŒ {name} - å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("ğŸ‰ RAGæ£€ç´¢åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print("ğŸ’¡ çŸ¥è¯†åº“å·²ç»å‡†å¤‡å¥½è¿›è¡Œé—®ç­”æµ‹è¯•")
        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("  1. é…ç½®SiliconFlow APIä»¥è¿›è¡ŒçœŸå®å‘é‡åµŒå…¥")
        print("  2. å®ç°å®Œæ•´çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢")
        print("  3. é›†æˆLLMè¿›è¡Œé—®ç­”ç”Ÿæˆ")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é—®é¢˜")
        return 1


if __name__ == "__main__":
    sys.exit(main())