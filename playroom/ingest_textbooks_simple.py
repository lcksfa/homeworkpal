#!/usr/bin/env python3
"""
ç®€åŒ–çš„çŸ¥è¯†åº“å…¥åº“è„šæœ¬
Simplified Textbook Knowledge Ingestion Script

ä½¿ç”¨ç®€åŒ–çš„PDFå¤„ç†å’Œæ–‡æœ¬åˆ†æ®µåŠŸèƒ½
"""

import sys
import os
from pathlib import Path
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from homeworkpal.database.connection import engine, get_db
from homeworkpal.database.models import TextbookChunk
from sqlalchemy.orm import sessionmaker
from simple_text_splitter import create_simple_splitter

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def process_textbook_documents_simple(data_dir: str) -> List[Dict[str, Any]]:
    """
    ç®€åŒ–ç‰ˆçš„æ•™ææ–‡æ¡£å¤„ç†ï¼ˆæš‚æ—¶ä¸å¤„ç†çœŸå®PDFï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬ï¼‰

    Args:
        data_dir: æ•™ææ–‡æ¡£ç›®å½•è·¯å¾„

    Returns:
        å¤„ç†åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"å¤„ç†æ•™ææ–‡æ¡£ï¼ˆç®€åŒ–æ¨¡å¼ï¼‰: {data_dir}", "ğŸ“š")

    # ç¤ºä¾‹æ•™æå†…å®¹
    sample_documents = [
        {
            "file_name": "æ•°å­¦ 3 ä¸Š.pdf",
            "subject": "æ•°å­¦",
            "grade": "ä¸‰å¹´çº§",
            "content": """
            ä¸‰å¹´çº§æ•°å­¦ä¸Šå†Œç¬¬ä¸€å•å…ƒï¼šæ—¶ã€åˆ†ã€ç§’

            1. è®¤è¯†é’Ÿè¡¨

            é’Ÿè¡¨æ˜¯ç”¨æ¥è®¡æ—¶çš„å·¥å…·ã€‚æˆ‘ä»¬å¸¸è§çš„é’Ÿè¡¨æœ‰æ—¶é’ˆã€åˆ†é’ˆå’Œç§’é’ˆã€‚

            æ—¶é’ˆæœ€çŸ­ï¼Œèµ°å¾—æœ€æ…¢ï¼›åˆ†é’ˆæ¯”æ—¶é’ˆé•¿ï¼Œèµ°å¾—æ¯”æ—¶é’ˆå¿«ï¼›ç§’é’ˆæœ€é•¿ï¼Œèµ°å¾—æœ€å¿«ã€‚

            ä¾‹é¢˜1ï¼šçœ‹å›¾å¡«ç©º

            å›¾ä¸­é’Ÿè¡¨æ˜¾ç¤ºçš„æ—¶é—´æ˜¯3æ—¶15åˆ†ã€‚

            ç»ƒä¹ ï¼š
            1. è¯´å‡ºä¸‹é¢é’Ÿè¡¨æ˜¾ç¤ºçš„æ—¶é—´ï¼š
               (1) 7æ—¶30åˆ†
               (2) 12æ—¶45åˆ†
               (3) 9æ—¶æ•´

            2. å°æ˜æ—©ä¸Š7æ—¶èµ·åºŠï¼Œ8æ—¶åˆ°å­¦æ ¡ï¼Œä»–è·¯ä¸Šç”¨äº†å¤šé•¿æ—¶é—´ï¼Ÿ
            """
        },
        {
            "file_name": "è¯­æ–‡ä¸‰ä¸Š.pdf",
            "subject": "è¯­æ–‡",
            "grade": "ä¸‰å¹´çº§",
            "content": """
            ä¸‰å¹´çº§è¯­æ–‡ä¸Šå†Œç¬¬ä¸€å•å…ƒï¼šæˆ‘ä»¬çš„å­¦æ ¡

            1. æˆ‘ä»¬çš„å­¦æ ¡

            æˆ‘ä»¬çš„å­¦æ ¡å¾ˆç¾ä¸½ã€‚æ ¡å›­é‡Œæœ‰é«˜å¤§çš„æ•™å­¦æ¥¼ï¼Œå®½é˜”çš„æ“åœºï¼Œè¿˜æœ‰ç»¿æ²¹æ²¹çš„å°è‰ã€‚

            æ•™å­¦æ¥¼æœ‰äº”å±‚é«˜ï¼Œæ¯ä¸€å±‚éƒ½æœ‰æ˜äº®çš„æ•™å®¤ã€‚æ•™å®¤é‡Œæœ‰æ•´é½çš„è¯¾æ¡Œæ¤…ï¼Œå¹²å‡€çš„é»‘æ¿ã€‚

            æ“åœºä¸Šæœ‰ç¯®çƒæ¶ã€è¶³çƒé—¨ã€‚ä¸‹è¯¾çš„æ—¶å€™ï¼ŒåŒå­¦ä»¬éƒ½å–œæ¬¢åˆ°æ“åœºä¸Šç©ã€‚

            æ ¡å›­çš„å‘¨å›´ç§ç€è®¸å¤šæ ‘ã€‚æ˜¥å¤©ï¼Œæ ‘æœ¨å‘èŠ½ï¼›å¤å¤©ï¼Œæ ‘æœ¨é•¿å¾—éƒéƒè‘±è‘±ã€‚

            æˆ‘çˆ±æˆ‘ä»¬çš„å­¦æ ¡ï¼Œçˆ±è¿™é‡Œçš„ä¸€è‰ä¸€æœ¨ã€‚

            ç”Ÿå­—ï¼šæ ¡å›­ æ“åœº æ•™å­¦ æ•´é½ å¹²å‡€ å‘¨å›´ æ ‘æœ¨ å‘èŠ½
            """
        }
    ]

    all_chunks = []
    text_splitter = create_simple_splitter(chunk_size=1500, chunk_overlap=200)

    for doc in sample_documents:
        print(f"  ğŸ”„ æ­£åœ¨å¤„ç†: {doc['file_name']}")

        # åˆ†å‰²æ–‡æœ¬
        chunks = text_splitter.split_text(doc['content'])

        for i, chunk_text in enumerate(chunks):
            chunk_text = chunk_text.strip()
            if not chunk_text:
                continue

            chunk_id = f"{doc['file_name']}_chunk_{i+1}"

            chunk = {
                'content': chunk_text,
                'source': f"data/textbooks/{doc['file_name']}",
                'file_name': doc['file_name'],
                'file_type': 'pdf',
                'chunk_id': chunk_id,
                'page_number': 1,  # ç¤ºä¾‹æ–‡æ¡£
                'chunk_index': i,
                'quality_score': 1.0,  # ç¤ºä¾‹è´¨é‡åˆ†æ•°
                'metadata': {
                    'pdf_file': doc['file_name'],
                    'subject': doc['subject'],
                    'grade': doc['grade'],
                    'page_number': 1,
                    'total_pages': 1,
                    'processed_date': datetime.now().isoformat(),
                    'content_type': 'æ­£æ–‡å†…å®¹',
                    'has_images': False
                }
            }

            all_chunks.append(chunk)

        print(f"  âœ… {doc['file_name']}: ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")

    print_status(f"å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ", "ğŸ“š")
    return all_chunks


def generate_mock_embeddings(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„å‘é‡åµŒå…¥ï¼ˆç”¨äºæµ‹è¯•ï¼‰

    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨

    Returns:
        åŒ…å«æ¨¡æ‹ŸåµŒå…¥å‘é‡çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µçš„æ¨¡æ‹Ÿå‘é‡åµŒå…¥", "ğŸ”")

    import random

    # BGE-M3çš„å‘é‡ç»´åº¦æ˜¯1024
    dimension = 1024

    for i, chunk in enumerate(chunks):
        # ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹ï¼‰
        embedding = [random.uniform(-1, 1) for _ in range(dimension)]
        chunk['embedding'] = embedding
        chunk['content_hash'] = hashlib.md5(chunk['content'].encode('utf-8')).hexdigest()

    print_status(f"æˆåŠŸç”Ÿæˆ {len(chunks)} ä¸ªæ¨¡æ‹Ÿå‘é‡åµŒå…¥", "âœ…")
    return chunks


def save_to_database(chunks: List[Dict[str, Any]],
                     batch_size: int = 10) -> bool:
    """
    å°†æ–‡æ¡£ç‰‡æ®µä¿å­˜åˆ°æ•°æ®åº“

    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    print_status(f"ä¿å­˜ {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°æ•°æ®åº“", "ğŸ’¾")

    try:
        # åˆ›å»ºæ•°æ®åº“ä¼šè¯
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # å»é‡ï¼šæ£€æŸ¥å†…å®¹å“ˆå¸Œæ˜¯å¦å·²å­˜åœ¨
        existing_hashes = set()
        existing_records = session.query(TextbookChunk.content_hash).filter(
            TextbookChunk.content_hash.in_([chunk.get('content_hash', '') for chunk in chunks])
        ).all()
        existing_hashes = set(record[0] for record in existing_records)

        # è¿‡æ»¤æ–°ç‰‡æ®µ
        new_chunks = [chunk for chunk in chunks if chunk.get('content_hash', '') not in existing_hashes]

        if not new_chunks:
            print_status("æ‰€æœ‰ç‰‡æ®µéƒ½å·²å­˜åœ¨äºæ•°æ®åº“ä¸­", "âœ…")
            session.close()
            return True

        print(f"  ğŸ“Š è¿‡æ»¤é‡å¤åæ–°å¢ {len(new_chunks)} ä¸ªç‰‡æ®µ")

        # æ‰¹é‡ä¿å­˜
        saved_count = 0
        for i in range(0, len(new_chunks), batch_size):
            batch = new_chunks[i:i + batch_size]

            for chunk in batch:
                # åˆ›å»ºTextbookChunkå¯¹è±¡
                textbook_chunk = TextbookChunk(
                    content=chunk['content'],
                    embedding=chunk['embedding'],
                    metadata_json=chunk['metadata'],
                    source_file=chunk['source'],
                    chunk_index=chunk['chunk_index'],
                    content_hash=chunk.get('content_hash', hashlib.md5(chunk['content'].encode('utf-8')).hexdigest()),
                    page_number=chunk.get('page_number'),
                    quality_score=chunk.get('quality_score', 1.0)
                )

                session.add(textbook_chunk)
                saved_count += 1

            # æäº¤æ‰¹æ¬¡
            session.commit()
            print(f"  âœ… å·²ä¿å­˜ {min(i + batch_size, len(new_chunks))}/{len(new_chunks)} ä¸ªç‰‡æ®µ")

        session.close()

        print_status(f"æˆåŠŸä¿å­˜ {saved_count} ä¸ªæ–°æ–‡æ¡£ç‰‡æ®µåˆ°æ•°æ®åº“", "âœ…")
        return True

    except Exception as e:
        print_status(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}", "âŒ")
        if 'session' in locals():
            session.rollback()
            session.close()
        return False


def verify_ingestion():
    """
    éªŒè¯å…¥åº“ç»“æœ
    """
    print_status("éªŒè¯å…¥åº“ç»“æœ", "ğŸ”")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # æŸ¥è¯¢æ€»è®°å½•æ•°
        total_count = session.query(TextbookChunk).count()
        print(f"  ğŸ“Š æ•°æ®åº“ä¸­æ€»è®°å½•æ•°: {total_count}")

        if total_count > 0:
            # æŸ¥è¯¢ç¤ºä¾‹è®°å½•
            sample_chunk = session.query(TextbookChunk).first()
            print(f"  ğŸ“ ç¤ºä¾‹å†…å®¹é•¿åº¦: {len(sample_chunk.content)} å­—ç¬¦")
            embedding_dim = len(sample_chunk.embedding) if sample_chunk.embedding is not None else 0
            print(f"  ğŸ”¢ å‘é‡ç»´åº¦: {embedding_dim}")
            print(f"  ğŸ“„ æºæ–‡ä»¶: {sample_chunk.source_file}")
            print(f"  ğŸ“‹ å…ƒæ•°æ®: {json.dumps(sample_chunk.metadata_json, ensure_ascii=False, indent=2)}")

        session.close()
        print_status("å…¥åº“éªŒè¯å®Œæˆ", "âœ…")
        return True

    except Exception as e:
        print_status(f"éªŒè¯å¤±è´¥: {e}", "âŒ")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ä½œä¸šæ­å­ RAG ç³»ç»Ÿ - ç®€åŒ–çŸ¥è¯†åº“å…¥åº“è„šæœ¬")
    print("=" * 60)
    print()

    # é…ç½®å‚æ•°
    data_dir = os.getenv("TEXTBOOK_DIR", "data/textbooks")

    print(f"ğŸ“‚ æ•™æç›®å½•: {data_dir}")
    print(f"ğŸ”§ æ¨¡å¼: ç®€åŒ–æµ‹è¯•æ¨¡å¼ï¼ˆä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬ï¼‰")
    print()

    try:
        # æ­¥éª¤1: å¤„ç†æ•™ææ–‡æ¡£
        chunks = process_textbook_documents_simple(data_dir)
        if not chunks:
            print("âŒ å¤„ç†æ•™ææ–‡æ¡£ - å¤±è´¥")
            return 1
        print("âœ… å¤„ç†æ•™ææ–‡æ¡£ - é€šè¿‡")

        # æ­¥éª¤2: ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡åµŒå…¥
        embedded_chunks = generate_mock_embeddings(chunks)
        if not embedded_chunks:
            print("âŒ ç”Ÿæˆå‘é‡åµŒå…¥ - å¤±è´¥")
            return 1
        print("âœ… ç”Ÿæˆå‘é‡åµŒå…¥ - é€šè¿‡")

        # æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®åº“
        if not save_to_database(embedded_chunks):
            print("âŒ ä¿å­˜åˆ°æ•°æ®åº“ - å¤±è´¥")
            return 1
        print("âœ… ä¿å­˜åˆ°æ•°æ®åº“ - é€šè¿‡")

        # æ­¥éª¤4: éªŒè¯å…¥åº“ç»“æœ
        if not verify_ingestion():
            print("âŒ éªŒè¯å…¥åº“ç»“æœ - å¤±è´¥")
            return 1
        print("âœ… éªŒè¯å…¥åº“ç»“æœ - é€šè¿‡")

        passed = 4
        total = 4

    except Exception as e:
        print(f"âŒ å¤„ç†æµç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

    print("\n" + "=" * 60)
    print(f"ğŸ“Š å…¥åº“ç»“æœ: {passed}/{total} æ­¥éª¤å®Œæˆ")

    if passed == total:
        print("ğŸ‰ çŸ¥è¯†åº“å…¥åº“å®Œæˆ!")
        print("âœ… ç¤ºä¾‹æ•™æå†…å®¹å·²æˆåŠŸå‘é‡åŒ–å¹¶å­˜å‚¨åˆ°æ•°æ®åº“")
        print("ğŸ” ç°åœ¨å¯ä»¥è¿›è¡Œè¯­ä¹‰æ£€ç´¢å’Œé—®ç­”æµ‹è¯•")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. å®‰è£…PyMuPDFä»¥å¤„ç†çœŸå®PDFæ–‡ä»¶")
        print("  2. é…ç½®SiliconFlow APIä»¥ä½¿ç”¨çœŸå®å‘é‡åµŒå…¥")
        print("  3. æµ‹è¯•æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½")
        return 0
    else:
        print("âš ï¸ çŸ¥è¯†åº“å…¥åº“æœªå®Œæˆï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1


if __name__ == "__main__":
    sys.exit(main())