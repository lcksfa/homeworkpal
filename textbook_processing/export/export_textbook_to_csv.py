#!/usr/bin/env python3
"""
PDFæ•™ææ•°æ®å¯¼å‡ºåˆ°CSVè„šæœ¬
Export Textbook PDF Data to CSV Script

å°†äººæ•™ç‰ˆè¯­æ–‡æ•™æçš„PDFå†…å®¹æŒ‰ç»“æ„åŒ–æ–¹å¼å¯¼å‡ºåˆ°CSVæ–‡ä»¶
"""

import sys
import os
from pathlib import Path
import pandas as pd
import re
from typing import List, Dict, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from homeworkpal.document import create_pdf_processor, create_pdf_splitter
from homeworkpal.document.chinese_text_processor import ChineseTextProcessor
from homeworkpal.document.chinese_textbook_analyzer import ChineseTextbookAnalyzer
from homeworkpal.llm.siliconflow import SiliconFlowEmbeddingModel


def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾", "ğŸš€": "ğŸš€", "ğŸ“–": "ğŸ“–"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def extract_textbook_content_to_csv(pdf_path: str, output_csv: str = "textbook_content.csv"):
    """
    å°†PDFæ•™æå†…å®¹æå–åˆ°CSVæ–‡ä»¶

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        output_csv: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
    """
    print_status(f"å¼€å§‹å¤„ç†æ•™æPDF: {pdf_path}", "ğŸ“š")

    # 1. æå–PDFåŸºæœ¬å†…å®¹
    pdf_processor = create_pdf_processor()
    pdf_result = pdf_processor.extract_text_from_pdf(pdf_path)

    print(f"ğŸ“„ PDFä¿¡æ¯:")
    print(f"  æ€»é¡µæ•°: {pdf_result.get('total_pages', 0)}")
    print(f"  æ–‡ä»¶å¤§å°: {pdf_result.get('file_size', 0)} bytes")

    # 2. æŒ‰é¡µæå–æ–‡æœ¬å†…å®¹
    pages_content = []
    for i, page in enumerate(pdf_result.get('pages', []), 1):
        page_text = page.get('text', '').strip()
        if page_text:
            pages_content.append({
                'page_number': i,
                'content': page_text,
                'content_length': len(page_text)
            })

    print(f"ğŸ“ æå–äº† {len(pages_content)} é¡µæœ‰æ•ˆå†…å®¹")

    # 3. åˆ›å»ºæ–‡æœ¬åˆ†æ®µå™¨
    text_splitter = create_pdf_splitter(chunk_size=1500, chunk_overlap=200)
    chunks = text_splitter.split_pdf_content(pdf_result)

    # 4. åº”ç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†ï¼ˆä¸è¿›è¡Œå‘é‡åŒ–ï¼‰
    print_status("åº”ç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†å’Œè´¨é‡è¯„ä¼°", "ğŸ”§")

    # ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬çš„å¤„ç†å™¨ï¼ˆä¸éœ€è¦APIå¯†é’¥ï¼‰
    processed_chunks = []
    for chunk in chunks:
        content = chunk['content']

        # åŸºç¡€æ–‡æœ¬æ¸…ç†
        content = re.sub(r'\s+', ' ', content)  # åˆå¹¶ç©ºç™½
        content = re.sub(r'\n+', ' ', content)  # åˆå¹¶æ¢è¡Œç¬¦

        # ç§»é™¤æ˜æ˜¾çš„å™ªéŸ³
        content = re.sub(r'[^\u4e00-\u9fff\u3000-\u303f\uff00-\uffef\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]', '', content)
        content = content.strip()

        if len(content) > 10:  # åªä¿ç•™æœ‰æ„ä¹‰çš„å†…å®¹
            processed_chunks.append({
                'chunk_id': chunk['id'],
                'page_number': chunk.get('page_number', 0),
                'chunk_index': chunk.get('chunk_index', 0),
                'content': content,
                'content_length': len(content),
                'source_file': pdf_path
            })

    print(f"ğŸ” å¤„ç†åå¾—åˆ° {len(processed_chunks)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")

    # 5. ä½¿ç”¨æ™ºèƒ½åˆ†æå™¨è¯†åˆ«è¯¾æ–‡ç»“æ„
    print_status("å¼€å§‹æ™ºèƒ½è¯¾æ–‡ç»“æ„åˆ†æ", "ğŸ§ ")

    # åˆ›å»ºä¸´æ—¶çš„åµŒå…¥æ¨¡å‹ç”¨äºæ–‡æœ¬å¤„ç†å™¨ï¼ˆä¸è°ƒç”¨APIï¼‰
    class DummyEmbeddingModel:
        def embed_query(self, text):
            return [0.0] * 1024  # è¿”å›å‡å‘é‡

    dummy_embedding = DummyEmbeddingModel()
    chinese_processor = ChineseTextProcessor(dummy_embedding)

    # æ›´æ–°å¤„ç†å™¨ä»¥ä¸è°ƒç”¨API
    def simple_preprocess(text):
        if not text:
            return text
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'[^\u4e00-\u9fff\u3000-\u303f\uff00-\uffef\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]', '', text)
        text = text.strip()
        return text

    # åˆ†æå™¨ä¹Ÿéœ€è¦ä¿®æ”¹ä»¥é¿å…APIè°ƒç”¨
    analyzer = ChineseTextbookAnalyzer()

    try:
        # ä½¿ç”¨åˆ†æå™¨è¿›è¡Œç»“æ„åˆ†æ
        structure = analyzer.analyze_textbook_structure(processed_chunks)

        print(f"ğŸ“Š åˆ†æç»“æœ:")
        print(f"  å¹´çº§: {structure.grade}")
        print(f"  ç§‘ç›®: {structure.subject}")
        print(f"  å•å…ƒæ•°: {len(set(lesson.unit_number for lesson in structure.units))}")
        print(f"  è¯¾æ–‡æ•°: {structure.total_lessons}")

        # 6. å°†ç‰‡æ®µä¸è¯¾æ–‡ç»“æ„å…³è”
        chunk_with_structure = []
        for chunk in processed_chunks:
            # æŸ¥æ‰¾å¯¹åº”çš„è¯¾æ–‡
            matched_lesson = None
            for lesson in structure.units:
                if (lesson.start_page <= chunk['page_number'] <= (lesson.end_page or 999)):
                    matched_lesson = lesson
                    break

            chunk_info = chunk.copy()
            if matched_lesson:
                chunk_info.update({
                    'unit_number': matched_lesson.unit_number,
                    'unit_title': matched_lesson.unit_title,
                    'lesson_number': matched_lesson.lesson_number,
                    'lesson_title': matched_lesson.lesson_title,
                    'lesson_start_page': matched_lesson.start_page,
                    'lesson_end_page': matched_lesson.end_page
                })
            else:
                chunk_info.update({
                    'unit_number': None,
                    'unit_title': None,
                    'lesson_number': None,
                    'lesson_title': None,
                    'lesson_start_page': None,
                    'lesson_end_page': None
                })

            # æ·»åŠ æ–‡æœ¬è´¨é‡è¯„ä¼°
            chunk_info['text_quality'] = assess_text_quality(chunk_info['content'])

            chunk_with_structure.append(chunk_info)

    except Exception as e:
        print_status(f"ç»“æ„åˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ•°æ®: {e}", "âš ï¸")
        chunk_with_structure = []
        for chunk in processed_chunks:
            chunk_info = chunk.copy()
            chunk_info.update({
                'unit_number': None,
                'unit_title': None,
                'lesson_number': None,
                'lesson_title': None,
                'lesson_start_page': None,
                'lesson_end_page': None,
                'text_quality': assess_text_quality(chunk_info['content'])
            })
            chunk_with_structure.append(chunk_info)

    # 7. è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
    print_status("åˆ›å»ºCSVæ•°æ®æ–‡ä»¶", "ğŸ’¾")

    df = pd.DataFrame(chunk_with_structure)

    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    column_order = [
        'chunk_id', 'page_number', 'chunk_index', 'unit_number', 'unit_title',
        'lesson_number', 'lesson_title', 'lesson_start_page', 'lesson_end_page',
        'content_length', 'text_quality', 'content', 'source_file'
    ]

    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
    for col in column_order:
        if col not in df.columns:
            df[col] = None

    df = df[column_order]

    # ä¿å­˜åˆ°CSV
    df.to_csv(output_csv, index=False, encoding='utf-8-sig')

    print_status(f"æˆåŠŸå¯¼å‡ºåˆ°CSV: {output_csv}", "âœ…")
    print(f"ğŸ“Š CSVç»Ÿè®¡:")
    print(f"  æ€»è¡Œæ•°: {len(df)}")
    print(f"  æœ‰è¯¾æ–‡ä¿¡æ¯çš„: {df['lesson_title'].notna().sum()}")
    print(f"  å¹³å‡å†…å®¹é•¿åº¦: {df['content_length'].mean():.1f}")

    # 8. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_summary_report(df, output_csv.replace('.csv', '_summary.txt'))

    return df


def assess_text_quality(text: str) -> Dict[str, Any]:
    """
    è¯„ä¼°æ–‡æœ¬è´¨é‡ï¼ˆä¸è°ƒç”¨APIçš„ç®€åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        text: æ–‡æœ¬å†…å®¹

    Returns:
        è´¨é‡è¯„ä¼°ç»“æœ
    """
    if not text:
        return {'score': 0.0, 'is_suitable': False, 'reason': 'æ–‡æœ¬ä¸ºç©º'}

    score = 0.5  # åŸºç¡€åˆ†æ•°
    reasons = []

    # é•¿åº¦è¯„åˆ†
    length = len(text)
    if 50 <= length <= 500:
        score += 0.3
        reasons.append('é•¿åº¦é€‚ä¸­')
    elif length < 20:
        score -= 0.3
        reasons.append('æ–‡æœ¬è¿‡çŸ­')
    elif length > 1000:
        score -= 0.1
        reasons.append('æ–‡æœ¬è¾ƒé•¿')

    # ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    if length > 0:
        chinese_ratio = chinese_chars / length
        score += chinese_ratio * 0.2
        if chinese_ratio > 0.7:
            reasons.append('ä¸­æ–‡ä¸ºä¸»')

    # æ•™è‚²å…³é”®è¯
    edu_keywords = ['è¯¾æ–‡', 'ç”Ÿå­—', 'è¯è¯­', 'ç»ƒä¹ ', 'é˜…è¯»', 'å­¦ä¹ ', 'ç†è§£']
    keyword_count = sum(1 for keyword in edu_keywords if keyword in text)
    if keyword_count > 0:
        score += min(keyword_count * 0.05, 0.2)
        reasons.append('æ•™è‚²ç›¸å…³')

    score = max(0.0, min(1.0, score))

    return {
        'score': score,
        'is_suitable': score > 0.4,
        'reason': ', '.join(reasons) if reasons else 'åŸºç¡€è´¨é‡'
    }


def generate_summary_report(df: pd.DataFrame, report_path: str):
    """
    ç”Ÿæˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Š

    Args:
        df: æ•°æ®DataFrame
        report_path: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("äººæ•™ç‰ˆè¯­æ–‡æ•™ææ•°æ®ç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("åŸºæœ¬ç»Ÿè®¡:\n")
        f.write(f"  æ€»ç‰‡æ®µæ•°: {len(df)}\n")
        f.write(f"  æ€»é¡µæ•°: {df['page_number'].nunique()}\n")
        f.write(f"  å¹³å‡ç‰‡æ®µé•¿åº¦: {df['content_length'].mean():.1f} å­—ç¬¦\n")
        f.write(f"  ä¸­ä½æ•°ç‰‡æ®µé•¿åº¦: {df['content_length'].median():.1f} å­—ç¬¦\n\n")

        # å•å…ƒç»Ÿè®¡
        if df['unit_number'].notna().any():
            unit_stats = df[df['unit_number'].notna()].groupby('unit_number').agg({
                'chunk_id': 'count',
                'content_length': 'mean'
            }).round(1)

            f.write("å•å…ƒåˆ†å¸ƒ:\n")
            for unit_num, row in unit_stats.iterrows():
                f.write(f"  ç¬¬{unit_num}å•å…ƒ: {row['chunk_id']}ä¸ªç‰‡æ®µ, å¹³å‡é•¿åº¦{row['content_length']:.1f}å­—ç¬¦\n")
            f.write("\n")

        # è¯¾æ–‡ç»Ÿè®¡
        if df['lesson_title'].notna().any():
            lesson_stats = df[df['lesson_title'].notna()].groupby('lesson_title').agg({
                'chunk_id': 'count',
                'content_length': 'mean'
            }).round(1)

            f.write("è¯¾æ–‡åˆ†å¸ƒ (å‰10ç¯‡):\n")
            for lesson_title, row in lesson_stats.head(10).iterrows():
                f.write(f"  {lesson_title}: {row['chunk_id']}ä¸ªç‰‡æ®µ, å¹³å‡é•¿åº¦{row['content_length']:.1f}å­—ç¬¦\n")
            f.write("\n")

        # è´¨é‡åˆ†å¸ƒ
        quality_stats = df['text_quality'].apply(lambda x: x.get('score', 0) if isinstance(x, dict) else 0)
        f.write("è´¨é‡åˆ†å¸ƒ:\n")
        f.write(f"  å¹³å‡è´¨é‡åˆ†æ•°: {quality_stats.mean():.3f}\n")
        f.write(f"  é«˜è´¨é‡ç‰‡æ®µ(>0.7): {(quality_stats > 0.7).sum()}ä¸ª\n")
        f.write(f"  ä¸­ç­‰è´¨é‡ç‰‡æ®µ(0.4-0.7): {((quality_stats >= 0.4) & (quality_stats <= 0.7)).sum()}ä¸ª\n")
        f.write(f"  ä½è´¨é‡ç‰‡æ®µ(<0.4): {(quality_stats < 0.4).sum()}ä¸ª\n")

    print_status(f"ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}", "ğŸ“Š")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—ï¸ ä½œä¸šæ­å­ RAG ç³»ç»Ÿ - PDFæ•™ææ•°æ®å¯¼å‡ºåˆ°CSV")
    print("=" * 60)
    print()

    # é…ç½®å‚æ•°
    data_dir = os.getenv("TEXTBOOK_DIR", "data/textbooks")
    output_dir = Path("exports")
    output_dir.mkdir(exist_ok=True)

    print(f"ğŸ“‚ æ•™æç›®å½•: {data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print()

    # æŸ¥æ‰¾è¯­æ–‡æ•™æPDF
    data_path = Path(data_dir)
    if not data_path.exists():
        print_status(f"æ•™æç›®å½•ä¸å­˜åœ¨: {data_dir}", "âŒ")
        return 1

    pdf_files = [f for f in data_path.glob("*.pdf") if "è¯­æ–‡" in f.name]
    if not pdf_files:
        print_status("æœªæ‰¾åˆ°è¯­æ–‡æ•™æPDFæ–‡ä»¶", "âŒ")
        return 1

    # å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶
    for pdf_file in pdf_files:
        try:
            output_csv = output_dir / f"{pdf_file.stem}_content.csv"
            df = extract_textbook_content_to_csv(str(pdf_file), str(output_csv))

            print(f"âœ… å®Œæˆå¤„ç†: {pdf_file.name} -> {output_csv.name}")

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ {pdf_file.name}: {e}")
            continue

    print()
    print("ğŸ‰ PDFæ•°æ®å¯¼å‡ºå®Œæˆ!")
    print("ğŸ“‹ ä¸‹ä¸€æ­¥:")
    print("  1. æ£€æŸ¥CSVæ–‡ä»¶ä¸­çš„æ•°æ®ç»“æ„")
    print("  2. éªŒè¯å•å…ƒå’Œè¯¾æ–‡ä¿¡æ¯çš„å‡†ç¡®æ€§")
    print("  3. æ ¹æ®éœ€è¦è°ƒæ•´å’Œæ¸…ç†æ•°æ®")
    print("  4. å‡†å¤‡è¿›è¡Œå‘é‡åŒ–å¤„ç†")

    return 0


if __name__ == "__main__":
    sys.exit(main())