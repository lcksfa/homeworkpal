#!/usr/bin/env python3
"""
è¯­æ–‡æ•™æå‘é‡åŒ–å¯¼å…¥è„šæœ¬
Chinese Textbook Vectorization Import Script

ä»CSVæ–‡ä»¶å¯¼å…¥è¯­æ–‡æ•™æå†…å®¹åˆ°å‘é‡æ•°æ®åº“
"""

import os
import sys
import pandas as pd
import hashlib
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

from homeworkpal.database.connection import engine
from sqlalchemy.orm import sessionmaker
from homeworkpal.database.models import TextbookChunk
from homeworkpal.llm.base import BaseEmbeddingModel
from homeworkpal.llm.siliconflow import SiliconFlowEmbeddingModel

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾", "ğŸ“–": "ğŸ“–"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def load_csv_data(csv_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½CSVæ•°æ®"""
    print_status(f"åŠ è½½CSVæ–‡ä»¶: {csv_path}", "ğŸ“š")

    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_path)

        # è¿‡æ»¤é«˜è´¨é‡å†…å®¹
        df = df[df['text_quality'].str.contains("'is_suitable': True", na=False)]

        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        chunks = []
        for _, row in df.iterrows():
            # è§£ææ–‡æœ¬è´¨é‡ä¿¡æ¯
            try:
                text_quality = eval(row['text_quality']) if isinstance(row['text_quality'], str) else {}
                quality_score = text_quality.get('score', 0.8)
            except:
                quality_score = 0.8

            # åˆ›å»ºå…ƒæ•°æ®
            metadata = {
                'pdf_file': 'è¯­æ–‡ä¸‰ä¸Š.pdf',
                'subject': 'è¯­æ–‡',
                'grade': 'ä¸‰å¹´çº§',
                'page_number': int(row['page_number']),
                'unit_number': row['unit_number'] if pd.notna(row['unit_number']) else None,
                'unit_title': row['unit_title'] if pd.notna(row['unit_title']) else None,
                'lesson_number': row['lesson_number'] if pd.notna(row['lesson_number']) else None,
                'lesson_title': row['lesson_title'] if pd.notna(row['lesson_title']) else None,
                'lesson_start_page': row['lesson_start_page'] if pd.notna(row['lesson_start_page']) else None,
                'lesson_end_page': row['lesson_end_page'] if pd.notna(row['lesson_end_page']) else None,
                'content_length': int(row['content_length']) if pd.notna(row['content_length']) else 0,
                'processed_date': datetime.now().isoformat(),
                'content_type': 'è¯¾æ–‡å†…å®¹',
                'source_file': row['source_file']
            }

            # æ·»åŠ å†…å®¹åˆ†ç±»ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'content_category' in row and pd.notna(row['content_category']):
                metadata['content_category'] = row['content_category']
                # æ›´æ–°content_typeä»¥åŒ¹é…åˆ†ç±»
                if row['content_category'] == 'è¯¾æ–‡':
                    metadata['content_type'] = 'è¯¾æ–‡ä¸»ä½“'
                elif row['content_category'] == 'ä¹ ä½œ':
                    metadata['content_type'] = 'ä¹ ä½œæŒ‡å¯¼'
                elif row['content_category'] == 'äº¤æµ':
                    metadata['content_type'] = 'å£è¯­äº¤é™…'
                elif row['content_category'] == 'ç»ƒä¹ ':
                    metadata['content_type'] = 'è¯¾åç»ƒä¹ '
                elif row['content_category'] == 'æ—¥ç§¯æœˆç´¯':
                    metadata['content_type'] = 'æ—¥ç§¯æœˆç´¯'
                elif row['content_category'] == 'é˜…è¯»':
                    metadata['content_type'] = 'é˜…è¯»ææ–™'
                else:
                    metadata['content_type'] = 'å…¶ä»–å†…å®¹'

            chunk = {
                'content': row['content'],
                'page_number': int(row['page_number']),
                'chunk_index': int(row['chunk_index']),
                'metadata_json': metadata,
                'quality_score': quality_score,
                'source_file': row['source_file']
            }
            chunks.append(chunk)

        print_status(f"æˆåŠŸåŠ è½½ {len(chunks)} ä¸ªé«˜è´¨é‡ç‰‡æ®µ", "âœ…")
        return chunks

    except Exception as e:
        print_status(f"åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {e}", "âŒ")
        return []


def generate_content_hash(content: str) -> str:
    """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def get_embedding_llm():
    """è·å–åµŒå…¥æ¨¡å‹"""
    try:
        import os
        api_key = os.getenv("SILICONFLOW_API_KEY")
        base_url = os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1")

        if not api_key:
            print_status("æœªè®¾ç½®SILICONFLOW_API_KEYç¯å¢ƒå˜é‡", "âš ï¸")
            return None

        embedding_model = SiliconFlowEmbeddingModel(
            api_key=api_key,
            base_url=base_url
        )
        print_status("æˆåŠŸåŠ è½½SiliconFlowåµŒå…¥æ¨¡å‹", "âœ…")
        return embedding_model
    except Exception as e:
        print_status(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}", "âš ï¸")
        return None


def import_chunks_to_database(chunks: List[Dict[str, Any]], llm = None):
    """å¯¼å…¥ç‰‡æ®µåˆ°æ•°æ®åº“"""
    print_status("å¼€å§‹å¯¼å…¥ç‰‡æ®µåˆ°æ•°æ®åº“", "ğŸ’¾")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # æ¸…é™¤ç°æœ‰çš„è¯­æ–‡æ•™ææ•°æ®
        existing_count = session.query(TextbookChunk).filter(
            TextbookChunk.source_file.like('%è¯­æ–‡%')
        ).count()

        if existing_count > 0:
            print_status(f"æ¸…é™¤ç°æœ‰ {existing_count} ä¸ªè¯­æ–‡æ•™æç‰‡æ®µ", "ğŸ”§")
            session.query(TextbookChunk).filter(
                TextbookChunk.source_file.like('%è¯­æ–‡%')
            ).delete()
            session.commit()

        imported_count = 0
        skipped_count = 0

        for chunk_data in chunks:
            # æ£€æŸ¥å†…å®¹æ˜¯å¦å·²å­˜åœ¨
            content_hash = generate_content_hash(chunk_data['content'])
            existing = session.query(TextbookChunk).filter(
                TextbookChunk.content_hash == content_hash
            ).first()

            if existing:
                skipped_count += 1
                continue

            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = None
            if llm:
                try:
                    embedding = llm.embed_query(chunk_data['content'])
                except Exception as e:
                    print_status(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}", "âš ï¸")
                    embedding = None

            # åˆ›å»ºæ•°æ®åº“è®°å½•
            db_chunk = TextbookChunk(
                content=chunk_data['content'],
                embedding=embedding,
                content_hash=content_hash,
                metadata_json=chunk_data['metadata_json'],
                source_file=chunk_data['source_file'],
                chunk_index=chunk_data['chunk_index'],
                page_number=chunk_data['page_number'],
                quality_score=chunk_data['quality_score']
            )

            session.add(db_chunk)
            imported_count += 1

            # æ¯10ä¸ªç‰‡æ®µæäº¤ä¸€æ¬¡
            if imported_count % 10 == 0:
                session.commit()
                print_status(f"å·²å¯¼å…¥ {imported_count} ä¸ªç‰‡æ®µ", "ğŸ’¾")

        # æœ€ç»ˆæäº¤
        session.commit()
        session.close()

        print_status(f"æˆåŠŸå¯¼å…¥ {imported_count} ä¸ªç‰‡æ®µï¼Œè·³è¿‡ {skipped_count} ä¸ªé‡å¤ç‰‡æ®µ", "âœ…")
        return True

    except Exception as e:
        print_status(f"å¯¼å…¥æ•°æ®åº“å¤±è´¥: {e}", "âŒ")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ è¯­æ–‡æ•™æå‘é‡åŒ–å¯¼å…¥è„šæœ¬")
    print("=" * 50)

    # CSVæ–‡ä»¶è·¯å¾„
    csv_path = "/Users/lizhao/workspace/hulus/homeworkpal/exports/è¯­æ–‡ä¸‰ä¸Š_content_cleaned.csv"

    if not os.path.exists(csv_path):
        print_status(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}", "âŒ")
        return 1

    # åŠ è½½CSVæ•°æ®
    chunks = load_csv_data(csv_path)
    if not chunks:
        print_status("æ²¡æœ‰å¯å¯¼å…¥çš„æ•°æ®", "âŒ")
        return 1

    # è·å–åµŒå…¥æ¨¡å‹
    llm = get_embedding_llm()

    # å¯¼å…¥åˆ°æ•°æ®åº“
    if import_chunks_to_database(chunks, llm):
        print("\nğŸ‰ è¯­æ–‡æ•™æå‘é‡åŒ–å¯¼å…¥å®Œæˆï¼")
        print("ğŸ’¡ å¯ä»¥å¼€å§‹è¿›è¡Œæ™ºèƒ½æ£€ç´¢æµ‹è¯•")
        return 0
    else:
        print("\nâš ï¸ å¯¼å…¥å¤±è´¥")
        return 1


if __name__ == "__main__":
    sys.exit(main())