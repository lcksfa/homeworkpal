#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆçŸ¥è¯†åº“å…¥åº“è„šæœ¬
Enhanced Textbook Knowledge Ingestion Script for Homework Pal RAG System

é›†æˆäº†ä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨ï¼Œæ”¯æŒæ‹¼éŸ³ä¿®å¤å’Œæ–‡æœ¬è´¨é‡ä¼˜åŒ–
"""

import sys
import os
from pathlib import Path
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from homeworkpal.database.connection import engine, get_db
from homeworkpal.database.models import TextbookChunk
from homeworkpal.llm.siliconflow import create_siliconflow_client, SiliconFlowEmbeddingModel
from homeworkpal.document import create_pdf_processor, create_pdf_splitter
from homeworkpal.document.chinese_text_processor import ChineseTextProcessor
from sqlalchemy.orm import sessionmaker

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾", "ğŸš€": "ğŸš€"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def process_textbook_documents_enhanced(data_dir: str) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨å¢å¼ºçš„æ–‡æœ¬å¤„ç†å™¨å¤„ç†æ•™ææ–‡æ¡£

    Args:
        data_dir: æ•™ææ–‡æ¡£ç›®å½•è·¯å¾„

    Returns:
        å¤„ç†åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"å¤„ç†æ•™ææ–‡æ¡£: {data_dir}", "ğŸ“š")

    all_chunks = []
    data_path = Path(data_dir)

    if not data_path.exists():
        print_status(f"æ•™æç›®å½•ä¸å­˜åœ¨: {data_dir}", "âŒ")
        return all_chunks

    # åˆ›å»ºå¤„ç†å™¨
    pdf_processor = create_pdf_processor()
    text_splitter = create_pdf_splitter(chunk_size=1500, chunk_overlap=200)

    # åˆ›å»ºä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨
    embedding_model = SiliconFlowEmbeddingModel(
        api_key=os.getenv('SILICONFLOW_API_KEY'),
        base_url=os.getenv('SILICONFLOW_BASE_URL'),
        model_name='BAAI/bge-m3'
    )
    chinese_processor = ChineseTextProcessor(embedding_model)

    # ä¼˜å…ˆå¤„ç†PDFæ–‡ä»¶
    pdf_files = list(data_path.glob("*.pdf"))
    print(f"ğŸ“„ å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

    for pdf_file in pdf_files:
        try:
            print(f"  ğŸ”„ æ­£åœ¨å¤„ç†: {pdf_file.name}")

            # ä½¿ç”¨PDFå¤„ç†å™¨æå–å†…å®¹
            pdf_result = pdf_processor.extract_text_from_pdf(str(pdf_file))

            # ä½¿ç”¨æ™ºèƒ½åˆ†æ®µå™¨å¤„ç†å†…å®¹
            chunks = text_splitter.split_pdf_content(pdf_result)

            # åº”ç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†å’Œè´¨é‡è¯„ä¼°
            processed_count = 0
            for chunk in chunks:
                # ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨é¢„å¤„ç†
                original_content = chunk['content']
                processed_content = chinese_processor.preprocess_chinese_text_for_embedding(original_content)

                if not processed_content.strip():
                    continue  # è·³è¿‡ç©ºå†…å®¹

                # è´¨é‡è¯„ä¼°
                quality_assessment = chinese_processor.assess_embedding_quality(processed_content)

                # åªä¿ç•™é«˜è´¨é‡å†…å®¹
                if quality_assessment['is_suitable']:
                    processed_chunk = {
                        'content': processed_content,
                        'original_content': original_content,
                        'source': str(pdf_file),
                        'file_name': pdf_file.name,
                        'file_type': 'pdf',
                        'chunk_id': chunk['id'],
                        'page_number': chunk['page_number'],
                        'chunk_index': chunk['chunk_index'],
                        'quality_score': quality_assessment['score'],
                        'quality_details': quality_assessment,
                        'metadata': chunk['metadata']
                    }

                    all_chunks.append(processed_chunk)
                    processed_count += 1

            print(f"  âœ… {pdf_file.name}: åŸå§‹ {len(chunks)} ç‰‡æ®µ â†’ é«˜è´¨é‡ {processed_count} ç‰‡æ®µ")

        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥ {pdf_file.name}: {e}")

    # å¤„ç†å…¶ä»–æ ¼å¼æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    other_extensions = ['.md', '.txt']
    for ext in other_extensions:
        for file_path in data_path.glob(f"*{ext}"):
            try:
                print(f"  ğŸ”„ æ­£åœ¨å¤„ç†: {file_path.name}")

                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨é¢„å¤„ç†
                processed_content = chinese_processor.preprocess_chinese_text_for_embedding(content)

                if not processed_content.strip():
                    continue

                # è´¨é‡è¯„ä¼°
                quality_assessment = chinese_processor.assess_embedding_quality(processed_content)

                if quality_assessment['is_suitable']:
                    # ç®€å•åˆ†æ®µ
                    from langchain.text_splitter import RecursiveCharacterTextSplitter
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1500,
                        chunk_overlap=200,
                        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
                    )

                    docs = splitter.create_documents([processed_content])

                    for i, doc in enumerate(docs):
                        metadata = {
                            'file_name': file_path.name,
                            'file_type': ext,
                            'source': str(file_path),
                            'subject': 'æœªçŸ¥',
                            'grade': 'ä¸‰å¹´çº§',
                            'processed_date': datetime.now().isoformat(),
                            'content_type': 'æ­£æ–‡å†…å®¹',
                            'quality_score': quality_assessment['score']
                        }

                        all_chunks.append({
                            'content': doc.page_content,
                            'original_content': content,
                            'source': str(file_path),
                            'file_name': file_path.name,
                            'file_type': ext,
                            'chunk_index': i,
                            'quality_score': quality_assessment['score'],
                            'quality_details': quality_assessment,
                            'metadata': metadata
                        })

                    print(f"  âœ… {file_path.name}: ç”Ÿæˆ {len(docs)} ä¸ªç‰‡æ®µ")

                else:
                    print(f"  âš ï¸ {file_path.name}: è´¨é‡ä¸ç¬¦åˆè¦æ±‚")

            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥ {file_path.name}: {e}")

    print_status(f"å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªé«˜è´¨é‡æ–‡æ¡£ç‰‡æ®µ", "ğŸš€")
    return all_chunks


def generate_embeddings_enhanced(chunks: List[Dict[str, Any]],
                                embedding_client) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨å¢å¼ºçš„æ‰¹å¤„ç†ç”Ÿæˆæ–‡æ¡£ç‰‡æ®µçš„å‘é‡åµŒå…¥

    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        embedding_client: åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯

    Returns:
        åŒ…å«åµŒå…¥å‘é‡çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µçš„å‘é‡åµŒå…¥", "ğŸ”")

    if not chunks:
        print_status("æ²¡æœ‰æ–‡æ¡£ç‰‡æ®µå¯ä¾›å¤„ç†", "âŒ")
        return []

    # æå–æ–‡æœ¬å†…å®¹
    texts = [chunk['content'] for chunk in chunks]

    try:
        # ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨çš„æ‰¹é‡å‘é‡åŒ–åŠŸèƒ½
        chinese_processor = ChineseTextProcessor(embedding_client)
        embeddings, quality_results = chinese_processor.batch_vectorize_with_quality_control(
            texts,
            batch_size=5,  # ä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†ç¡®ä¿ç¨³å®šæ€§
            max_retries=3,
            quality_threshold=0.4
        )

        # éªŒè¯åµŒå…¥å‘é‡æ•°é‡å’Œç»´åº¦
        if len(embeddings) != len(chunks):
            raise ValueError(f"åµŒå…¥å‘é‡æ•°é‡({len(embeddings)})ä¸ç‰‡æ®µæ•°é‡({len(chunks)})ä¸åŒ¹é…")

        expected_dim = 1024  # BGE-M3çš„ç»´åº¦
        valid_count = 0
        for i, embedding in enumerate(embeddings):
            if len(embedding) == expected_dim and embedding != [0.0] * expected_dim:
                valid_count += 1

        print(f"  ğŸ“Š æœ‰æ•ˆå‘é‡: {valid_count}/{len(embeddings)}")

        # å°†åµŒå…¥å‘é‡æ·»åŠ åˆ°ç‰‡æ®µä¸­
        for i, chunk in enumerate(chunks):
            chunk['embedding'] = embeddings[i]
            # æ·»åŠ å†…å®¹å“ˆå¸Œç”¨äºå»é‡ï¼ˆä½¿ç”¨å¤„ç†åçš„å†…å®¹ï¼‰
            chunk['content_hash'] = hashlib.md5(chunk['content'].encode('utf-8')).hexdigest()
            # æ·»åŠ è´¨é‡è¯„ä¼°ç»“æœ
            if i < len(quality_results):
                chunk['quality_details'] = quality_results[i]

        print_status(f"æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡åµŒå…¥", "âœ…")
        return chunks

    except Exception as e:
        print_status(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}", "âŒ")
        return []


def save_to_database_enhanced(chunks: List[Dict[str, Any]],
                             batch_size: int = 10) -> bool:
    """
    å°†å¢å¼ºå¤„ç†çš„æ–‡æ¡£ç‰‡æ®µä¿å­˜åˆ°æ•°æ®åº“

    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    print_status(f"ä¿å­˜ {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°æ•°æ®åº“", "ğŸ’¾")

    try:
        # åˆ›å»ºæ•°æ®åº“ä¼šè¯
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # å»é‡ï¼šæ£€æŸ¥å†…å®¹å“ˆå¸Œæ˜¯å¦å·²å­˜åœ¨
        existing_hashes = set()
        existing_records = session.query(TextbookChunk.content_hash).filter(
            TextbookChunk.content_hash.in_([chunk.get('content_hash', '') for chunk in chunks])
        ).all()
        existing_hashes = set(record[0] for record in existing_records)

        # è¿‡æ»¤æ–°ç‰‡æ®µ
        new_chunks = [chunk for chunk in chunks if chunk.get('content_hash', '') not in existing_hashes]

        if not new_chunks:
            print_status("æ‰€æœ‰ç‰‡æ®µéƒ½å·²å­˜åœ¨äºæ•°æ®åº“ä¸­", "âœ…")
            session.close()
            return True

        print(f"  ğŸ“Š è¿‡æ»¤é‡å¤åæ–°å¢ {len(new_chunks)} ä¸ªç‰‡æ®µ")

        # ç»Ÿè®¡è´¨é‡åˆ†å¸ƒ
        quality_scores = [chunk.get('quality_score', 0) for chunk in new_chunks]
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        high_quality_count = sum(1 for score in quality_scores if score >= 0.8)

        print(f"  ğŸ“ˆ å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.3f}")
        print(f"  ğŸ“Š é«˜è´¨é‡ç‰‡æ®µ: {high_quality_count}/{len(new_chunks)} ({high_quality_count/len(new_chunks)*100:.1f}%)")

        # æ‰¹é‡ä¿å­˜
        saved_count = 0
        for i in range(0, len(new_chunks), batch_size):
            batch = new_chunks[i:i + batch_size]

            for chunk in batch:
                # å¢å¼ºå…ƒæ•°æ®
                enhanced_metadata = chunk.get('metadata', {})
                enhanced_metadata.update({
                    'processed_date': datetime.now().isoformat(),
                    'quality_score': chunk.get('quality_score', 0),
                    'has_original': 'original_content' in chunk,
                    'text_processor': 'enhanced_chinese_processor'
                })

                # åˆ›å»ºTextbookChunkå¯¹è±¡
                textbook_chunk = TextbookChunk(
                    content=chunk['content'],
                    embedding=chunk['embedding'],
                    metadata_json=enhanced_metadata,
                    source_file=chunk['source'],
                    chunk_index=chunk['chunk_index'],
                    content_hash=chunk.get('content_hash', hashlib.md5(chunk['content'].encode('utf-8')).hexdigest()),
                    page_number=chunk.get('page_number'),
                    quality_score=chunk.get('quality_score', 1.0)
                )

                session.add(textbook_chunk)
                saved_count += 1

            # æäº¤æ‰¹æ¬¡
            session.commit()
            print(f"  âœ… å·²ä¿å­˜ {min(i + batch_size, len(new_chunks))}/{len(new_chunks)} ä¸ªç‰‡æ®µ")

        session.close()

        print_status(f"æˆåŠŸä¿å­˜ {saved_count} ä¸ªæ–°æ–‡æ¡£ç‰‡æ®µåˆ°æ•°æ®åº“", "âœ…")
        return True

    except Exception as e:
        print_status(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}", "âŒ")
        if 'session' in locals():
            session.rollback()
            session.close()
        return False


def verify_ingestion_enhanced():
    """
    éªŒè¯å¢å¼ºç‰ˆå…¥åº“ç»“æœ
    """
    print_status("éªŒè¯å¢å¼ºç‰ˆå…¥åº“ç»“æœ", "ğŸ”")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # æŸ¥è¯¢æ€»è®°å½•æ•°
        total_count = session.query(TextbookChunk).count()
        print(f"  ğŸ“Š æ•°æ®åº“ä¸­æ€»è®°å½•æ•°: {total_count}")

        if total_count > 0:
            # æŸ¥è¯¢ç¤ºä¾‹è®°å½•
            sample_chunk = session.query(TextbookChunk).first()
            print(f"  ğŸ“ ç¤ºä¾‹å†…å®¹é•¿åº¦: {len(sample_chunk.content)} å­—ç¬¦")
            embedding_dim = len(sample_chunk.embedding) if hasattr(sample_chunk.embedding, '__len__') else 0
            print(f"  ğŸ”¢ å‘é‡ç»´åº¦: {embedding_dim}")
            print(f"  ğŸ“„ æºæ–‡ä»¶: {sample_chunk.source_file}")
            print(f"  ğŸ¯ è´¨é‡è¯„åˆ†: {sample_chunk.quality_score}")
            print(f"  ğŸ“‹ å…ƒæ•°æ®: {json.dumps(sample_chunk.metadata_json, ensure_ascii=False, indent=2)}")

            # è´¨é‡åˆ†å¸ƒç»Ÿè®¡
            from sqlalchemy import func
            quality_stats = session.query(
                func.avg(TextbookChunk.quality_score).label('avg_quality'),
                func.min(TextbookChunk.quality_score).label('min_quality'),
                func.max(TextbookChunk.quality_score).label('max_quality')
            ).first()

            print(f"  ğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
            print(f"    å¹³å‡: {quality_stats.avg_quality:.3f}")
            print(f"    æœ€ä½: {quality_stats.min_quality:.3f}")
            print(f"    æœ€é«˜: {quality_stats.max_quality:.3f}")

        session.close()
        print_status("å¢å¼ºç‰ˆå…¥åº“éªŒè¯å®Œæˆ", "âœ…")
        return True

    except Exception as e:
        print_status(f"éªŒè¯å¤±è´¥: {e}", "âŒ")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä½œä¸šæ­å­ RAG ç³»ç»Ÿ - å¢å¼ºç‰ˆçŸ¥è¯†åº“å…¥åº“è„šæœ¬")
    print("=" * 60)
    print()

    # é…ç½®å‚æ•°
    data_dir = os.getenv("TEXTBOOK_DIR", "data/textbooks")
    chunk_size = int(os.getenv("CHUNK_SIZE", "1000"))
    chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "200"))

    print(f"ğŸ“‚ æ•™æç›®å½•: {data_dir}")
    print(f"ğŸ“ ç‰‡æ®µå¤§å°: {chunk_size}")
    print(f"ğŸ”„ ç‰‡æ®µé‡å : {chunk_overlap}")
    print("ğŸš€ å¯ç”¨å¢å¼ºç‰ˆä¸­æ–‡æ–‡æœ¬å¤„ç†å’Œæ‹¼éŸ³ä¿®å¤")
    print()

    # æ‰§è¡Œå¤„ç†æµç¨‹
    try:
        # æ­¥éª¤1: å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†
        chunks = process_textbook_documents_enhanced(data_dir)
        if not chunks:
            print("âŒ å¤„ç†æ•™ææ–‡æ¡£ - å¤±è´¥")
            return 1
        print("âœ… å¤„ç†æ•™ææ–‡æ¡£ - é€šè¿‡")

        # æ­¥éª¤2: åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if not initialize_embedding_model():
            print("âŒ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - å¤±è´¥")
            return 1
        print("âœ… åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - é€šè¿‡")

        # æ­¥éª¤3: å¢å¼ºç‰ˆå‘é‡åµŒå…¥
        embedding_client = create_siliconflow_client()
        embedded_chunks = generate_embeddings_enhanced(chunks, embedding_client)
        if not embedded_chunks:
            print("âŒ ç”Ÿæˆå‘é‡åµŒå…¥ - å¤±è´¥")
            return 1
        print("âœ… ç”Ÿæˆå‘é‡åµŒå…¥ - é€šè¿‡")

        # æ­¥éª¤4: å¢å¼ºç‰ˆæ•°æ®åº“ä¿å­˜
        if not save_to_database_enhanced(embedded_chunks):
            print("âŒ ä¿å­˜åˆ°æ•°æ®åº“ - å¤±è´¥")
            return 1
        print("âœ… ä¿å­˜åˆ°æ•°æ®åº“ - é€šè¿‡")

        # æ­¥éª¤5: éªŒè¯å…¥åº“ç»“æœ
        if not verify_ingestion_enhanced():
            print("âŒ éªŒè¯å…¥åº“ç»“æœ - å¤±è´¥")
            return 1
        print("âœ… éªŒè¯å…¥åº“ç»“æœ - é€šè¿‡")

        passed = 5
        total = 5

    except Exception as e:
        print(f"âŒ å¤„ç†æµç¨‹å¤±è´¥: {e}")
        return 1

    print("\n" + "=" * 60)
    print(f"ğŸ“Š å¢å¼ºç‰ˆå…¥åº“ç»“æœ: {passed}/{total} æ­¥éª¤å®Œæˆ")

    if passed == total:
        print("ğŸ‰ å¢å¼ºç‰ˆçŸ¥è¯†åº“å…¥åº“å®Œæˆ!")
        print("âœ… äººæ•™ç‰ˆæ•™æå·²æˆåŠŸå‘é‡åŒ–å¹¶å­˜å‚¨åˆ°æ•°æ®åº“")
        print("ğŸš€ ä½¿ç”¨äº†ä¸­æ–‡æ‹¼éŸ³ä¿®å¤å’Œè´¨é‡ä¼˜åŒ–æŠ€æœ¯")
        print("ğŸ” ç°åœ¨å¯ä»¥è¿›è¡Œé«˜è´¨é‡çš„è¯­ä¹‰æ£€ç´¢å’Œé—®ç­”")
        return 0
    else:
        print("âš ï¸ å¢å¼ºç‰ˆçŸ¥è¯†åº“å…¥åº“æœªå®Œæˆï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1


def initialize_embedding_model():
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
    try:
        client = create_siliconflow_client()

        # æµ‹è¯•è¿æ¥
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        embedding = client.embed_query(test_text)

        if len(embedding) != 1024:
            raise ValueError(f"å‘é‡ç»´åº¦é”™è¯¯: {len(embedding)} (æœŸæœ›: 1024)")

        print_status("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ", "âœ…")
        return True

    except Exception as e:
        print_status(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}", "âŒ")
        return False


if __name__ == "__main__":
    sys.exit(main())