#!/usr/bin/env python3
"""
ç»“æ„åŒ–è¯¾æ–‡çŸ¥è¯†åº“å…¥åº“è„šæœ¬
Structured Textbook Knowledge Ingestion Script

ä¸“é—¨å¤„ç†äººæ•™ç‰ˆè¯­æ–‡æ•™æï¼ŒæŒ‰å•å…ƒ-è¯¾æ–‡ç»“æ„è¿›è¡Œæ™ºèƒ½åˆ†æå’Œå­˜å‚¨
"""

import sys
import os
from pathlib import Path
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from homeworkpal.database.connection import engine, get_db
from homeworkpal.database.models import TextbookChunk
from homeworkpal.llm.siliconflow import create_siliconflow_client, SiliconFlowEmbeddingModel
from homeworkpal.document import create_pdf_processor, create_pdf_splitter
from homeworkpal.document.chinese_text_processor import ChineseTextProcessor
from homeworkpal.document.chinese_textbook_analyzer import ChineseTextbookAnalyzer, LessonInfo, TextbookStructure
from sqlalchemy.orm import sessionmaker

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾", "ğŸš€": "ğŸš€", "ğŸ“–": "ğŸ“–", "ğŸ—ï¸": "ğŸ—ï¸"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def process_textbook_structured(data_dir: str) -> TextbookStructure:
    """
    ç»“æ„åŒ–å¤„ç†æ•™ææ–‡æ¡£ï¼ŒæŒ‰å•å…ƒ-è¯¾æ–‡è¿›è¡Œåˆ†æ

    Args:
        data_dir: æ•™ææ–‡æ¡£ç›®å½•è·¯å¾„

    Returns:
        ç»“æ„åŒ–çš„æ•™æä¿¡æ¯
    """
    print_status(f"ç»“æ„åŒ–å¤„ç†æ•™ææ–‡æ¡£: {data_dir}", "ğŸ“–")

    all_chunks = []
    data_path = Path(data_dir)

    if not data_path.exists():
        print_status(f"æ•™æç›®å½•ä¸å­˜åœ¨: {data_dir}", "âŒ")
        return None

    # åˆ›å»ºå¤„ç†å™¨
    pdf_processor = create_pdf_processor()
    text_splitter = create_pdf_splitter(chunk_size=2000, chunk_overlap=300)  # æ›´å¤§çš„æ®µè½ä¾¿äºè¯¾æ–‡è¯†åˆ«

    # åˆ›å»ºä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨
    embedding_model = SiliconFlowEmbeddingModel(
        api_key=os.getenv('SILICONFLOW_API_KEY'),
        base_url=os.getenv('SILICONFLOW_BASE_URL'),
        model_name='BAAI/bge-m3'
    )
    chinese_processor = ChineseTextProcessor(embedding_model)

    # åˆ›å»ºè¯¾æ–‡åˆ†æå™¨
    textbook_analyzer = ChineseTextbookAnalyzer()

    pdf_files = [f for f in data_path.glob("*.pdf") if "è¯­æ–‡" in f.name]
    if not pdf_files:
        print_status("æœªæ‰¾åˆ°è¯­æ–‡æ•™æPDFæ–‡ä»¶", "âŒ")
        return None

    for pdf_file in pdf_files:
        try:
            print(f"  ğŸ”„ æ­£åœ¨å¤„ç†: {pdf_file.name}")

            # ä½¿ç”¨PDFå¤„ç†å™¨æå–å†…å®¹
            pdf_result = pdf_processor.extract_text_from_pdf(str(pdf_file))

            # ä½¿ç”¨æ™ºèƒ½åˆ†æ®µå™¨å¤„ç†å†…å®¹
            chunks = text_splitter.split_pdf_content(pdf_result)

            # åº”ç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†å’Œè´¨é‡è¯„ä¼°
            processed_count = 0
            for chunk in chunks:
                # ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨é¢„å¤„ç†
                original_content = chunk['content']
                processed_content = chinese_processor.preprocess_chinese_text_for_embedding(original_content)

                if not processed_content.strip():
                    continue  # è·³è¿‡ç©ºå†…å®¹

                # è´¨é‡è¯„ä¼°
                quality_assessment = chinese_processor.assess_embedding_quality(processed_content)

                # åªä¿ç•™é«˜è´¨é‡å†…å®¹
                if quality_assessment['is_suitable']:
                    processed_chunk = {
                        'content': processed_content,
                        'original_content': original_content,
                        'source': str(pdf_file),
                        'file_name': pdf_file.name,
                        'file_type': 'pdf',
                        'chunk_id': chunk['id'],
                        'page_number': chunk['page_number'],
                        'chunk_index': chunk['chunk_index'],
                        'quality_score': quality_assessment['score'],
                        'quality_details': quality_assessment,
                        'metadata': chunk['metadata']
                    }

                    all_chunks.append(processed_chunk)
                    processed_count += 1

            print(f"  âœ… {pdf_file.name}: åŸå§‹ {len(chunks)} ç‰‡æ®µ â†’ é«˜è´¨é‡ {processed_count} ç‰‡æ®µ")

        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥ {pdf_file.name}: {e}")
            return None

    print(f"ğŸ“š å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªé«˜è´¨é‡æ–‡æ¡£ç‰‡æ®µ")

    # ä½¿ç”¨æ™ºèƒ½åˆ†æå™¨åˆ†ææ•™æç»“æ„
    print_status("å¼€å§‹æ™ºèƒ½è¯¾æ–‡ç»“æ„åˆ†æ", "ğŸ§ ")
    structure = textbook_analyzer.analyze_textbook_structure(all_chunks)

    if structure and structure.units:
        print_status(f"åˆ†æå®Œæˆ: {structure.total_lessons} ç¯‡è¯¾æ–‡, {len(set(l.unit_number for l in structure.units))} ä¸ªå•å…ƒ", "ğŸ‰")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = textbook_analyzer.get_lesson_statistics(structure)
        print(f"ğŸ“Š æ•™æç»Ÿè®¡:")
        print(f"  å¹´çº§: {structure.grade} {structure.subject}")
        print(f"  æ€»å•å…ƒæ•°: {stats['total_units']}")
        print(f"  æ€»è¯¾æ–‡æ•°: {stats['total_lessons']}")

        for unit_key, unit_info in stats['units'].items():
            print(f"  {unit_key} - {unit_info['unit_title']}: {len(unit_info['lessons'])} ç¯‡è¯¾æ–‡")
    else:
        print_status("è¯¾æ–‡ç»“æ„åˆ†æå¤±è´¥", "âŒ")
        return None

    return structure


def generate_structured_embeddings(structure: TextbookStructure, embedding_client) -> List[Dict[str, Any]]:
    """
    ä¸ºç»“æ„åŒ–çš„è¯¾æ–‡ç”Ÿæˆå‘é‡åµŒå…¥

    Args:
        structure: æ•™æç»“æ„ä¿¡æ¯
        embedding_client: åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯

    Returns:
        åŒ…å«åµŒå…¥å‘é‡çš„è¯¾æ–‡æ•°æ®åˆ—è¡¨
    """
    print_status(f"ä¸º {structure.total_lessons} ç¯‡è¯¾æ–‡ç”Ÿæˆå‘é‡åµŒå…¥", "ğŸ”")

    if not structure or not structure.units:
        return []

    # åˆ›å»ºä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨
    chinese_processor = ChineseTextProcessor(embedding_client)

    all_lesson_embeddings = []
    total_chunks = 0

    for lesson in structure.units:
        if not lesson.content_chunks:
            continue

        print(f"  ğŸ“– å¤„ç†è¯¾æ–‡: ç¬¬{lesson.unit_number}å•å…ƒ ç¬¬{lesson.lesson_number}è¯¾ - {lesson.lesson_title}")

        # æå–è¯¾æ–‡å†…å®¹
        lesson_texts = [chunk['content'] for chunk in lesson.content_chunks]
        total_chunks += len(lesson_texts)

        try:
            # æ‰¹é‡ç”Ÿæˆå‘é‡åµŒå…¥
            embeddings, quality_results = chinese_processor.batch_vectorize_with_quality_control(
                lesson_texts,
                batch_size=3,  # è¯¾æ–‡å†…å®¹è¾ƒé•¿ï¼Œä½¿ç”¨æ›´å°çš„æ‰¹å¤„ç†
                max_retries=3,
                quality_threshold=0.6  # è¯¾æ–‡å†…å®¹è´¨é‡è¦æ±‚æ›´é«˜
            )

            # åˆ›å»ºè¯¾æ–‡çš„å®Œæ•´å…ƒæ•°æ®
            lesson_metadata = {
                'grade': structure.grade,
                'subject': structure.subject,
                'unit_number': lesson.unit_number,
                'unit_title': lesson.unit_title,
                'lesson_number': lesson.lesson_number,
                'lesson_title': lesson.lesson_title,
                'start_page': lesson.start_page,
                'end_page': lesson.end_page,
                'total_chunks': len(lesson.content_chunks),
                'text_processor': 'structured_chinese_processor',
                'analysis_timestamp': datetime.now().isoformat()
            }

            # ä¸ºæ¯ä¸ªå†…å®¹ç‰‡æ®µåˆ›å»ºå®Œæ•´è®°å½•
            for i, (chunk, embedding, quality) in enumerate(zip(lesson.content_chunks, embeddings, quality_results)):
                chunk_record = {
                    'content': chunk['content'],
                    'original_content': chunk['original_content'],
                    'embedding': embedding,
                    'content_hash': hashlib.md5(chunk['content'].encode('utf-8')).hexdigest(),
                    'metadata_json': {
                        **chunk['metadata'],
                        **lesson_metadata,
                        'chunk_index_in_lesson': i,
                        'quality_details': quality
                    },
                    'source_file': chunk['source'],
                    'chunk_index': chunk['chunk_index'],
                    'page_number': chunk['page_number'],
                    'quality_score': quality['score']
                }

                all_lesson_embeddings.append(chunk_record)

            print(f"    âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªåµŒå…¥å‘é‡")

        except Exception as e:
            print(f"    âŒ è¯¾æ–‡å¤„ç†å¤±è´¥: {e}")
            # ä¸ºå¤±è´¥çš„è¯¾æ–‡æ·»åŠ å ä½è®°å½•
            for chunk in lesson.content_chunks:
                chunk_record = {
                    'content': chunk['content'],
                    'embedding': [0.0] * 1024,  # é›¶å‘é‡å ä½
                    'content_hash': hashlib.md5(chunk['content'].encode('utf-8')).hexdigest(),
                    'metadata_json': {
                        **chunk['metadata'],
                        **{
                            'grade': structure.grade,
                            'subject': structure.subject,
                            'unit_number': lesson.unit_number,
                            'unit_title': lesson.unit_title,
                            'lesson_number': lesson.lesson_number,
                            'lesson_title': lesson.lesson_title,
                            'start_page': lesson.start_page,
                            'end_page': lesson.end_page,
                            'processing_error': str(e)
                        }
                    },
                    'source_file': chunk['source'],
                    'chunk_index': chunk['chunk_index'],
                    'page_number': chunk['page_number'],
                    'quality_score': 0.0
                }

                all_lesson_embeddings.append(chunk_record)

    print_status(f"æˆåŠŸä¸º {total_chunks} ä¸ªå†…å®¹ç‰‡æ®µç”Ÿæˆå‘é‡åµŒå…¥", "âœ…")
    return all_lesson_embeddings


def save_structured_to_database(lesson_data: List[Dict[str, Any]], batch_size: int = 10) -> bool:
    """
    å°†ç»“æ„åŒ–çš„è¯¾æ–‡æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“

    Args:
        lesson_data: è¯¾æ–‡æ•°æ®åˆ—è¡¨
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    print_status(f"ä¿å­˜ {len(lesson_data)} ä¸ªç»“æ„åŒ–è¯¾æ–‡ç‰‡æ®µåˆ°æ•°æ®åº“", "ğŸ’¾")

    try:
        # åˆ›å»ºæ•°æ®åº“ä¼šè¯
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # å»é‡ï¼šæ£€æŸ¥å†…å®¹å“ˆå¸Œæ˜¯å¦å·²å­˜åœ¨
        existing_hashes = set()
        existing_records = session.query(TextbookChunk.content_hash).filter(
            TextbookChunk.content_hash.in_([data.get('content_hash', '') for data in lesson_data])
        ).all()
        existing_hashes = set(record[0] for record in existing_records)

        # è¿‡æ»¤æ–°ç‰‡æ®µ
        new_chunks = [data for data in lesson_data if data.get('content_hash', '') not in existing_hashes]

        if not new_chunks:
            print_status("æ‰€æœ‰è¯¾æ–‡ç‰‡æ®µéƒ½å·²å­˜åœ¨äºæ•°æ®åº“ä¸­", "âœ…")
            session.close()
            return True

        print(f"  ğŸ“Š è¿‡æ»¤é‡å¤åæ–°å¢ {len(new_chunks)} ä¸ªç‰‡æ®µ")

        # ç»Ÿè®¡è´¨é‡åˆ†å¸ƒ
        quality_scores = [chunk.get('quality_score', 0) for chunk in new_chunks]
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        high_quality_count = sum(1 for score in quality_scores if score >= 0.8)

        print(f"  ğŸ“ˆ å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.3f}")
        print(f"  ğŸŒŸ é«˜è´¨é‡ç‰‡æ®µ: {high_quality_count}/{len(new_chunks)} ({high_quality_count/len(new_chunks)*100:.1f}%)")

        # æŒ‰å•å…ƒåˆ†ç»„ç»Ÿè®¡
        unit_stats = {}
        for chunk in new_chunks:
            metadata = chunk.get('metadata_json', {})
            unit_key = f"ç¬¬{metadata.get('unit_number', 0)}å•å…ƒ"
            unit_stats[unit_key] = unit_stats.get(unit_key, 0) + 1

        print(f"  ğŸ“š æŒ‰å•å…ƒåˆ†å¸ƒ:")
        for unit_key, count in unit_stats.items():
            print(f"    {unit_key}: {count} ä¸ªç‰‡æ®µ")

        # æ‰¹é‡ä¿å­˜
        saved_count = 0
        for i in range(0, len(new_chunks), batch_size):
            batch = new_chunks[i:i + batch_size]

            for chunk in batch:
                # åˆ›å»ºTextbookChunkå¯¹è±¡
                textbook_chunk = TextbookChunk(
                    content=chunk['content'],
                    embedding=chunk['embedding'],
                    metadata_json=chunk['metadata_json'],
                    source_file=chunk['source_file'],
                    chunk_index=chunk['chunk_index'],
                    content_hash=chunk['content_hash'],
                    page_number=chunk['page_number'],
                    quality_score=chunk['quality_score']
                )

                session.add(textbook_chunk)
                saved_count += 1

            # æäº¤æ‰¹æ¬¡
            session.commit()
            print(f"  âœ… å·²ä¿å­˜ {min(i + batch_size, len(new_chunks))}/{len(new_chunks)} ä¸ªç‰‡æ®µ")

        session.close()

        print_status(f"æˆåŠŸä¿å­˜ {saved_count} ä¸ªç»“æ„åŒ–è¯¾æ–‡ç‰‡æ®µåˆ°æ•°æ®åº“", "âœ…")
        return True

    except Exception as e:
        print_status(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}", "âŒ")
        if 'session' in locals():
            session.rollback()
            session.close()
        return False


def verify_structured_ingestion():
    """éªŒè¯ç»“æ„åŒ–å…¥åº“ç»“æœ"""
    print_status("éªŒè¯ç»“æ„åŒ–å…¥åº“ç»“æœ", "ğŸ”")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # æŸ¥è¯¢æ€»è®°å½•æ•°
        total_count = session.query(TextbookChunk).count()
        print(f"  ğŸ“Š æ•°æ®åº“ä¸­æ€»è®°å½•æ•°: {total_count}")

        if total_count > 0:
            # æŸ¥è¯¢ç¤ºä¾‹è®°å½•
            sample_chunk = session.query(TextbookChunk).first()
            print(f"  ğŸ“ ç¤ºä¾‹å†…å®¹é•¿åº¦: {len(sample_chunk.content)} å­—ç¬¦")
            embedding_dim = len(sample_chunk.embedding) if hasattr(sample_chunk.embedding, '__len__') else 0
            print(f"  ğŸ”¢ å‘é‡ç»´åº¦: {embedding_dim}")
            print(f"  ğŸ“„ æºæ–‡ä»¶: {sample_chunk.source_file}")
            print(f"  ğŸ¯ è´¨é‡è¯„åˆ†: {sample_chunk.quality_score}")

            # æ˜¾ç¤ºç»“æ„åŒ–å…ƒæ•°æ®
            metadata = sample_chunk.metadata_json or {}
            if metadata.get('unit_number'):
                print(f"  ğŸ“– å•å…ƒè¯¾æ–‡ä¿¡æ¯:")
                print(f"    å•å…ƒ: ç¬¬{metadata.get('unit_number')}å•å…ƒ {metadata.get('unit_title', '')}")
                print(f"    è¯¾æ–‡: ç¬¬{metadata.get('lesson_number')}è¯¾ {metadata.get('lesson_title', '')}")
                print(f"    é¡µé¢: {metadata.get('start_page')}-{metadata.get('end_page')}")
                print(f"    å¤„ç†å™¨: {metadata.get('text_processor', 'unknown')}")

            # æŒ‰å•å…ƒç»Ÿè®¡
            from sqlalchemy import func
            unit_stats = session.query(
                func.count(TextbookChunk.id).label('count')
            ).filter(
                TextbookChunk.metadata_json['unit_number'].astext != ''
            ).all()

            print(f"  ğŸ“š æŒ‰å•å…ƒç»Ÿè®¡: {unit_stats[0].count} ä¸ªç»“æ„åŒ–ç‰‡æ®µ")

        session.close()
        print_status("ç»“æ„åŒ–å…¥åº“éªŒè¯å®Œæˆ", "âœ…")
        return True

    except Exception as e:
        print_status(f"éªŒè¯å¤±è´¥: {e}", "âŒ")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—ï¸ ä½œä¸šæ­å­ RAG ç³»ç»Ÿ - ç»“æ„åŒ–è¯¾æ–‡çŸ¥è¯†åº“å…¥åº“è„šæœ¬")
    print("=" * 60)
    print()

    # é…ç½®å‚æ•°
    data_dir = os.getenv("TEXTBOOK_DIR", "data/textbooks")
    print(f"ğŸ“‚ æ•™æç›®å½•: {data_dir}")
    print("ğŸ“– å¯ç”¨æ™ºèƒ½è¯¾æ–‡ç»“æ„åˆ†æå’Œå•å…ƒ-è¯¾æ–‡å…ƒæ•°æ®å­˜å‚¨")
    print()

    # æ‰§è¡Œå¤„ç†æµç¨‹
    try:
        # æ­¥éª¤1: ç»“æ„åŒ–æ–‡æ¡£åˆ†æ
        structure = process_textbook_structured(data_dir)
        if not structure:
            print("âŒ ç»“æ„åŒ–æ–‡æ¡£åˆ†æ - å¤±è´¥")
            return 1
        print("âœ… ç»“æ„åŒ–æ–‡æ¡£åˆ†æ - é€šè¿‡")

        # æ­¥éª¤2: åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if not initialize_embedding_model():
            print("âŒ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - å¤±è´¥")
            return 1
        print("âœ… åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - é€šè¿‡")

        # æ­¥éª¤3: ç”Ÿæˆç»“æ„åŒ–å‘é‡åµŒå…¥
        embedding_client = create_siliconflow_client()
        lesson_embeddings = generate_structured_embeddings(structure, embedding_client)
        if not lesson_embeddings:
            print("âŒ ç”Ÿæˆç»“æ„åŒ–å‘é‡åµŒå…¥ - å¤±è´¥")
            return 1
        print("âœ… ç”Ÿæˆç»“æ„åŒ–å‘é‡åµŒå…¥ - é€šè¿‡")

        # æ­¥éª¤4: ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°æ•°æ®åº“
        if not save_structured_to_database(lesson_embeddings):
            print("âŒ ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°æ•°æ®åº“ - å¤±è´¥")
            return 1
        print("âœ… ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°æ•°æ®åº“ - é€šè¿‡")

        # æ­¥éª¤5: éªŒè¯å…¥åº“ç»“æœ
        if not verify_structured_ingestion():
            print("âŒ éªŒè¯ç»“æ„åŒ–å…¥åº“ç»“æœ - å¤±è´¥")
            return 1
        print("âœ… éªŒè¯ç»“æ„åŒ–å…¥åº“ç»“æœ - é€šè¿‡")

        passed = 5
        total = 5

    except Exception as e:
        print(f"âŒ å¤„ç†æµç¨‹å¤±è´¥: {e}")
        return 1

    print("\n" + "=" * 60)
    print(f"ğŸ“Š ç»“æ„åŒ–å…¥åº“ç»“æœ: {passed}/{total} æ­¥éª¤å®Œæˆ")

    if passed == total:
        print("ğŸ‰ ç»“æ„åŒ–è¯¾æ–‡çŸ¥è¯†åº“å…¥åº“å®Œæˆ!")
        print("âœ… äººæ•™ç‰ˆè¯­æ–‡æ•™æå·²æŒ‰å•å…ƒ-è¯¾æ–‡ç»“æ„åŒ–å­˜å‚¨")
        print("ğŸ§–ï¸ ä½¿ç”¨äº†æ™ºèƒ½è¯¾æ–‡åˆ†æå’Œç»“æ„åŒ–å…ƒæ•°æ®æŠ€æœ¯")
        print("ğŸ” ç°åœ¨å¯ä»¥è¿›è¡ŒåŸºäºè¯¾æ–‡ç»“æ„çš„ç²¾å‡†è¯­ä¹‰æ£€ç´¢å’Œé—®ç­”")
        return 0
    else:
        print("âš ï¸ ç»“æ„åŒ–è¯¾æ–‡çŸ¥è¯†åº“å…¥åº“æœªå®Œæˆï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1


def initialize_embedding_model():
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
    try:
        client = create_siliconflow_client()

        # æµ‹è¯•è¿æ¥
        test_text = "è¿™æ˜¯ä¸€ç¯‡å…³äºæ˜¥å¤©çš„è¯¾æ–‡"
        embedding = client.embed_query(test_text)

        if len(embedding) != 1024:
            raise ValueError(f"å‘é‡ç»´åº¦é”™è¯¯: {len(embedding)} (æœŸæœ›: 1024)")

        print_status("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ", "âœ…")
        return True

    except Exception as e:
        print_status(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}", "âŒ")
        return False


if __name__ == "__main__":
    sys.exit(main())