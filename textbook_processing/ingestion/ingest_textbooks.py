#!/usr/bin/env python3
"""
çŸ¥è¯†åº“å…¥åº“è„šæœ¬
Textbook Knowledge Ingestion Script for Homework Pal RAG System

ç”¨äºå¤„ç†äººæ•™ç‰ˆæ•™ææ–‡æ¡£ï¼Œç”Ÿæˆå‘é‡åµŒå…¥å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
"""

import sys
import os
from pathlib import Path
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from homeworkpal.database.connection import engine, get_db
from homeworkpal.database.models import TextbookChunk
from homeworkpal.llm.siliconflow import create_siliconflow_client
from homeworkpal.document import create_pdf_processor, create_pdf_splitter
from sqlalchemy.orm import sessionmaker

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def process_textbook_documents(data_dir: str) -> List[Dict[str, Any]]:
    """
    å¤„ç†æ•™ææ–‡æ¡£ï¼ˆåŒ…æ‹¬PDFå’Œå…¶ä»–æ ¼å¼ï¼‰

    Args:
        data_dir: æ•™ææ–‡æ¡£ç›®å½•è·¯å¾„

    Returns:
        å¤„ç†åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"å¤„ç†æ•™ææ–‡æ¡£: {data_dir}", "ğŸ“š")

    all_chunks = []
    data_path = Path(data_dir)

    if not data_path.exists():
        print_status(f"æ•™æç›®å½•ä¸å­˜åœ¨: {data_dir}", "âŒ")
        return all_chunks

    # åˆ›å»ºå¤„ç†å™¨
    pdf_processor = create_pdf_processor()
    text_splitter = create_pdf_splitter(chunk_size=1500, chunk_overlap=200)

    # ä¼˜å…ˆå¤„ç†PDFæ–‡ä»¶
    pdf_files = list(data_path.glob("*.pdf"))
    print(f"ğŸ“„ å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

    for pdf_file in pdf_files:
        try:
            print(f"  ğŸ”„ æ­£åœ¨å¤„ç†: {pdf_file.name}")

            # ä½¿ç”¨PDFå¤„ç†å™¨æå–å†…å®¹
            pdf_result = pdf_processor.extract_text_from_pdf(str(pdf_file))

            # ä½¿ç”¨æ™ºèƒ½åˆ†æ®µå™¨å¤„ç†å†…å®¹
            chunks = text_splitter.split_pdf_content(pdf_result)

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            for chunk in chunks:
                all_chunks.append({
                    'content': chunk['content'],
                    'source': str(pdf_file),
                    'file_name': pdf_file.name,
                    'file_type': 'pdf',
                    'chunk_id': chunk['id'],
                    'page_number': chunk['page_number'],
                    'chunk_index': chunk['chunk_index'],
                    'quality_score': chunk['quality_score'],
                    'metadata': chunk['metadata']
                })

            print(f"  âœ… {pdf_file.name}: ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")

        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥ {pdf_file.name}: {e}")

    # å¤„ç†å…¶ä»–æ ¼å¼æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    other_extensions = ['.md', '.txt']
    for ext in other_extensions:
        for file_path in data_path.glob(f"*{ext}"):
            try:
                print(f"  ğŸ”„ æ­£åœ¨å¤„ç†: {file_path.name}")

                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ç®€å•åˆ†æ®µ
                from langchain.text_splitter import RecursiveCharacterTextSplitter
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1500,
                    chunk_overlap=200,
                    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
                )

                docs = splitter.create_documents([content])

                for i, doc in enumerate(docs):
                    metadata = {
                        'file_name': file_path.name,
                        'file_type': ext,
                        'source': str(file_path),
                        'subject': 'æœªçŸ¥',
                        'grade': 'ä¸‰å¹´çº§',
                        'processed_date': datetime.now().isoformat(),
                        'content_type': 'æ­£æ–‡å†…å®¹'
                    }

                    all_chunks.append({
                        'content': doc.page_content,
                        'source': str(file_path),
                        'file_name': file_path.name,
                        'file_type': ext,
                        'chunk_index': i,
                        'quality_score': 1.0,
                        'metadata': metadata
                    })

                print(f"  âœ… {file_path.name}: ç”Ÿæˆ {len(docs)} ä¸ªç‰‡æ®µ")

            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥ {file_path.name}: {e}")

    print_status(f"å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ", "ğŸ“š")
    return all_chunks






def generate_embeddings(chunks: List[Dict[str, Any]],
                       embedding_client) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆæ–‡æ¡£ç‰‡æ®µçš„å‘é‡åµŒå…¥

    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        embedding_client: åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯

    Returns:
        åŒ…å«åµŒå…¥å‘é‡çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µçš„å‘é‡åµŒå…¥", "ğŸ”")

    # è¿‡æ»¤é«˜è´¨é‡ç‰‡æ®µ
    high_quality_chunks = [chunk for chunk in chunks if chunk.get('quality_score', 1.0) > 0.3]
    print(f"  ğŸ“Š ä» {len(chunks)} ä¸ªç‰‡æ®µä¸­ç­›é€‰å‡º {len(high_quality_chunks)} ä¸ªé«˜è´¨é‡ç‰‡æ®µ")

    if not high_quality_chunks:
        print_status("æ²¡æœ‰é«˜è´¨é‡çš„æ–‡æ¡£ç‰‡æ®µå¯ä¾›å¤„ç†", "âŒ")
        return []

    # æå–æ–‡æœ¬å†…å®¹
    texts = [chunk['content'] for chunk in high_quality_chunks]

    try:
        # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = embedding_client.embed_documents(texts)

        # éªŒè¯åµŒå…¥å‘é‡æ•°é‡å’Œç»´åº¦
        if len(embeddings) != len(high_quality_chunks):
            raise ValueError(f"åµŒå…¥å‘é‡æ•°é‡({len(embeddings)})ä¸ç‰‡æ®µæ•°é‡({len(high_quality_chunks)})ä¸åŒ¹é…")

        expected_dim = 1024  # BGE-M3çš„ç»´åº¦
        for i, embedding in enumerate(embeddings):
            if len(embedding) != expected_dim:
                print(f"  âš ï¸ ç‰‡æ®µ {i} å‘é‡ç»´åº¦ä¸æ­£ç¡®: {len(embedding)} (æœŸæœ›: {expected_dim})")

        # å°†åµŒå…¥å‘é‡æ·»åŠ åˆ°ç‰‡æ®µä¸­
        for i, chunk in enumerate(high_quality_chunks):
            chunk['embedding'] = embeddings[i]
            # æ·»åŠ å†…å®¹å“ˆå¸Œç”¨äºå»é‡
            chunk['content_hash'] = hashlib.md5(chunk['content'].encode('utf-8')).hexdigest()

        print_status(f"æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡åµŒå…¥", "âœ…")
        return high_quality_chunks

    except Exception as e:
        print_status(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}", "âŒ")
        return []


def save_to_database(chunks: List[Dict[str, Any]],
                     batch_size: int = 10) -> bool:
    """
    å°†æ–‡æ¡£ç‰‡æ®µä¿å­˜åˆ°æ•°æ®åº“

    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    print_status(f"ä¿å­˜ {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°æ•°æ®åº“", "ğŸ’¾")

    try:
        # åˆ›å»ºæ•°æ®åº“ä¼šè¯
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # å»é‡ï¼šæ£€æŸ¥å†…å®¹å“ˆå¸Œæ˜¯å¦å·²å­˜åœ¨
        existing_hashes = set()
        existing_records = session.query(TextbookChunk.content_hash).filter(
            TextbookChunk.content_hash.in_([chunk.get('content_hash', '') for chunk in chunks])
        ).all()
        existing_hashes = set(record[0] for record in existing_records)

        # è¿‡æ»¤æ–°ç‰‡æ®µ
        new_chunks = [chunk for chunk in chunks if chunk.get('content_hash', '') not in existing_hashes]

        if not new_chunks:
            print_status("æ‰€æœ‰ç‰‡æ®µéƒ½å·²å­˜åœ¨äºæ•°æ®åº“ä¸­", "âœ…")
            session.close()
            return True

        print(f"  ğŸ“Š è¿‡æ»¤é‡å¤åæ–°å¢ {len(new_chunks)} ä¸ªç‰‡æ®µ")

        # æ‰¹é‡ä¿å­˜
        saved_count = 0
        for i in range(0, len(new_chunks), batch_size):
            batch = new_chunks[i:i + batch_size]

            for chunk in batch:
                # åˆ›å»ºTextbookChunkå¯¹è±¡
                textbook_chunk = TextbookChunk(
                    content=chunk['content'],
                    embedding=chunk['embedding'],
                    metadata_json=chunk['metadata'],
                    source_file=chunk['source'],
                    chunk_index=chunk['chunk_index'],
                    content_hash=chunk.get('content_hash', hashlib.md5(chunk['content'].encode('utf-8')).hexdigest()),
                    page_number=chunk.get('page_number'),
                    quality_score=chunk.get('quality_score', 1.0)
                )

                session.add(textbook_chunk)
                saved_count += 1

            # æäº¤æ‰¹æ¬¡
            session.commit()
            print(f"  âœ… å·²ä¿å­˜ {min(i + batch_size, len(new_chunks))}/{len(new_chunks)} ä¸ªç‰‡æ®µ")

        session.close()

        print_status(f"æˆåŠŸä¿å­˜ {saved_count} ä¸ªæ–°æ–‡æ¡£ç‰‡æ®µåˆ°æ•°æ®åº“", "âœ…")
        return True

    except Exception as e:
        print_status(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}", "âŒ")
        if 'session' in locals():
            session.rollback()
            session.close()
        return False


def verify_ingestion():
    """
    éªŒè¯å…¥åº“ç»“æœ
    """
    print_status("éªŒè¯å…¥åº“ç»“æœ", "ğŸ”")

    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()

        # æŸ¥è¯¢æ€»è®°å½•æ•°
        total_count = session.query(TextbookChunk).count()
        print(f"  ğŸ“Š æ•°æ®åº“ä¸­æ€»è®°å½•æ•°: {total_count}")

        if total_count > 0:
            # æŸ¥è¯¢ç¤ºä¾‹è®°å½•
            sample_chunk = session.query(TextbookChunk).first()
            print(f"  ğŸ“ ç¤ºä¾‹å†…å®¹é•¿åº¦: {len(sample_chunk.content)} å­—ç¬¦")
            embedding_dim = len(sample_chunk.embedding) if hasattr(sample_chunk.embedding, '__len__') else 0
            print(f"  ğŸ”¢ å‘é‡ç»´åº¦: {embedding_dim}")
            print(f"  ğŸ“„ æºæ–‡ä»¶: {sample_chunk.source_file}")
            print(f"  ğŸ“‹ å…ƒæ•°æ®: {json.dumps(sample_chunk.metadata_json, ensure_ascii=False, indent=2)}")

        session.close()
        print_status("å…¥åº“éªŒè¯å®Œæˆ", "âœ…")
        return True

    except Exception as e:
        print_status(f"éªŒè¯å¤±è´¥: {e}", "âŒ")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ä½œä¸šæ­å­ RAG ç³»ç»Ÿ - çŸ¥è¯†åº“å…¥åº“è„šæœ¬")
    print("=" * 60)
    print()

    # é…ç½®å‚æ•°
    data_dir = os.getenv("TEXTBOOK_DIR", "data/textbooks")
    chunk_size = int(os.getenv("CHUNK_SIZE", "1000"))
    chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "200"))

    print(f"ğŸ“‚ æ•™æç›®å½•: {data_dir}")
    print(f"ğŸ“ ç‰‡æ®µå¤§å°: {chunk_size}")
    print(f"ğŸ”„ ç‰‡æ®µé‡å : {chunk_overlap}")
    print()

    # æ‰§è¡Œå¤„ç†æµç¨‹
    try:
        # æ­¥éª¤1: å¤„ç†æ•™ææ–‡æ¡£
        chunks = process_textbook_documents(data_dir)
        if not chunks:
            print("âŒ å¤„ç†æ•™ææ–‡æ¡£ - å¤±è´¥")
            return 1
        print("âœ… å¤„ç†æ•™ææ–‡æ¡£ - é€šè¿‡")

        # æ­¥éª¤2: åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if not initialize_embedding_model():
            print("âŒ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - å¤±è´¥")
            return 1
        print("âœ… åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - é€šè¿‡")

        # æ­¥éª¤3: ç”Ÿæˆå‘é‡åµŒå…¥
        embedding_client = create_siliconflow_client()
        embedded_chunks = generate_embeddings(chunks, embedding_client)
        if not embedded_chunks:
            print("âŒ ç”Ÿæˆå‘é‡åµŒå…¥ - å¤±è´¥")
            return 1
        print("âœ… ç”Ÿæˆå‘é‡åµŒå…¥ - é€šè¿‡")

        # æ­¥éª¤4: ä¿å­˜åˆ°æ•°æ®åº“
        if not save_to_database(embedded_chunks):
            print("âŒ ä¿å­˜åˆ°æ•°æ®åº“ - å¤±è´¥")
            return 1
        print("âœ… ä¿å­˜åˆ°æ•°æ®åº“ - é€šè¿‡")

        # æ­¥éª¤5: éªŒè¯å…¥åº“ç»“æœ
        if not verify_ingestion():
            print("âŒ éªŒè¯å…¥åº“ç»“æœ - å¤±è´¥")
            return 1
        print("âœ… éªŒè¯å…¥åº“ç»“æœ - é€šè¿‡")

        passed = 5
        total = 5

    except Exception as e:
        print(f"âŒ å¤„ç†æµç¨‹å¤±è´¥: {e}")
        return 1

    print("\n" + "=" * 60)
    print(f"ğŸ“Š å…¥åº“ç»“æœ: {passed}/{total} æ­¥éª¤å®Œæˆ")

    if passed == total:
        print("ğŸ‰ çŸ¥è¯†åº“å…¥åº“å®Œæˆ!")
        print("âœ… äººæ•™ç‰ˆæ•™æå·²æˆåŠŸå‘é‡åŒ–å¹¶å­˜å‚¨åˆ°æ•°æ®åº“")
        print("ğŸ” ç°åœ¨å¯ä»¥è¿›è¡Œè¯­ä¹‰æ£€ç´¢å’Œé—®ç­”")
        return 0
    else:
        print("âš ï¸ çŸ¥è¯†åº“å…¥åº“æœªå®Œæˆï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1


def initialize_embedding_model():
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
    try:
        client = create_siliconflow_client()

        # æµ‹è¯•è¿æ¥
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        embedding = client.embed_query(test_text)

        if len(embedding) != 1024:
            raise ValueError(f"å‘é‡ç»´åº¦é”™è¯¯: {len(embedding)} (æœŸæœ›: 1024)")

        print_status("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ", "âœ…")
        return True

    except Exception as e:
        print_status(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}", "âŒ")
        return False


if __name__ == "__main__":
    sys.exit(main())