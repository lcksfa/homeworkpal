#!/usr/bin/env python3
"""
è¯­æ–‡çŸ¥è¯†åº“æœç´¢æµ‹è¯•è„šæœ¬
Chinese Knowledge Base Search Test Script
"""

import sys
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

from homeworkpal.database.connection import SessionLocal
from sqlalchemy import text
import hashlib

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")

def create_simple_query_embedding(query: str) -> List[float]:
    """
    ä¸ºæŸ¥è¯¢åˆ›å»ºç®€å•çš„åµŒå…¥å‘é‡ï¼ˆä¸å¤„ç†è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    """
    hash_obj = hashlib.md5(query.encode('utf-8'))
    hash_hex = hash_obj.hexdigest()

    # å°†å“ˆå¸Œå€¼è½¬æ¢ä¸º1024ç»´å‘é‡
    vector = []
    for i in range(0, len(hash_hex), 2):
        hex_pair = hash_hex[i:i+2]
        value = int(hex_pair, 16) / 255.0 - 0.5  # å½’ä¸€åŒ–åˆ°[-0.5, 0.5]
        vector.extend([value] * 64)  # æ¯ä¸ªå­—èŠ‚æ‰©å±•ä¸º64ä¸ªå€¼

    # ç¡®ä¿å‘é‡é•¿åº¦ä¸º1024
    while len(vector) < 1024:
        vector.append(0.0)

    return vector[:1024]

def simple_vector_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    ç®€å•çš„å‘é‡ç›¸ä¼¼æ€§æœç´¢
    """
    print_status(f"æœç´¢æŸ¥è¯¢: '{query}'", "ğŸ”")

    try:
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = create_simple_query_embedding(query)

        session = SessionLocal()

        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œç®€å•çš„å‘é‡æœç´¢
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨pgvectorçš„å†…ç½®å‡½æ•°
        result = session.execute(text('''
            SELECT
                content,
                page_number,
                chunk_index,
                metadata_json,
                embedding,
                CASE
                    WHEN embedding IS NOT NULL THEN
                        1.0 - ABS(SUM(ABS(
                            (embedding[:1]::vector + embedding[1:2]::vector + embedding[2:3]::vector) -
                            (:query_vec[:1]::vector + :query_vec[1:2]::vector + :query_vec[2:3]::vector)
                        )) / 3.0)
                    ELSE 0.0
                END as similarity_score
            FROM textbook_chunks
            WHERE metadata_json->>'subject' = 'è¯­æ–‡'
            AND embedding IS NOT NULL
            GROUP BY content, page_number, chunk_index, metadata_json, embedding
            ORDER BY similarity_score DESC, page_number, chunk_index
            LIMIT :limit
        '''), {
            'query_vec': query_embedding,
            'limit': limit
        })

        results = []
        for row in result.fetchall():
            results.append({
                'content': row.content,
                'page_number': row.page_number,
                'chunk_index': row.chunk_index,
                'metadata': row.metadata_json,
                'similarity_score': row.similarity_score
            })

        session.close()

        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç‰‡æ®µ")
        return results

    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        return []

def search_by_keyword(keyword: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    åŸºäºå…³é”®è¯çš„æœç´¢
    """
    print_status(f"å…³é”®è¯æœç´¢: '{keyword}'", "ğŸ”")

    try:
        session = SessionLocal()

        result = session.execute(text('''
            SELECT
                content,
                page_number,
                chunk_index,
                metadata_json,
                CASE
                    WHEN content ILIKE '%' || :keyword || '%' THEN 1.0
                    WHEN metadata_json ILIKE '%' || :keyword || '%' THEN 0.5
                    ELSE 0.0
                END as keyword_score
            FROM textbook_chunks
            WHERE metadata_json->>'subject' = 'è¯­æ–‡'
            AND (content ILIKE '%' || :keyword || '%' OR metadata_json ILIKE '%' || :keyword || '%')
            ORDER BY keyword_score DESC, page_number, chunk_index
            LIMIT :limit
        '''), {
            'keyword': keyword,
            'limit': limit
        })

        results = []
        for row in result.fetchall():
            results.append({
                'content': row.content,
                'page_number': row.page_number,
                'chunk_index': row.chunk_index,
                'metadata': row.metadata_json,
                'keyword_score': row.keyword_score
            })

        session.close()

        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç‰‡æ®µ")
        return results

    except Exception as e:
        print(f"âŒ å…³é”®è¯æœç´¢å¤±è´¥: {e}")
        return []

def display_results(results: List[Dict[str, Any]], search_type: str):
    """
    æ˜¾ç¤ºæœç´¢ç»“æœ
    """
    print(f"\nğŸ“‹ {search_type}æœç´¢ç»“æœ:")
    print("=" * 60)

    if not results:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
        return

    for i, result in enumerate(results, 1):
        print(f"\nğŸ” ç»“æœ {i}:")
        print(f"   ğŸ“„ é¡µç : {result['page_number']}")
        print(f"   ğŸ“ ç‰‡æ®µ: {result['chunk_index']}")
        print(f"   ğŸ“Š ç›¸å…³æ€§: {result.get('similarity_score', result.get('keyword_score', 0)):.3f}")

        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        content = result['content']
        preview = content[:200] + '...' if len(content) > 200 else content
        print(f"   ğŸ“– å†…å®¹é¢„è§ˆ:")
        print(f"      {preview}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ è¯­æ–‡çŸ¥è¯†åº“æœç´¢æµ‹è¯•")
    print("=" * 60)
    print()

    # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
    test_queries = [
        "æˆ‘ä»¬çš„å­¦æ ¡",
        "è€å¸ˆ",
        "å­¦ä¹ ",
        "è¯¾æ–‡",
        "ç”Ÿå­—"
    ]

    test_keywords = [
        "å­¦æ ¡",
        "è€å¸ˆ",
        "å­¦ä¹ ",
        "è¯¾æ–‡",
        "ç»ƒä¹ "
    ]

    try:
        # æ‰§è¡Œå‘é‡æœç´¢æµ‹è¯•
        print("ğŸš€ å‘é‡æœç´¢æµ‹è¯•")
        print("-" * 40)
        for query in test_queries:
            results = simple_vector_search(query, limit=3)
            display_results(results, f"å‘é‡æœç´¢ - {query}")
            print()

        # æ‰§è¡Œå…³é”®è¯æœç´¢æµ‹è¯•
        print("\nğŸš€ å…³é”®è¯æœç´¢æµ‹è¯•")
        print("-" * 40)
        for keyword in test_keywords:
            results = search_by_keyword(keyword, limit=3)
            display_results(results, f"å…³é”®è¯æœç´¢ - {keyword}")
            print()

        print("=" * 60)
        print("ğŸ‰ è¯­æ–‡çŸ¥è¯†åº“æœç´¢æµ‹è¯•å®Œæˆ!")
        print("âœ… æœç´¢åŠŸèƒ½åŸºæœ¬å¯ç”¨")
        print("âš ï¸  æ³¨æ„ï¼šå½“å‰ä½¿ç”¨ç®€åŒ–ç®—æ³•ï¼Œå®é™…éƒ¨ç½²æ—¶éœ€è¦ä½¿ç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹")

        return 0

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())