#!/usr/bin/env python3
"""
RAGæ£€ç´¢åŠŸèƒ½æµ‹è¯•è„šæœ¬
RAG Search Functionality Test Script

æµ‹è¯•å‘é‡åŒ–å¯¼å…¥åçš„æ£€ç´¢åŠŸèƒ½ï¼ŒéªŒè¯æ•°æ®è´¨é‡å’Œæ£€ç´¢æ•ˆæœ
"""

import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

from homeworkpal.database.connection import engine
from sqlalchemy.orm import sessionmaker
from homeworkpal.database.models import TextbookChunk
from homeworkpal.llm.base import BaseEmbeddingModel
from homeworkpal.llm.siliconflow import SiliconFlowEmbeddingModel
import numpy as np

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾", "ğŸ“–": "ğŸ“–"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def get_database_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()
        return session
    except Exception as e:
        print_status(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}", "âŒ")
        return None


def get_embedding_model():
    """è·å–åµŒå…¥æ¨¡å‹"""
    try:
        api_key = os.getenv("SILICONFLOW_API_KEY")
        base_url = os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1")

        if not api_key:
            print_status("æœªè®¾ç½®SILICONFLOW_API_KEYç¯å¢ƒå˜é‡", "âš ï¸")
            return None

        # åˆ›å»ºå¸¦æœ‰æ›´é•¿è¶…æ—¶æ—¶é—´çš„åµŒå…¥æ¨¡å‹
        embedding_model = SiliconFlowEmbeddingModel(
            api_key=api_key,
            base_url=base_url
        )

        # å¦‚æœæ¨¡å‹æ”¯æŒè¶…æ—¶è®¾ç½®ï¼Œå¯ä»¥åœ¨è¿™é‡Œé…ç½®
        if hasattr(embedding_model, 'timeout'):
            embedding_model.timeout = 60  # å¢åŠ åˆ°60ç§’

        print_status("æˆåŠŸåŠ è½½åµŒå…¥æ¨¡å‹", "âœ…")
        return embedding_model
    except Exception as e:
        print_status(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}", "âŒ")
        return None


def cosine_similarity(vec1, vec2) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(vec1, str):
            vec1 = eval(vec1) if vec1.startswith('[') else []
        if isinstance(vec2, str):
            vec2 = eval(vec2) if vec2.startswith('[') else []

        # å°†å‘é‡è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œç¡®ä¿æ˜¯floatç±»å‹
        vec1_array = np.array(vec1, dtype=float)
        vec2_array = np.array(vec2, dtype=float)

        # æ£€æŸ¥å‘é‡æœ‰æ•ˆæ€§
        if vec1_array.size == 0 or vec2_array.size == 0:
            return 0.0

        # è®¡ç®—ç‚¹ç§¯å’Œæ¨¡é•¿
        dot_product = np.dot(vec1_array, vec2_array)
        norm1 = np.linalg.norm(vec1_array)
        norm2 = np.linalg.norm(vec2_array)

        # é¿å…é™¤é›¶
        if norm1 == 0.0 or norm2 == 0.0:
            return 0.0

        # è¿”å›ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = float(dot_product / (norm1 * norm2))
        return similarity
    except Exception as e:
        print_status(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}", "âš ï¸")
        return 0.0


def test_database_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥å’ŒåŸºç¡€æ•°æ®"""
    print_status("æµ‹è¯•æ•°æ®åº“è¿æ¥", "ğŸ”§")

    session = get_database_connection()
    if not session:
        return False

    try:
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„è®°å½•æ•°
        total_count = session.query(TextbookChunk).count()
        print_status(f"æ•°æ®åº“ä¸­å…±æœ‰ {total_count} æ¡è®°å½•", "ğŸ“Š")

        if total_count == 0:
            print_status("æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•ï¼Œè¯·å…ˆè¿è¡Œå¯¼å…¥è„šæœ¬", "âš ï¸")
            session.close()
            return False

        # æ£€æŸ¥æœ‰å‘é‡çš„è®°å½•æ•°
        vector_count = session.query(TextbookChunk).filter(
            TextbookChunk.embedding.isnot(None)
        ).count()
        print_status(f"æœ‰å‘é‡åµŒå…¥çš„è®°å½•æ•°: {vector_count}", "ğŸ“Š")

        # æŸ¥çœ‹å‰å‡ æ¡è®°å½•çš„å…ƒæ•°æ®
        sample_records = session.query(TextbookChunk).limit(3).all()
        print_status("æŸ¥çœ‹æ ·æœ¬è®°å½•å…ƒæ•°æ®:", "ğŸ“–")
        for i, record in enumerate(sample_records):
            metadata = record.metadata_json or {}
            content_type = metadata.get('content_type', 'æœªçŸ¥')
            content_category = metadata.get('content_category', 'æœªåˆ†ç±»')
            content_preview = record.content[:50] + "..." if len(record.content) > 50 else record.content
            print(f"  è®°å½•{i+1}: é¡µç {record.page_number}, ç±»å‹={content_type}, åˆ†ç±»={content_category}")
            print(f"    å†…å®¹é¢„è§ˆ: {content_preview}")

        session.close()
        return True

    except Exception as e:
        print_status(f"æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}", "âŒ")
        session.close()
        return False


def test_content_categories():
    """æµ‹è¯•å†…å®¹åˆ†ç±»ç»Ÿè®¡"""
    print_status("æµ‹è¯•å†…å®¹åˆ†ç±»ç»Ÿè®¡", "ğŸ“Š")

    session = get_database_connection()
    if not session:
        return False

    try:
        # ç»Ÿè®¡å„ç§å†…å®¹ç±»å‹
        categories = {}
        records = session.query(TextbookChunk).all()

        for record in records:
            metadata = record.metadata_json or {}
            content_category = metadata.get('content_category', 'æœªåˆ†ç±»')
            content_type = metadata.get('content_type', 'æœªçŸ¥')

            key = f"{content_category} ({content_type})"
            categories[key] = categories.get(key, 0) + 1

        print_status("å†…å®¹åˆ†ç±»ç»Ÿè®¡:", "ğŸ“‹")
        for category, count in sorted(categories.items()):
            print(f"  {category}: {count} æ¡")

        session.close()
        return True

    except Exception as e:
        print_status(f"åˆ†ç±»ç»Ÿè®¡å¤±è´¥: {e}", "âŒ")
        session.close()
        return False


def test_semantic_search(embedding_model):
    """æµ‹è¯•è¯­ä¹‰æœç´¢åŠŸèƒ½"""
    print_status("æµ‹è¯•è¯­ä¹‰æœç´¢åŠŸèƒ½", "ğŸ”")

    session = get_database_connection()
    if not session:
        return False

    try:
        # å®šä¹‰æµ‹è¯•é—®é¢˜
        test_queries = [
            "å¤è¯—ä¸‰é¦–",
            "å¬å¬ç§‹çš„å£°éŸ³",
            "å¤è¯—å±±è¡Œ",
            "å£è¯­äº¤é™…æš‘å‡ç”Ÿæ´»",
            "æ³°æˆˆå°”èŠ±çš„å­¦æ ¡",
            "ä¸æ‡‚å°±è¦é—®å­™ä¸­å±±"
        ]

        print_status("æ‰§è¡Œè¯­ä¹‰æœç´¢æµ‹è¯•:", "ğŸ”")
        # è·å–æ‰€æœ‰æœ‰å‘é‡çš„è®°å½•ï¼ˆä¸€æ¬¡æ€§è·å–ä»¥æé«˜æ•ˆç‡ï¼‰
        records = session.query(TextbookChunk).filter(
            TextbookChunk.embedding.isnot(None)
        ).all()

        successful_searches = 0
        total_searches = len(test_queries)

        for query in test_queries:
            print(f"\nğŸ“ æœç´¢é—®é¢˜: '{query}'")

            # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶
            query_embedding = None
            max_retries = 2
            for attempt in range(max_retries + 1):
                try:
                    if attempt > 0:
                        print_status(f"é‡è¯•ç”ŸæˆæŸ¥è¯¢å‘é‡ (å°è¯• {attempt + 1}/{max_retries + 1})", "ğŸ”„")
                    query_embedding = embedding_model.embed_query(query)
                    if query_embedding:
                        break
                except Exception as e:
                    print_status(f"ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)[:100]}...", "âš ï¸")
                    if attempt < max_retries:
                        continue
                    else:
                        print_status(f"è·³è¿‡æŸ¥è¯¢ '{query}' - æ‰€æœ‰é‡è¯•å‡å¤±è´¥", "âŒ")
                        break

            # åªæœ‰æˆåŠŸç”Ÿæˆå‘é‡æ—¶æ‰è¿›è¡Œæœç´¢
            if query_embedding is None:
                print_status(f"æ— æ³•ä¸ºæŸ¥è¯¢ '{query}' ç”Ÿæˆå‘é‡ï¼Œè·³è¿‡æœç´¢", "âš ï¸")
                continue

            # è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
            similarities = []
            for record in records:
                try:
                    # æ£€æŸ¥å‘é‡æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
                    if record.embedding is None:
                        continue

                    # å¤„ç†å‘é‡æ ¼å¼
                    embedding = record.embedding
                    if isinstance(embedding, str):
                        embedding = eval(embedding) if embedding.startswith('[') else []

                    # ç¡®ä¿å‘é‡ä¸ä¸ºç©º
                    if embedding is None or (isinstance(embedding, (list, tuple)) and len(embedding) == 0):
                        continue

                    similarity = cosine_similarity(query_embedding, embedding)
                    similarities.append((similarity, record))
                except Exception as e:
                    print_status(f"å¤„ç†è®°å½• {getattr(record, 'id', 'æœªçŸ¥')} å¤±è´¥: {e}", "âš ï¸")
                    continue

            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            similarities.sort(key=lambda x: x[0], reverse=True)

            # æ˜¾ç¤ºå‰3ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
            print(f"  æ‰¾åˆ° {len(records)} æ¡è®°å½•ï¼Œæ˜¾ç¤ºå‰3ä¸ªæœ€ç›¸å…³ç»“æœ:")
            for i, (similarity, record) in enumerate(similarities[:3]):
                metadata = record.metadata_json or {}
                content_type = metadata.get('content_type', 'æœªçŸ¥')
                page_number = record.page_number
                content_preview = record.content[:80] + "..." if len(record.content) > 80 else record.content

                print(f"    {i+1}. ç›¸ä¼¼åº¦: {similarity:.4f} | é¡µç : {page_number} | ç±»å‹: {content_type}")
                print(f"       å†…å®¹: {content_preview}")

            # å¦‚æœæ‰¾åˆ°äº†ç›¸ä¼¼çš„ç»“æœï¼Œå¢åŠ æˆåŠŸè®¡æ•°
            if similarities:
                successful_searches += 1

        # æ˜¾ç¤ºæœç´¢æˆåŠŸç‡æŠ¥å‘Š
        print_status(f"è¯­ä¹‰æœç´¢å®Œæˆ: {successful_searches}/{total_searches} ä¸ªæŸ¥è¯¢æˆåŠŸ", "ğŸ“Š")

        session.close()
        # å¦‚æœè‡³å°‘æœ‰ä¸€åŠçš„æŸ¥è¯¢æˆåŠŸï¼Œè®¤ä¸ºæµ‹è¯•é€šè¿‡
        return successful_searches >= total_searches // 2

    except Exception as e:
        print_status(f"è¯­ä¹‰æœç´¢æµ‹è¯•å¤±è´¥: {e}", "âŒ")
        session.close()
        return False


def test_category_specific_search(embedding_model):
    """æµ‹è¯•æŒ‰åˆ†ç±»æœç´¢"""
    print_status("æµ‹è¯•æŒ‰åˆ†ç±»æœç´¢", "ğŸ”")

    session = get_database_connection()
    if not session:
        return False

    try:
        # æµ‹è¯•æŒ‰ä¸åŒåˆ†ç±»æœç´¢
        category_tests = [
            ("è¯¾æ–‡ä¸»ä½“", "å¤§é’æ ‘ä¸‹çš„å°å­¦"),
            ("ä¹ ä½œæŒ‡å¯¼", "å†™æ—¥è®°"),
            ("è¯¾åç»ƒä¹ ", "æœ—è¯»è¯¾æ–‡"),
            ("æ—¥ç§¯æœˆç´¯", "å±±è¡Œ"),
            ("å£è¯­äº¤é™…", "æš‘å‡ç”Ÿæ´»")
        ]

        for category, keyword in category_tests:
            print(f"\nğŸ“‚ æµ‹è¯•åˆ†ç±»: {category}")

            # æŸ¥è¯¢æŒ‡å®šåˆ†ç±»çš„è®°å½•
            records = session.query(TextbookChunk).filter(
                TextbookChunk.embedding.isnot(None)
            ).all()

            # ç­›é€‰æŒ‡å®šåˆ†ç±»çš„è®°å½•
            category_records = []
            for record in records:
                metadata = record.metadata_json or {}
                content_type = metadata.get('content_type', '')
                # ç¡®ä¿content_typeæ˜¯å­—ç¬¦ä¸²
                if content_type and isinstance(content_type, str) and category in content_type:
                    category_records.append(record)

            print(f"  æ‰¾åˆ° {len(category_records)} æ¡ '{category}' è®°å½•")

            # ç”Ÿæˆå…³é”®è¯å‘é‡å¹¶æœç´¢
            if category_records:
                # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶
                query_embedding = None
                max_retries = 2
                for attempt in range(max_retries + 1):
                    try:
                        if attempt > 0:
                            print_status(f"é‡è¯•ç”Ÿæˆåˆ†ç±»æŸ¥è¯¢å‘é‡ (å°è¯• {attempt + 1}/{max_retries + 1})", "ğŸ”„")
                        query_embedding = embedding_model.embed_query(keyword)
                        if query_embedding:
                            break
                    except Exception as e:
                        print_status(f"ç”Ÿæˆåˆ†ç±»æŸ¥è¯¢å‘é‡å¤±è´¥ (å°è¯• {attempt + 1}): {str(e)[:100]}...", "âš ï¸")
                        if attempt < max_retries:
                            continue
                        else:
                            print_status(f"è·³è¿‡åˆ†ç±» '{category}' çš„æœç´¢ - æ‰€æœ‰é‡è¯•å‡å¤±è´¥", "âŒ")
                            break

                if query_embedding is None:
                    print_status(f"æ— æ³•ä¸ºåˆ†ç±» '{category}' ç”Ÿæˆå‘é‡ï¼Œè·³è¿‡æœç´¢", "âš ï¸")
                    continue

                similarities = []

                for record in category_records:
                    try:
                        # æ£€æŸ¥å‘é‡æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
                        if record.embedding is None:
                            continue

                        # å¤„ç†å‘é‡æ ¼å¼
                        embedding = record.embedding
                        if isinstance(embedding, str):
                            embedding = eval(embedding) if embedding.startswith('[') else []

                        # ç¡®ä¿å‘é‡ä¸ä¸ºç©º
                        if embedding is None or (isinstance(embedding, (list, tuple)) and len(embedding) == 0):
                            continue

                        similarity = cosine_similarity(query_embedding, embedding)
                        similarities.append((similarity, record))
                    except Exception as e:
                        print_status(f"å¤„ç†åˆ†ç±»è®°å½• {getattr(record, 'id', 'æœªçŸ¥')} å¤±è´¥: {e}", "âš ï¸")
                        continue

                similarities.sort(key=lambda x: x[0], reverse=True)

                # æ˜¾ç¤ºæœ€ç›¸å…³çš„ç»“æœ
                if similarities:
                    best_similarity, best_record = similarities[0]
                    metadata = best_record.metadata_json or {}
                    page_number = best_record.page_number
                    content_preview = best_record.content[:60] + "..." if len(best_record.content) > 60 else best_record.content

                    print(f"  æœ€ç›¸å…³è®°å½•: ç›¸ä¼¼åº¦ {best_similarity:.4f} | é¡µç  {page_number}")
                    print(f"  å†…å®¹: {content_preview}")
            else:
                print(f"  æ²¡æœ‰æ‰¾åˆ° '{category}' ç±»å‹çš„è®°å½•")

        session.close()
        return True

    except Exception as e:
        print_status(f"åˆ†ç±»æœç´¢æµ‹è¯•å¤±è´¥: {e}", "âŒ")
        session.close()
        return False


def test_lesson_structure():
    """æµ‹è¯•è¯¾ç¨‹ç»“æ„ä¿¡æ¯"""
    print_status("æµ‹è¯•è¯¾ç¨‹ç»“æ„ä¿¡æ¯", "ğŸ“š")

    session = get_database_connection()
    if not session:
        return False

    try:
        # è·å–æ‰€æœ‰è®°å½•
        records = session.query(TextbookChunk).all()

        # ç»Ÿè®¡å•å…ƒä¿¡æ¯
        units = {}
        lessons = {}

        for record in records:
            metadata = record.metadata_json or {}
            unit_title = metadata.get('unit_title', '')
            lesson_title = metadata.get('lesson_title', '')
            page_number = record.page_number

            if unit_title:
                if unit_title not in units:
                    units[unit_title] = {'count': 0, 'pages': set()}
                units[unit_title]['count'] += 1
                units[unit_title]['pages'].add(page_number)

            if lesson_title:
                if lesson_title not in lessons:
                    lessons[lesson_title] = {'count': 0, 'pages': set()}
                lessons[lesson_title]['count'] += 1
                lessons[lesson_title]['pages'].add(page_number)

        print_status("å•å…ƒç»“æ„ç»Ÿè®¡:", "ğŸ“–")
        for unit_title, data in sorted(units.items()):
            pages = sorted(list(data['pages']))
            page_range = f"{min(pages)}-{max(pages)}" if len(pages) > 1 else str(pages[0])
            print(f"  {unit_title}: {data['count']} ä¸ªç‰‡æ®µ, é¡µç èŒƒå›´: {page_range}")

        print_status("\nè¯¾æ–‡ç»“æ„ç»Ÿè®¡:", "ğŸ“–")
        for lesson_title, data in sorted(lessons.items()):
            pages = sorted(list(data['pages']))
            page_range = f"{min(pages)}-{max(pages)}" if len(pages) > 1 else str(pages[0])
            print(f"  {lesson_title}: {data['count']} ä¸ªç‰‡æ®µ, é¡µç èŒƒå›´: {page_range}")

        session.close()
        return True

    except Exception as e:
        print_status(f"è¯¾ç¨‹ç»“æ„æµ‹è¯•å¤±è´¥: {e}", "âŒ")
        session.close()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ RAGæ£€ç´¢åŠŸèƒ½æµ‹è¯•è„šæœ¬")
    print("=" * 60)

    # æµ‹è¯•1: æ•°æ®åº“è¿æ¥
    if not test_database_connection():
        print("\nâŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯¼å…¥æ˜¯å¦å®Œæˆ")
        return 1

    # æµ‹è¯•2: å†…å®¹åˆ†ç±»ç»Ÿè®¡
    if not test_content_categories():
        print("\nâŒ å†…å®¹åˆ†ç±»æµ‹è¯•å¤±è´¥")
        return 1

    # æµ‹è¯•3: è¯¾ç¨‹ç»“æ„
    if not test_lesson_structure():
        print("\nâŒ è¯¾ç¨‹ç»“æ„æµ‹è¯•å¤±è´¥")
        return 1

    # è·å–åµŒå…¥æ¨¡å‹
    embedding_model = get_embedding_model()
    if not embedding_model:
        print("\nâŒ æ— æ³•åŠ è½½åµŒå…¥æ¨¡å‹ï¼Œè·³è¿‡è¯­ä¹‰æœç´¢æµ‹è¯•")
        print("ğŸ” ä½†æ•°æ®åº“è¿æ¥å’ŒåŸºç¡€æµ‹è¯•å·²å®Œæˆ")
        return 0

    # æµ‹è¯•4: è¯­ä¹‰æœç´¢
    if not test_semantic_search(embedding_model):
        print("\nâš ï¸ è¯­ä¹‰æœç´¢æµ‹è¯•å¤±è´¥ï¼Œä½†ç»§ç»­å…¶ä»–æµ‹è¯•")

    # æµ‹è¯•5: æŒ‰åˆ†ç±»æœç´¢
    if not test_category_specific_search(embedding_model):
        print("\nâš ï¸ åˆ†ç±»æœç´¢æµ‹è¯•å¤±è´¥ï¼Œä½†åŸºç¡€åŠŸèƒ½å·²å®Œæˆ")

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼RAGç³»ç»Ÿè¿è¡Œæ­£å¸¸")
    print("ğŸ’¡ ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œæ™ºèƒ½æ£€ç´¢")

    return 0


if __name__ == "__main__":
    sys.exit(main())