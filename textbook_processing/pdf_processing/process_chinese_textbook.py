#!/usr/bin/env python3
"""
è¯­æ–‡ä¸‰ä¸Š PDF å¤„ç†å’Œå‘é‡åŒ–è„šæœ¬
Chinese Grade 3 Textbook PDF Processing and Vectorization Script
"""

import sys
import os
from pathlib import Path
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from homeworkpal.database.connection import engine, SessionLocal
from homeworkpal.database.models import TextbookChunk
from sqlalchemy.orm import sessionmaker

def print_status(message: str, status: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    icons = {"âœ…": "âœ…", "âŒ": "âŒ", "âš ï¸": "âš ï¸", "ğŸ”§": "ğŸ”§", "ğŸ“š": "ğŸ“š", "ğŸ”": "ğŸ”", "ğŸ’¾": "ğŸ’¾"}
    print(f"{icons.get(status, 'â„¹ï¸')} {message}")


def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨PyMuPDFæå–PDFæ–‡æœ¬å†…å®¹

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„

    Returns:
        æå–çš„é¡µé¢æ–‡æœ¬åˆ—è¡¨
    """
    import fitz  # PyMuPDF

    print_status(f"æå–PDFæ–‡æœ¬: {pdf_path}", "ğŸ“š")

    try:
        doc = fitz.open(pdf_path)
        pages_text = []

        for page_num in range(len(doc)):
            page = doc[page_num]
            text = page.get_text()

            if text.strip():  # åªä¿å­˜éç©ºé¡µé¢
                pages_text.append({
                    'page_number': page_num + 1,
                    'content': text.strip(),
                    'char_count': len(text.strip())
                })

        doc.close()

        print(f"âœ… æˆåŠŸæå– {len(pages_text)} é¡µæ–‡æœ¬å†…å®¹")
        return pages_text

    except Exception as e:
        print(f"âŒ PDFæ–‡æœ¬æå–å¤±è´¥: {e}")
        return []


def split_text_into_chunks(pages_text: List[Dict[str, Any]],
                         chunk_size: int = 1000,
                         chunk_overlap: int = 200) -> List[Dict[str, Any]]:
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆé€‚åˆçš„ç‰‡æ®µ

    Args:
        pages_text: é¡µé¢æ–‡æœ¬åˆ—è¡¨
        chunk_size: ç‰‡æ®µå¤§å°
        chunk_overlap: ç‰‡æ®µé‡å 

    Returns:
        æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
    """
    print_status(f"åˆ†å‰²æ–‡æœ¬ä¸ºç‰‡æ®µ (å¤§å°: {chunk_size}, é‡å : {chunk_overlap})", "âœ‚ï¸")

    all_chunks = []
    current_chunk = ""
    current_page = 1
    chunk_index = 0

    for page_data in pages_text:
        page_text = page_data['content']
        page_number = page_data['page_number']

        # å¦‚æœå½“å‰ç‰‡æ®µä¸ºç©ºï¼Œå¼€å§‹æ–°ç‰‡æ®µ
        if not current_chunk:
            current_page = page_number
            current_chunk = page_text
        else:
            current_chunk += "\n\n" + page_text

        # å½“ç‰‡æ®µè¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œåˆ›å»ºç‰‡æ®µ
        if len(current_chunk) >= chunk_size:
            # æ·»åŠ å½“å‰ç‰‡æ®µ
            chunk = {
                'content': current_chunk,
                'page_number': current_page,
                'chunk_index': chunk_index,
                'quality_score': 1.0,  # ç®€å•çš„è´¨é‡è¯„åˆ†
                'metadata': {
                    'subject': 'è¯­æ–‡',
                    'grade': 'ä¸‰å¹´çº§',
                    'semester': 'ä¸Šå†Œ',
                    'textbook': 'äººæ•™ç‰ˆ',
                    'source_type': 'pdf_textbook',
                    'processed_date': datetime.now().isoformat(),
                    'content_type': 'æ•™æå†…å®¹',
                    'language': 'chinese'
                }
            }
            all_chunks.append(chunk)
            chunk_index += 1

            # ä¿ç•™é‡å éƒ¨åˆ†ç”¨äºä¸‹ä¸€ä¸ªç‰‡æ®µ
            if chunk_overlap > 0:
                overlap_start = max(0, len(current_chunk) - chunk_overlap)
                current_chunk = current_chunk[overlap_start:]
            else:
                current_chunk = ""

    # å¤„ç†æœ€åä¸€ä¸ªç‰‡æ®µ
    if current_chunk.strip():
        chunk = {
            'content': current_chunk,
            'page_number': current_page,
            'chunk_index': chunk_index,
            'quality_score': 1.0,
            'metadata': {
                'subject': 'è¯­æ–‡',
                'grade': 'ä¸‰å¹´çº§',
                'semester': 'ä¸Šå†Œ',
                'textbook': 'äººæ•™ç‰ˆ',
                'source_type': 'pdf_textbook',
                'processed_date': datetime.now().isoformat(),
                'content_type': 'æ•™æå†…å®¹',
                'language': 'chinese'
            }
        }
        all_chunks.append(chunk)

    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
    return all_chunks


def create_simple_embeddings(texts: List[str]) -> List[List[float]]:
    """
    åˆ›å»ºç®€å•çš„ä¼ªåµŒå…¥å‘é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    åœ¨å®é™…ä½¿ç”¨ä¸­åº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„åµŒå…¥æ¨¡å‹è°ƒç”¨

    Args:
        texts: æ–‡æœ¬åˆ—è¡¨

    Returns:
        åµŒå…¥å‘é‡åˆ—è¡¨
    """
    print_status("åˆ›å»ºåµŒå…¥å‘é‡ (ä½¿ç”¨ç®€å•å“ˆå¸Œæ–¹æ³• - ä»…ç”¨äºæµ‹è¯•)", "ğŸ”")

    embeddings = []
    for text in texts:
        # ä½¿ç”¨æ–‡æœ¬çš„å“ˆå¸Œå€¼åˆ›å»ºå›ºå®šé•¿åº¦çš„å‘é‡
        hash_obj = hashlib.md5(text.encode('utf-8'))
        hash_hex = hash_obj.hexdigest()

        # å°†å“ˆå¸Œå€¼è½¬æ¢ä¸º1024ç»´å‘é‡
        vector = []
        for i in range(0, len(hash_hex), 2):
            hex_pair = hash_hex[i:i+2]
            value = int(hex_pair, 16) / 255.0 - 0.5  # å½’ä¸€åŒ–åˆ°[-0.5, 0.5]
            vector.extend([value] * 64)  # æ¯ä¸ªå­—èŠ‚æ‰©å±•ä¸º64ä¸ªå€¼

        # ç¡®ä¿å‘é‡é•¿åº¦ä¸º1024
        while len(vector) < 1024:
            vector.append(0.0)

        embeddings.append(vector[:1024])

    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ª1024ç»´åµŒå…¥å‘é‡")
    return embeddings


def save_chunks_to_database(chunks: List[Dict[str, Any]]) -> bool:
    """
    å°†æ–‡æœ¬ç‰‡æ®µä¿å­˜åˆ°æ•°æ®åº“

    Args:
        chunks: æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨

    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    print_status(f"ä¿å­˜ {len(chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µåˆ°æ•°æ®åº“", "ğŸ’¾")

    try:
        session = SessionLocal()

        saved_count = 0
        for chunk in chunks:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = create_simple_embeddings([chunk['content']])[0]
            content_hash = hashlib.md5(chunk['content'].encode('utf-8')).hexdigest()

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = session.query(TextbookChunk).filter(
                TextbookChunk.content_hash == content_hash
            ).first()

            if existing:
                continue

            # åˆ›å»ºæ–°çš„TextbookChunkå¯¹è±¡
            textbook_chunk = TextbookChunk(
                content=chunk['content'],
                embedding=embedding,
                metadata_json=chunk['metadata'],
                source_file="data/textbooks/è¯­æ–‡ä¸‰ä¸Š.pdf",
                chunk_index=chunk['chunk_index'],
                content_hash=content_hash,
                page_number=chunk['page_number'],
                quality_score=chunk['quality_score']
            )

            session.add(textbook_chunk)
            saved_count += 1

        session.commit()
        session.close()

        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ä¸ªæ–°æ–‡æœ¬ç‰‡æ®µåˆ°æ•°æ®åº“")
        return True

    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        if 'session' in locals():
            session.rollback()
            session.close()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ è¯­æ–‡ä¸‰ä¸Š PDF å¤„ç†å’Œå‘é‡åŒ–è„šæœ¬")
    print("=" * 60)
    print()

    # é…ç½®å‚æ•°
    pdf_path = "data/textbooks/è¯­æ–‡ä¸‰ä¸Š.pdf"

    if not Path(pdf_path).exists():
        print(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return 1

    try:
        # æ­¥éª¤1: æå–PDFæ–‡æœ¬
        pages_text = extract_text_from_pdf(pdf_path)
        if not pages_text:
            print("âŒ æå–PDFæ–‡æœ¬ - å¤±è´¥")
            return 1
        print("âœ… æå–PDFæ–‡æœ¬ - é€šè¿‡")

        # æ­¥éª¤2: åˆ†å‰²æ–‡æœ¬
        chunks = split_text_into_chunks(pages_text)
        if not chunks:
            print("âŒ åˆ†å‰²æ–‡æœ¬ - å¤±è´¥")
            return 1
        print("âœ… åˆ†å‰²æ–‡æœ¬ - é€šè¿‡")

        # æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®åº“
        if not save_chunks_to_database(chunks):
            print("âŒ ä¿å­˜åˆ°æ•°æ®åº“ - å¤±è´¥")
            return 1
        print("âœ… ä¿å­˜åˆ°æ•°æ®åº“ - é€šè¿‡")

        print("\n" + "=" * 60)
        print("ğŸ‰ è¯­æ–‡ä¸‰ä¸Š PDF å¤„ç†å®Œæˆ!")
        print("âœ… è¯­æ–‡æ•™æå·²æˆåŠŸå¤„ç†å¹¶å­˜å‚¨åˆ°æ•°æ®åº“")
        print("ğŸ” ç°åœ¨å¯ä»¥è¿›è¡Œè¯­æ–‡çŸ¥è¯†çš„è¯­ä¹‰æ£€ç´¢")

        return 0

    except Exception as e:
        print(f"âŒ å¤„ç†æµç¨‹å¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())