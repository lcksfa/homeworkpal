#!/usr/bin/env python3
"""
ä½œä¸šæ­å­é—®ç­”ç³»ç»Ÿæ•™è‚²åœºæ™¯æµ‹è¯•
Test QA System for Educational Scenarios
"""

import asyncio
import time
import json
from homeworkpal.rag.qa_service import QAService, QARequest


async def test_educational_qa():
    """æµ‹è¯•æ•™è‚²åœºæ™¯é—®ç­”åŠŸèƒ½"""

    print("ğŸŒŸ ä½œä¸šæ­å­é—®ç­”ç³»ç»Ÿæ•™è‚²åœºæ™¯æµ‹è¯•")
    print("=" * 50)

    # åˆå§‹åŒ–é—®ç­”æœåŠ¡
    qa_service = QAService()

    # æµ‹è¯•é—®é¢˜åˆ—è¡¨ï¼ˆé€‚åˆä¸‰å¹´çº§å­¦ç”Ÿï¼‰
    test_questions = [
        {
            "question": "å‘¨é•¿æ˜¯ä»€ä¹ˆ",
            "subject": "æ•°å­¦",
            "grade": "ä¸‰å¹´çº§",
            "description": "æ•°å­¦æ¦‚å¿µç†è§£æµ‹è¯•"
        },
        {
            "question": "æ€ä¹ˆå†™å¥½ä½œæ–‡çš„å¼€å¤´",
            "subject": "è¯­æ–‡",
            "grade": "ä¸‰å¹´çº§",
            "description": "è¯­æ–‡å†™ä½œæŒ‡å¯¼æµ‹è¯•"
        },
        {
            "question": "å¦‚ä½•æå†™ç¾ä¸½çš„æ™¯è‰²",
            "subject": "è¯­æ–‡",
            "grade": "ä¸‰å¹´çº§",
            "description": "å†™ä½œæŠ€å·§æŒ‡å¯¼æµ‹è¯•"
        },
        {
            "question": "ä»€ä¹ˆæ˜¯æ¯”å–»å¥",
            "subject": "è¯­æ–‡",
            "grade": "ä¸‰å¹´çº§",
            "description": "ä¿®è¾æ‰‹æ³•å­¦ä¹ æµ‹è¯•"
        }
    ]

    results = []

    for i, test_case in enumerate(test_questions, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {test_case['description']}")
        print(f"é—®é¢˜: {test_case['question']}")
        print(f"å­¦ç§‘: {test_case['subject']} | å¹´çº§: {test_case['grade']}")
        print("-" * 50)

        # åˆ›å»ºé—®ç­”è¯·æ±‚
        request = QARequest(
            question=test_case["question"],
            subject=test_case["subject"],
            grade=test_case["grade"],
            temperature=0.7,
            max_tokens=800
        )

        try:
            # æ‰§è¡Œé—®ç­”
            start_time = time.time()
            response = await qa_service.ask_question(request)
            end_time = time.time()

            # æ˜¾ç¤ºç»“æœ
            print(f"âœ… å›ç­”ç”ŸæˆæˆåŠŸ (è€—æ—¶: {response.response_time:.2f}ç§’)")
            print(f"ğŸ“š ä½¿ç”¨æ•™æä¸Šä¸‹æ–‡: {'æ˜¯' if response.context_used else 'å¦'}")
            print(f"ğŸ“– å‚è€ƒæ¥æºæ•°é‡: {len(response.sources)}")

            print(f"\nğŸ‘©â€ğŸ« è€å¸ˆçš„å›ç­”:")
            print(response.answer)

            if response.sources:
                print(f"\nğŸ“š æ•™æå‚è€ƒ:")
                for j, source in enumerate(response.sources[:2], 1):  # åªæ˜¾ç¤ºå‰2ä¸ªæ¥æº
                    print(f"  æ¥æº {j}: {source.get('source_file', 'æœªçŸ¥æ–‡ä»¶')} "
                          f"(ç¬¬{source.get('page_number', '?')}é¡µ) "
                          f"ç›¸ä¼¼åº¦: {source.get('score', 0):.3f}")

            # è¯„ä¼°ç»“æœ
            evaluation = evaluate_response(response, test_case)
            print(f"\nğŸ“Š è´¨é‡è¯„ä¼°:")
            print(f"  æ•™å¸ˆè¯­å½¢: {'âœ…' if evaluation['teacher_tone'] else 'âŒ'}")
            print(f"  é¼“åŠ±æ€§: {'âœ…' if evaluation['encouraging'] else 'âŒ'}")
            print(f"  å¹´é¾„é€‚é…: {'âœ…' if evaluation['age_appropriate'] else 'âŒ'}")
            print(f"  é•¿åº¦åˆé€‚: {'âœ…' if evaluation['good_length'] else 'âŒ'}")
            print(f"  ç»¼åˆè¯„åˆ†: {evaluation['overall_score']}/10")

            results.append({
                'test_case': test_case,
                'response_time': response.response_time,
                'context_used': response.context_used,
                'sources_count': len(response.sources),
                'answer_length': len(response.answer),
                'evaluation': evaluation
            })

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            results.append({
                'test_case': test_case,
                'error': str(e)
            })

        print("\n" + "=" * 80)

    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    generate_test_report(results)
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")


def evaluate_response(response, test_case):
    """è¯„ä¼°å›ç­”è´¨é‡"""
    evaluation = {
        'teacher_tone': False,
        'encouraging': False,
        'age_appropriate': False,
        'good_length': False,
        'overall_score': 0
    }

    answer = response.answer.lower()

    # æ£€æŸ¥æ•™å¸ˆè¯­å½¢
    teacher_words = ['å°æœ‹å‹', 'è€å¸ˆ', 'ä½ ', 'æˆ‘ä»¬', 'å®è´']
    evaluation['teacher_tone'] = any(word in answer for word in teacher_words)

    # æ£€æŸ¥é¼“åŠ±æ€§
    encouraging_words = ['åŠ æ²¹', 'ç»§ç»­', 'å¾ˆæ£’', 'ä¸é”™', 'ç›¸ä¿¡', 'ä¸€å®šèƒ½']
    evaluation['encouraging'] = any(word in answer for word in encouraging_words)

    # æ£€æŸ¥å¹´é¾„é€‚é…ï¼ˆé¿å…å¤æ‚è¯æ±‡ï¼‰
    complex_words = ['æŠ½è±¡', 'ç†è®º', 'æ¦‚å¿µ', 'å®šä¹‰', 'åˆ†æ', 'ç»¼åˆ']
    has_complex = any(word in answer for word in complex_words)
    evaluation['age_appropriate'] = not has_complex or answer.count('æ¯”å¦‚') > 0  # å¦‚æœæœ‰å¤æ‚è¯ä½†ä¸¾ä¾‹äº†ä¹Ÿç®—åˆé€‚

    # æ£€æŸ¥é•¿åº¦åˆé€‚
    length = len(response.answer)
    evaluation['good_length'] = 100 <= length <= 800  # é€‚åˆä¸‰å¹´çº§å­¦ç”Ÿé˜…è¯»çš„é•¿åº¦

    # è®¡ç®—ç»¼åˆè¯„åˆ†
    score = 0
    if evaluation['teacher_tone']:
        score += 2.5
    if evaluation['encouraging']:
        score += 2.5
    if evaluation['age_appropriate']:
        score += 2.5
    if evaluation['good_length']:
        score += 2.5

    evaluation['overall_score'] = score

    return evaluation


def generate_test_report(results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
    print("=" * 30)

    successful_tests = [r for r in results if 'error' not in r]
    failed_tests = [r for r in results if 'error' in r]

    print(f"æ€»æµ‹è¯•æ•°: {len(results)}")
    print(f"æˆåŠŸæµ‹è¯•: {len(successful_tests)}")
    print(f"å¤±è´¥æµ‹è¯•: {len(failed_tests)}")

    if successful_tests:
        avg_response_time = sum(r['response_time'] for r in successful_tests) / len(successful_tests)
        avg_sources_count = sum(r['sources_count'] for r in successful_tests) / len(successful_tests)
        avg_score = sum(r['evaluation']['overall_score'] for r in successful_tests) / len(successful_tests)

        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’")
        print(f"  å¹³å‡å‚è€ƒæ¥æº: {avg_sources_count:.1f}ä¸ª")
        print(f"  å¹³å‡è´¨é‡è¯„åˆ†: {avg_score:.1f}/10")

        print(f"\nğŸ¯ è´¨é‡åˆ†æ:")
        teacher_tone_count = sum(1 for r in successful_tests if r['evaluation']['teacher_tone'])
        encouraging_count = sum(1 for r in successful_tests if r['evaluation']['encouraging'])
        age_appropriate_count = sum(1 for r in successful_tests if r['evaluation']['age_appropriate'])
        good_length_count = sum(1 for r in successful_tests if r['evaluation']['good_length'])

        print(f"  æ•™å¸ˆè¯­å½¢: {teacher_tone_count}/{len(successful_tests)} âœ…")
        print(f"  é¼“åŠ±æ€§: {encouraging_count}/{len(successful_tests)} âœ…")
        print(f"  å¹´é¾„é€‚é…: {age_appropriate_count}/{len(successful_tests)} âœ…")
        print(f"  é•¿åº¦åˆé€‚: {good_length_count}/{len(successful_tests)} âœ…")

    if failed_tests:
        print(f"\nâŒ å¤±è´¥åŸå› :")
        for test in failed_tests:
            print(f"  {test['test_case']['question']}: {test['error']}")


if __name__ == "__main__":
    asyncio.run(test_educational_qa())