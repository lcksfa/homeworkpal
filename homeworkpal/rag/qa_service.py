"""
RAGÈóÆÁ≠îÊúçÂä°
RAG Question-Answering Service for Homework Pal

ÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑÈóÆÁ≠îÊµÅÁ®ãÔºöÈóÆÈ¢ò‚ÜíÂêëÈáèÂåñ‚ÜíÊ£ÄÁ¥¢‚ÜíÁîüÊàê‚ÜíÁ≠îÊ°à
Âü∫‰∫é‰∫∫ÊïôÁâàÊïôÊùêÂÜÖÂÆπ‰∏∫‰∏âÂπ¥Á∫ßÂ≠¶ÁîüÊèê‰æõÊïôËÇ≤ÂØºÂêëÁöÑÁ≠îÊ°à
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import json
import time

from sqlalchemy.orm import Session
from ..database.connection import get_db
from ..database.models import TextbookChunk
from ..rag.rag_service import RAGService, SearchResult
from ..llm.siliconflow import SiliconFlowClient

logger = logging.getLogger(__name__)


@dataclass
class QARequest:
    """ÈóÆÁ≠îËØ∑Ê±ÇÊï∞ÊçÆÁ±ª"""
    question: str
    subject: Optional[str] = None  # Â≠¶ÁßëËøáÊª§ÔºåÂ¶Ç"ËØ≠Êñá"„ÄÅ"Êï∞Â≠¶"
    grade: Optional[str] = None    # Âπ¥Á∫ßËøáÊª§ÔºåÂ¶Ç"‰∏âÂπ¥Á∫ß"
    unit: Optional[str] = None     # ÂçïÂÖÉËøáÊª§
    max_context_length: int = 3000 # ‰∏ä‰∏ãÊñáÊúÄÂ§ßÈïøÂ∫¶
    temperature: float = 0.7       # ÁîüÊàêÊ∏©Â∫¶
    max_tokens: int = 800          # ÊúÄÂ§ßÁîüÊàêtokenÊï∞


@dataclass
class QAResponse:
    """ÈóÆÁ≠îÂìçÂ∫îÊï∞ÊçÆÁ±ª"""
    answer: str                    # ÁîüÊàêÁöÑÁ≠îÊ°à
    sources: List[Dict[str, Any]] # ÂèÇËÄÉÊù•Ê∫ê
    question: str                 # ÂéüÂßãÈóÆÈ¢ò
    response_time: float          # ÂìçÂ∫îÊó∂Èó¥ÔºàÁßíÔºâ
    context_used: bool           # ÊòØÂê¶‰ΩøÁî®‰∫ÜÊïôÊùê‰∏ä‰∏ãÊñá
    metadata: Dict[str, Any]     # È¢ùÂ§ñÂÖÉÊï∞ÊçÆ

    def to_dict(self) -> Dict[str, Any]:
        """ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏Ê†ºÂºè"""
        return {
            'answer': self.answer,
            'sources': self.sources,
            'question': self.question,
            'response_time': self.response_time,
            'context_used': self.context_used,
            'metadata': self.metadata
        }


class QAService:
    """RAGÈóÆÁ≠îÊúçÂä°Á±ª

    ÂÆûÁé∞ÂÆåÊï¥ÁöÑÈóÆÁ≠îÊµÅÁ®ãÔºöÈóÆÈ¢òÁêÜËß£ ‚Üí ÂêëÈáèÊ£ÄÁ¥¢ ‚Üí ‰∏ä‰∏ãÊñáÊûÑÂª∫ ‚Üí LLMÁîüÊàêÁ≠îÊ°à ‚Üí ÁªìÊûúÂêéÂ§ÑÁêÜ
    ‰∏ìÈó®ÈíàÂØπ‰∏âÂπ¥Á∫ßÂ≠¶ÁîüÁöÑÊïôËÇ≤Âú∫ÊôØ‰ºòÂåñÔºå‰ΩøÁî®ÊïôÂ∏àËØ≠ÂΩ¢ÂíåÈºìÂä±ÊÄßËØ≠Ë®Ä
    """

    def __init__(self,
                 rag_service: Optional[RAGService] = None,
                 llm_client: Optional[SiliconFlowClient] = None):
        """
        ÂàùÂßãÂåñÈóÆÁ≠îÊúçÂä°

        Args:
            rag_service: RAGÊ£ÄÁ¥¢ÊúçÂä°ÂÆû‰æã
            llm_client: LLMÂÆ¢Êà∑Á´ØÂÆû‰æã
        """
        self.rag_service = rag_service or RAGService()
        self.llm_client = llm_client or SiliconFlowClient()

        # ÊïôÂ∏àËØ≠ÂΩ¢PromptÊ®°Êùø
        self.teacher_prompt_template = """
‰Ω†ÊòØ‰∏Ä‰ΩçÁªèÈ™å‰∏∞ÂØåÁöÑÂ∞èÂ≠¶‰∏âÂπ¥Á∫ßËØ≠ÊñáËÄÅÂ∏àÔºåÊ≠£Âú®ËÄêÂøÉÂõûÁ≠îÂ≠¶ÁîüÁöÑÈóÆÈ¢ò„ÄÇ

## Â≠¶ÁîüÁöÑÈóÆÈ¢òÔºö
{question}

## Áõ∏ÂÖ≥ÊïôÊùêÂÜÖÂÆπÔºö
{context}

## ÂõûÁ≠îË¶ÅÊ±ÇÔºö
1. ‰ΩøÁî®Ê∏©Êüî„ÄÅÈºìÂä±ÁöÑËØ≠Ê∞îÔºåÂÉèËÄÅÂ∏àÂíåÂ≠¶ÁîüËØ¥ËØù‰∏ÄÊ†∑‰∫≤Âàá
2. Áî®ÁÆÄÂçïÊòìÊáÇÁöÑËØ≠Ë®ÄËß£ÈáäÔºåÈÅøÂÖçÂ§çÊùÇËØçÊ±á
3. Â¶ÇÊûúÊúâÂÆö‰πâÔºåÂÖàÁªôÂá∫ÊòéÁ°ÆÁöÑÂÆö‰πâÔºåÁÑ∂ÂêéÁî®ÁîüÊ¥ª‰∏≠ÁöÑ‰æãÂ≠êÂ∏ÆÂä©ÁêÜËß£
4. ÂõûÁ≠îË¶ÅÂáÜÁ°ÆÂü∫‰∫éÊèê‰æõÁöÑÊïôÊùêÂÜÖÂÆπÔºå‰∏çË¶ÅÁºñÈÄ†‰ø°ÊÅØ
5. ÁªìÂ∞æË¶ÅÈºìÂä±Â≠¶ÁîüÁªßÁª≠Â≠¶‰π†Ôºå‰øùÊä§Â≠¶ÁîüÁöÑÂ≠¶‰π†ÂÖ¥Ë∂£
6. ÂõûÁ≠îÈïøÂ∫¶ÊéßÂà∂Âú®200-400Â≠ó‰πãÈó¥ÔºåÈÄÇÂêà‰∏âÂπ¥Á∫ßÂ≠¶ÁîüÈòÖËØª

## ËØ∑ÂõûÁ≠îÔºö
"""

        # Êó†‰∏ä‰∏ãÊñáÊó∂ÁöÑÂõûÁ≠îÊ®°Êùø
        self.no_context_prompt = """
‰Ω†ÊòØ‰∏Ä‰ΩçÁªèÈ™å‰∏∞ÂØåÁöÑÂ∞èÂ≠¶‰∏âÂπ¥Á∫ßËØ≠ÊñáËÄÅÂ∏àÔºåÊ≠£Âú®ÂõûÁ≠îÂ≠¶ÁîüÁöÑÈóÆÈ¢ò„ÄÇ

## Â≠¶ÁîüÁöÑÈóÆÈ¢òÔºö
{question}

## ÂõûÁ≠îË¶ÅÊ±ÇÔºö
1. ‰ΩøÁî®Ê∏©Êüî„ÄÅÈºìÂä±ÁöÑËØ≠Ê∞îÔºåÂÉèËÄÅÂ∏àÂíåÂ≠¶ÁîüËØ¥ËØù‰∏ÄÊ†∑‰∫≤Âàá
2. Áî®ÁÆÄÂçïÊòìÊáÇÁöÑËØ≠Ë®ÄËß£ÈáäÔºåÈÅøÂÖçÂ§çÊùÇËØçÊ±á
3. Â¶ÇÊûúÊòØÂ≠¶ÁßëÊ¶ÇÂøµÈóÆÈ¢òÔºåÁªôÂá∫Âü∫Á°ÄÂÆö‰πâÂíåÁîüÊ¥ª‰∏≠ÁöÑ‰æãÂ≠ê
4. ÊâøËÆ§ÊïôÊùê‰∏≠Ê≤°ÊúâÊâæÂà∞ÂÆåÂÖ®ÂØπÂ∫îÁöÑÂÜÖÂÆπÔºå‰ΩÜ‰ªçÊèê‰æõÊúâÁî®ÁöÑÊåáÂØº
5. Âª∫ËÆÆÂ≠¶ÁîüÂèØ‰ª•Êü•ÈòÖÁõ∏ÂÖ≥ÊïôÊùêÊàñËØ¢ÈóÆËÄÅÂ∏à
6. ÂõûÁ≠îÈïøÂ∫¶ÊéßÂà∂Âú®150-300Â≠ó‰πãÈó¥
7. ÁªìÂ∞æË¶ÅÈºìÂä±Â≠¶ÁîüÁöÑÂ≠¶‰π†ÁÉ≠ÊÉÖ

## ËØ∑ÂõûÁ≠îÔºö
"""

    async def ask_question(self, request: QARequest) -> QAResponse:
        """
        Â§ÑÁêÜÈóÆÁ≠îËØ∑Ê±ÇÁöÑÂÆåÊï¥ÊµÅÁ®ã

        Args:
            request: ÈóÆÁ≠îËØ∑Ê±ÇÂØπË±°

        Returns:
            ÈóÆÁ≠îÂìçÂ∫îÂØπË±°
        """
        start_time = time.time()

        try:
            logger.info(f"ÂºÄÂßãÂ§ÑÁêÜÈóÆÈ¢ò: {request.question}")

            # Ê≠•È™§1: ÂêëÈáèÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊïôÊùêÂÜÖÂÆπ
            search_results = await self._retrieve_relevant_content(request)

            # Ê≠•È™§2: ÊûÑÂª∫‰∏ä‰∏ãÊñáÂíåPrompt
            context, prompt = self._build_context_and_prompt(request, search_results)

            # Ê≠•È™§3: LLMÁîüÊàêÁ≠îÊ°à
            answer = await self._generate_answer(prompt, request.temperature, request.max_tokens)

            # Ê≠•È™§4: ÂáÜÂ§áÂìçÂ∫îÊï∞ÊçÆ
            response_time = time.time() - start_time

            sources = []
            for result in search_results:
                source_info = {
                    'content': result.content[:200] + "..." if len(result.content) > 200 else result.content,
                    'score': result.score,
                    'source_file': result.source_file,
                    'page_number': result.page_number,
                    'metadata': result.metadata
                }
                sources.append(source_info)

            metadata = {
                'subject_filter': request.subject,
                'grade_filter': request.grade,
                'unit_filter': request.unit,
                'search_results_count': len(search_results),
                'context_length': len(context),
                'model_info': self.llm_client.llm_client.get_model_info()
            }

            response = QAResponse(
                answer=answer,
                sources=sources,
                question=request.question,
                response_time=response_time,
                context_used=len(search_results) > 0,
                metadata=metadata
            )

            logger.info(f"ÈóÆÁ≠îÂÆåÊàêÔºåËÄóÊó∂: {response_time:.2f}ÁßíÔºåÊ£ÄÁ¥¢Âà∞{len(search_results)}‰∏™Áõ∏ÂÖ≥ÁâáÊÆµ")
            return response

        except Exception as e:
            logger.error(f"ÈóÆÁ≠îÂ§ÑÁêÜÂ§±Ë¥•: {e}")
            # ËøîÂõûÈîôËØØÂìçÂ∫î
            error_response_time = time.time() - start_time
            return QAResponse(
                answer=f"Êä±Ê≠âÔºåËÄÅÂ∏àÁé∞Âú®ÈÅáÂà∞‰∫Ü‰∏Ä‰∫õÊäÄÊúØÈóÆÈ¢òÔºåÊó†Ê≥ïÂõûÁ≠î‰Ω†ÁöÑÈóÆÈ¢ò„ÄÇËØ∑Á®çÂêéÂÜçËØïÔºåÊàñËÄÖÁõ¥Êé•ËØ¢ÈóÆ‰Ω†ÁöÑËØ≠ÊñáËÄÅÂ∏àÂì¶ÔºÅÁªßÁª≠Âä†Ê≤πÂ≠¶‰π†ÔºÅüí™",
                sources=[],
                question=request.question,
                response_time=error_response_time,
                context_used=False,
                metadata={'error': str(e)}
            )

    async def ask_question_with_error_handling(self, request: QARequest) -> QAResponse:
        """
        Â§ÑÁêÜÈóÆÁ≠îËØ∑Ê±ÇÔºåÂ∏¶ÊúâËØ¶ÁªÜÁöÑÈîôËØØÂ§ÑÁêÜÈÄªËæëÔºàÁî®‰∫éÊµãËØïÔºâ

        Args:
            request: ÈóÆÁ≠îËØ∑Ê±ÇÂØπË±°

        Returns:
            ÈóÆÁ≠îÂìçÂ∫îÂØπË±°
        """
        start_time = time.time()
        retrieval_failed = False

        try:
            logger.info(f"ÂºÄÂßãÂ§ÑÁêÜÈóÆÈ¢ò: {request.question}")

            # Ê≠•È™§1: ÂêëÈáèÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊïôÊùêÂÜÖÂÆπ
            try:
                search_results = await self._retrieve_relevant_content(request)
            except Exception as retrieval_error:
                logger.error(f"ÂÜÖÂÆπÊ£ÄÁ¥¢Â§±Ë¥•: {retrieval_error}")
                retrieval_failed = True
                # Áõ¥Êé•ËøîÂõûÈîôËØØÂìçÂ∫îÔºåÂõ†‰∏∫Êàë‰ª¨Êó†Ê≥ïËé∑Âèñ‰ªª‰Ωï‰∏ä‰∏ãÊñá
                error_response_time = time.time() - start_time
                return QAResponse(
                    answer=f"Êä±Ê≠âÔºåËÄÅÂ∏àÁöÑÁü•ËØÜÂ∫ìÁé∞Âú®ÈúÄË¶Å‰ºëÊÅØ‰∏Ä‰∏ã„ÄÇ‰Ω†ÂèØ‰ª•ÊääËøô‰∏™ÈóÆÈ¢òËÆ∞‰∏ãÊù•ÔºåÊòéÂ§©ÈóÆÂ≠¶Ê†°ÁöÑËÄÅÂ∏àÂì¶ÔºÅÁªßÁª≠Âä™ÂäõÔºÅüåü",
                    sources=[],
                    question=request.question,
                    response_time=error_response_time,
                    context_used=False,
                    metadata={'error': 'Ê£ÄÁ¥¢ÊúçÂä°‰∏çÂèØÁî®'}
                )

            # Ê≠•È™§2: ÊûÑÂª∫‰∏ä‰∏ãÊñáÂíåPrompt
            context, prompt = self._build_context_and_prompt(request, search_results)

            # Ê≠•È™§3: LLMÁîüÊàêÁ≠îÊ°à
            try:
                answer = await self._generate_answer(prompt, request.temperature, request.max_tokens)
            except Exception as generation_error:
                logger.error(f"Á≠îÊ°àÁîüÊàêÂ§±Ë¥•: {generation_error}")
                error_response_time = time.time() - start_time
                return QAResponse(
                    answer=f"Êä±Ê≠âÔºåËÄÅÂ∏àÁé∞Âú®ÈúÄË¶Å‰ºëÊÅØ‰∏Ä‰∏ãÔºåÊ≤°ËÉΩÂæàÂ•ΩÂú∞ÂõûÁ≠î‰Ω†ÁöÑÈóÆÈ¢ò„ÄÇ‰Ω†ÂèØ‰ª•ÊääËøô‰∏™ÈóÆÈ¢òËÆ∞‰∏ãÊù•ÔºåÊòéÂ§©ÈóÆÂ≠¶Ê†°ÁöÑËÄÅÂ∏àÂì¶ÔºÅÁªßÁª≠Âä™ÂäõÔºÅüåü",
                    sources=[],
                    question=request.question,
                    response_time=error_response_time,
                    context_used=False,
                    metadata={'error': 'ÁîüÊàêÊúçÂä°‰∏çÂèØÁî®'}
                )

            # Ê≠•È™§4: ÂáÜÂ§áÂìçÂ∫îÊï∞ÊçÆ
            response_time = time.time() - start_time

            sources = []
            for result in search_results:
                source_info = {
                    'content': result.content[:200] + "..." if len(result.content) > 200 else result.content,
                    'score': result.score,
                    'source_file': result.source_file,
                    'page_number': result.page_number,
                    'metadata': result.metadata
                }
                sources.append(source_info)

            metadata = {
                'subject_filter': request.subject,
                'grade_filter': request.grade,
                'unit_filter': request.unit,
                'search_results_count': len(search_results),
                'context_length': len(context),
                'model_info': self.llm_client.llm_client.get_model_info(),
                'retrieval_failed': retrieval_failed
            }

            response = QAResponse(
                answer=answer,
                sources=sources,
                question=request.question,
                response_time=response_time,
                context_used=len(search_results) > 0,
                metadata=metadata
            )

            logger.info(f"ÈóÆÁ≠îÂÆåÊàêÔºåËÄóÊó∂: {response_time:.2f}ÁßíÔºåÊ£ÄÁ¥¢Âà∞{len(search_results)}‰∏™Áõ∏ÂÖ≥ÁâáÊÆµ")
            return response

        except Exception as e:
            logger.error(f"ÈóÆÁ≠îÂ§ÑÁêÜÂÆåÂÖ®Â§±Ë¥•: {e}")
            # ËøîÂõûÈîôËØØÂìçÂ∫î
            error_response_time = time.time() - start_time
            return QAResponse(
                answer=f"Êä±Ê≠âÔºåËÄÅÂ∏àÁé∞Âú®ÈÅáÂà∞‰∫Ü‰∏Ä‰∫õÊäÄÊúØÈóÆÈ¢òÔºåÊó†Ê≥ïÂõûÁ≠î‰Ω†ÁöÑÈóÆÈ¢ò„ÄÇËØ∑Á®çÂêéÂÜçËØïÔºåÊàñËÄÖÁõ¥Êé•ËØ¢ÈóÆ‰Ω†ÁöÑËØ≠ÊñáËÄÅÂ∏àÂì¶ÔºÅÁªßÁª≠Âä†Ê≤πÂ≠¶‰π†ÔºÅüí™",
                sources=[],
                question=request.question,
                response_time=error_response_time,
                context_used=False,
                metadata={'error': str(e)}
            )

    async def _retrieve_relevant_content(self, request: QARequest) -> List[SearchResult]:
        """
        Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÁöÑÊïôÊùêÂÜÖÂÆπ

        Args:
            request: ÈóÆÁ≠îËØ∑Ê±ÇÂØπË±°

        Returns:
            Ê£ÄÁ¥¢ÁªìÊûúÂàóË°®
        """
        # ‰ΩøÁî®RAGÊúçÂä°ËøõË°åËØ≠‰πâÊêúÁ¥¢
        results = self.rag_service.search(
            query=request.question,
            top_k=5,  # Ê£ÄÁ¥¢5‰∏™ÊúÄÁõ∏ÂÖ≥ÁöÑÁâáÊÆµ
            subject=request.subject,
            grade=request.grade,
            unit=request.unit
        )

        # ËøáÊª§‰ΩéË¥®ÈáèÁªìÊûú
        filtered_results = []
        for result in results:
            if result.score > 0.3:  # Áõ∏‰ººÂ∫¶ÈòàÂÄº
                filtered_results.append(result)

        logger.info(f"Ê£ÄÁ¥¢Âà∞ {len(filtered_results)} ‰∏™È´òË¥®ÈáèÁõ∏ÂÖ≥ÁâáÊÆµÔºàÈòàÂÄº>0.3Ôºâ")
        return filtered_results

    def _build_context_and_prompt(self,
                                 request: QARequest,
                                 search_results: List[SearchResult]) -> Tuple[str, str]:
        """
        ÊûÑÂª∫‰∏ä‰∏ãÊñáÂíåPrompt

        Args:
            request: ÈóÆÁ≠îËØ∑Ê±ÇÂØπË±°
            search_results: Ê£ÄÁ¥¢ÁªìÊûúÂàóË°®

        Returns:
            ‰∏ä‰∏ãÊñáÊñáÊú¨ÂíåÂÆåÊï¥Prompt
        """
        context_parts = []
        current_length = 0
        max_context_length = request.max_context_length

        # ÊûÑÂª∫‰∏ä‰∏ãÊñá
        for i, result in enumerate(search_results):
            # Ê†ºÂºèÂåñÁâáÊÆµ
            source_info = []
            if result.source_file:
                source_info.append(f"Êù•Ê∫ê: {result.source_file}")
            if result.page_number:
                source_info.append(f"Á¨¨{result.page_number}È°µ")

            source_text = " | ".join(source_info) if source_info else "ÊïôÊùêÂÜÖÂÆπ"

            formatted_chunk = f"„ÄêÊïôÊùêÁâáÊÆµ{i+1}„Äë{source_text}\n{result.content}\n"

            # Ê£ÄÊü•ÈïøÂ∫¶ÈôêÂà∂
            if current_length + len(formatted_chunk) > max_context_length:
                break

            context_parts.append(formatted_chunk)
            current_length += len(formatted_chunk)

        context = "\n".join(context_parts)

        # ÈÄâÊã©ÂêàÈÄÇÁöÑPromptÊ®°Êùø
        if context.strip():
            prompt = self.teacher_prompt_template.format(
                question=request.question,
                context=context
            )
        else:
            # Ê≤°ÊúâÊâæÂà∞Áõ∏ÂÖ≥ÂÜÖÂÆπ
            context = "Ê≤°ÊúâÊâæÂà∞‰∏éÈóÆÈ¢òÁõ¥Êé•Áõ∏ÂÖ≥ÁöÑÊïôÊùêÂÜÖÂÆπ„ÄÇ"
            prompt = self.no_context_prompt.format(question=request.question)

        return context, prompt

    async def _generate_answer(self,
                             prompt: str,
                             temperature: float = 0.7,
                             max_tokens: int = 800) -> str:
        """
        ‰ΩøÁî®LLMÁîüÊàêÁ≠îÊ°à

        Args:
            prompt: ÂÆåÊï¥ÁöÑPrompt
            temperature: ÁîüÊàêÊ∏©Â∫¶
            max_tokens: ÊúÄÂ§ßtokenÊï∞

        Returns:
            ÁîüÊàêÁöÑÁ≠îÊ°àÊñáÊú¨
        """
        try:
            # ÂáÜÂ§áÊ∂àÊÅØ
            messages = [
                {"role": "system", "content": "‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÂ∞èÂ≠¶ËØ≠ÊñáËÄÅÂ∏àÔºåÊìÖÈïøÁî®ÁÆÄÂçïÊòìÊáÇÁöÑËØ≠Ë®ÄÊïô‰∏âÂπ¥Á∫ßÂ≠¶Áîü„ÄÇ"},
                {"role": "user", "content": prompt}
            ]

            logger.debug(f"ÁîüÊàêÁ≠îÊ°àÔºåPromptÈïøÂ∫¶: {len(prompt)}")

            # Ë∞ÉÁî®LLM
            response = self.llm_client.llm_client.chat_completion(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )

            # ÊèêÂèñÂõûÂ§çÊñáÊú¨
            answer = self.llm_client.llm_client.get_response_text(response)

            # Ê∏ÖÁêÜÁ≠îÊ°àÊ†ºÂºè
            answer = self._clean_answer(answer)

            logger.debug(f"ÁîüÊàêÁ≠îÊ°àÊàêÂäüÔºåÈïøÂ∫¶: {len(answer)}")
            return answer

        except Exception as e:
            logger.error(f"Á≠îÊ°àÁîüÊàêÂ§±Ë¥•: {e}")
            # ËøîÂõûÈªòËÆ§Á≠îÊ°à
            return "Êä±Ê≠âÔºåËÄÅÂ∏àÁé∞Âú®ÈúÄË¶Å‰ºëÊÅØ‰∏Ä‰∏ãÔºåÊ≤°ËÉΩÂæàÂ•ΩÂú∞ÂõûÁ≠î‰Ω†ÁöÑÈóÆÈ¢ò„ÄÇ‰Ω†ÂèØ‰ª•ÊääËøô‰∏™ÈóÆÈ¢òËÆ∞‰∏ãÊù•ÔºåÊòéÂ§©ÈóÆÂ≠¶Ê†°ÁöÑËÄÅÂ∏àÂì¶ÔºÅÁªßÁª≠Âä™ÂäõÔºÅüåü"

    def _clean_answer(self, answer: str) -> str:
        """
        Ê∏ÖÁêÜÁ≠îÊ°àÊñáÊú¨

        Args:
            answer: ÂéüÂßãÁ≠îÊ°à

        Returns:
            Ê∏ÖÁêÜÂêéÁöÑÁ≠îÊ°à
        """
        # ÁßªÈô§ÂèØËÉΩÁöÑJSONÊ†ºÂºèÊ†áËÆ∞
        answer = answer.strip()
        if answer.startswith('```json'):
            answer = answer[7:]
        if answer.startswith('```'):
            answer = answer[3:]
        if answer.endswith('```'):
            answer = answer[:-3]

        # ÁßªÈô§ÂèØËÉΩÁöÑÂºïÂè∑
        answer = answer.strip('"\'')

        # Ê∏ÖÁêÜÂ§ö‰ΩôÁöÑÁ©∫Ë°å
        lines = answer.split('\n')
        cleaned_lines = [line.strip() for line in lines if line.strip()]
        answer = '\n'.join(cleaned_lines)

        return answer

    def get_service_status(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñÊúçÂä°Áä∂ÊÄÅ

        Returns:
            ÊúçÂä°Áä∂ÊÄÅ‰ø°ÊÅØ
        """
        try:
            # ÊµãËØïRAGÊúçÂä°
            rag_status = "connected"
            rag_error = None
            try:
                test_results = self.rag_service.search("ÊµãËØï", top_k=1)
                rag_status = "working" if len(test_results) >= 0 else "error"
            except Exception as e:
                rag_status = "error"
                rag_error = str(e)

            # ÊµãËØïLLMÊúçÂä°
            llm_status = "connected"
            llm_error = None
            try:
                llm_info = self.llm_client.llm_client.get_model_info()
                llm_status = "working"
            except Exception as e:
                llm_status = "error"
                llm_error = str(e)

            return {
                "status": "operational" if rag_status == "working" and llm_status == "working" else "degraded",
                "components": {
                    "rag_service": {
                        "status": rag_status,
                        "error": rag_error
                    },
                    "llm_service": {
                        "status": llm_status,
                        "error": llm_error,
                        "model_info": llm_info if 'llm_info' in locals() else None
                    }
                }
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }


# Âçï‰æãÂÆû‰æã
qa_service = QAService()