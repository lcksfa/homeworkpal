"""
æ–‡æ¡£åˆ†æ®µå™¨
Document Splitter for Homework Pal

é’ˆå¯¹PDFæ–‡æ¡£å†…å®¹çš„æ™ºèƒ½åˆ†æ®µç­–ç•¥
"""

import re
from typing import List, Dict, Any, Optional
from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging

logger = logging.getLogger(__name__)


class PDFTextSplitter:
    """PDFæ–‡æ¡£æ™ºèƒ½åˆ†æ®µå™¨"""

    def __init__(self,
                 chunk_size: int = 1500,
                 chunk_overlap: int = 200,
                 respect_sentence_endings: bool = True,
                 respect_paragraph_breaks: bool = True):
        """
        åˆå§‹åŒ–PDFæ–‡æ¡£åˆ†æ®µå™¨

        Args:
            chunk_size: åˆ†æ®µå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: åˆ†æ®µé‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            respect_sentence_endings: æ˜¯å¦å°Šé‡å¥å­ç»“å°¾
            respect_paragraph_breaks: æ˜¯å¦å°Šé‡æ®µè½åˆ†éš”
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.respect_sentence_endings = respect_sentence_endings
        self.respect_paragraph_breaks = respect_paragraph_breaks

        # åˆ›å»ºLangChainæ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=self._get_pdf_separators()
        )

    def _get_pdf_separators(self) -> List[str]:
        """è·å–PDFæ–‡æ¡£çš„åˆ†éš”ç¬¦åˆ—è¡¨"""
        separators = []

        if self.respect_paragraph_breaks:
            # æ®µè½åˆ†éš”ç¬¦ï¼ˆåŒæ¢è¡Œä»¥ä¸Šï¼‰
            separators.append("\n\n\n")

        if self.respect_sentence_endings:
            # å¥å­åˆ†éš”ç¬¦
            separators.extend([
                "ã€‚\n",    # å¥å·+æ¢è¡Œ
                "ï¼\n",    # æ„Ÿå¹å·+æ¢è¡Œ
                "ï¼Ÿ\n",    # é—®å·+æ¢è¡Œ
                "ï¼›\n",    # åˆ†å·+æ¢è¡Œ
                "ï¼š\n",    # å†’å·+æ¢è¡Œ
            ])

        # å¸¸è§„åˆ†éš”ç¬¦
        separators.extend([
            "\n",         # å•ä¸ªæ¢è¡Œ
            "ã€‚",         # å¥å·
            "ï¼",         # æ„Ÿå¹å·
            "ï¼Ÿ",         # é—®å·
            "ï¼›",         # åˆ†å·
            "ï¼š",         # å†’å·
            "ï¼Œ",         # é€—å·
            " ",          # ç©ºæ ¼
        ])

        return separators

    def split_pdf_content(self, pdf_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        åˆ†å‰²PDFæ–‡æ¡£å†…å®¹

        Args:
            pdf_result: PDFå¤„ç†ç»“æœ

        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹åˆ†å‰²PDFæ–‡æ¡£: {pdf_result['file_name']}")

            # è·å–PDFçš„é¡µé¢ä¿¡æ¯
            pages = pdf_result.get('pages', [])

            if not pages:
                logger.warning("PDFæ²¡æœ‰å¯å¤„ç†çš„é¡µé¢")
                return []

            chunks = []

            for page in pages:
                page_chunks = self._split_page_content(page, pdf_result)
                chunks.extend(page_chunks)

            logger.info(f"PDFåˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")
            return chunks

        except Exception as e:
            logger.error(f"åˆ†å‰²PDFå†…å®¹æ—¶å‡ºé”™: {e}")
            raise

    def _split_page_content(self, page: Dict[str, Any], pdf_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†å‰²å•ä¸ªé¡µé¢çš„å†…å®¹"""
        page_text = page.get('text', '')
        page_number = page.get('page_number', 0)

        if not page_text.strip():
            return []

        # é¢„å¤„ç†é¡µé¢æ–‡æœ¬
        cleaned_text = self._preprocess_page_text(page_text)

        # ä½¿ç”¨LangChainè¿›è¡Œæ–‡æœ¬åˆ†å‰²
        documents = self.text_splitter.create_documents([cleaned_text])

        chunks = []
        for i, doc in enumerate(documents):
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
            chunk_id = f"{pdf_result['file_name']}_page_{page_number}_chunk_{i+1}"

            # æ£€æŸ¥ç‰‡æ®µè´¨é‡
            text_quality = self._assess_text_quality(doc.page_content)

            if text_quality['is_meaningful']:
                chunk = {
                    'id': chunk_id,
                    'content': doc.page_content.strip(),
                    'page_number': page_number,
                    'chunk_index': i,
                    'total_chunks': len(documents),
                    'text_length': len(doc.page_content),
                    'word_count': len(doc.page_content.split()),
                    'quality_score': text_quality['score'],
                    'metadata': {
                        'pdf_file': pdf_result['file_name'],
                        'subject': pdf_result['education_metadata'].get('subject', 'æœªè¯†åˆ«'),
                        'grade': pdf_result['education_metadata'].get('grade', 'æœªè¯†åˆ«'),
                        'page_number': page_number,
                        'total_pages': pdf_result.get('total_pages', 0),
                        'processed_date': pdf_result.get('processed_date', ''),
                        'content_type': self._identify_content_type(doc.page_content),
                        'has_images': len(page.get('images', [])) > 0
                    }
                }

                chunks.append(chunk)

        return chunks

    def _preprocess_page_text(self, text: str) -> str:
        """é¢„å¤„ç†é¡µé¢æ–‡æœ¬"""
        if not text:
            return text

        # ç§»é™¤é¡µé¢å¤´éƒ¨å’Œå°¾éƒ¨çš„é¡µç ç­‰
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()

            # è·³è¿‡é¡µç è¡Œ
            if self._is_page_number_line(line):
                continue

            # è·³è¿‡ç©ºè¡Œï¼ˆä¿ç•™ç»“æ„ï¼‰
            if line or cleaned_lines:  # å¦‚æœæ˜¯ç¬¬ä¸€è¡Œæˆ–è€…å·²ç»æœ‰å†…å®¹ï¼Œä¿ç•™ç©ºè¡Œ
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    def _is_page_number_line(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé¡µç è¡Œ"""
        line = line.strip()

        # å¸¸è§çš„é¡µç æ¨¡å¼
        page_patterns = [
            r'^\d+$',                      # çº¯æ•°å­—
            r'^ç¬¬\d+é¡µ$',                   # ç¬¬Xé¡µ
            r'^Page\s*\d+$',               # Page X
            r'^\d+\s*/\s*\d+$',            # 1/50
            r'^-\s*\d+\s*-$',               # - 5 -
            r'^\[\s*\d+\s*\]$',              # [1]
            r'^\(\s*\d+\s*\)$',              # (1)
        ]

        for pattern in page_patterns:
            if re.match(pattern, line):
                return True

        return False

    def _assess_text_quality(self, text: str) -> Dict[str, Any]:
        """è¯„ä¼°æ–‡æœ¬è´¨é‡"""
        text = text.strip()

        if not text:
            return {'is_meaningful': False, 'score': 0.0}

        score = 1.0  # åŸºç¡€åˆ†æ•°

        # æ–‡æœ¬é•¿åº¦è¯„åˆ†ï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿éƒ½æ‰£åˆ†ï¼‰
        length = len(text)
        if length < 50:
            score -= 0.5
        elif length > 2000:
            score -= 0.3

        # åŒ…å«ä¸­æ–‡å†…å®¹çš„è¯„åˆ†
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        if chinese_chars == 0:
            score -= 0.5
        else:
            chinese_ratio = chinese_chars / length
            score += chinese_ratio * 0.3

        # æ•™è‚²å†…å®¹å…³é”®è¯è¯„åˆ†
        education_keywords = [
            'ç»ƒä¹ ', 'ä¾‹é¢˜', 'ç­”æ¡ˆ', 'çŸ¥è¯†ç‚¹', 'å­¦ä¹ ', 'æ€è€ƒ', 'è®¨è®º',
            'æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­', 'è¿ç®—', 'æ¦‚å¿µ', 'æ–¹æ³•', 'æŠ€å·§',
            'ä¹˜æ³•è¡¨', 'åŠ æ³•', 'å‡æ³•', 'åº”ç”¨é¢˜', 'ç»¼åˆé¢˜'
        ]

        keyword_count = sum(1 for keyword in education_keywords if keyword in text)
        score += min(keyword_count * 0.1, 0.5)

        # ç»“æ„åŒ–å†…å®¹è¯„åˆ†ï¼ˆåŒ…å«æ ‡é¢˜ã€åˆ—è¡¨ç­‰ï¼‰
        if re.search(r'^(ç¬¬.*[ï¼š:])', text):
            score += 0.2
        if re.search(r'^\d+[[ã€.]', text):
            score += 0.2
        if re.search(r'^\*|^-', text):
            score += 0.1

        # æœ€ç»ˆè¯„åˆ†
        score = max(0.0, min(1.0, score))

        return {
            'is_meaningful': score > 0.3,  # æœ€ä½0.3åˆ†æ‰è®¤ä¸ºæœ‰æ„ä¹‰
            'score': score,
            'length': length,
            'chinese_chars': chinese_chars,
            'keyword_count': keyword_count
        }

    def _identify_content_type(self, text: str) -> str:
        """è¯†åˆ«å†…å®¹ç±»å‹"""
        text_lower = text.lower()

        # æ•™å­¦å†…å®¹ç±»å‹
        if re.search(r'ä¾‹é¢˜|ç»ƒä¹ |æµ‹è¯•|ä½œä¸š|è€ƒè¯•', text_lower):
            return 'ç»ƒä¹ é¢˜'
        elif re.search(r'æ¦‚å¿µ|å®šä¹‰|è§£é‡Š|è¯´æ˜', text_lower):
            return 'æ¦‚å¿µè®²è§£'
        elif re.search(r'æ­¥éª¤|è¿‡ç¨‹|æ–¹æ³•', text_lower):
            return 'æ–¹æ³•æ­¥éª¤'
        elif re.search(r'å…¬å¼|å®šç†|å®šå¾‹', text_lower):
            return 'å…¬å¼å®šç†'
        elif re.search(r'å›¾ç‰‡|æ’å›¾|å›¾è¡¨', text_lower):
            return 'å›¾ç¤ºè¯´æ˜'
        elif re.search(r'æ€»ç»“|å°ç»“|å›é¡¾', text_lower):
            return 'æ€»ç»“å¤ä¹ '
        else:
            return 'æ­£æ–‡å†…å®¹'

    def get_splitting_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†æ®µç»Ÿè®¡ä¿¡æ¯"""
        return {
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'respect_sentence_endings': self.respect_sentence_endings,
            'respect_paragraph_breaks': self.respect_paragraph_breaks,
            'separators_count': len(self._get_pdf_separators())
        }


def create_pdf_splitter(chunk_size: int = 1500,
                      chunk_overlap: int = 200) -> PDFTextSplitter:
    """åˆ›å»ºPDFåˆ†æ®µå™¨çš„å·¥å‚å‡½æ•°"""
    return PDFTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        respect_sentence_endings=True,
        respect_paragraph_breaks=True
    )


if __name__ == "__main__":
    # æµ‹è¯•PDFåˆ†æ®µå™¨
    from homeworkpal.document.pdf_processor import create_pdf_processor
    import os

    print("ğŸ”§ æµ‹è¯•PDFæ–‡æ¡£åˆ†æ®µå™¨")
    print("=" * 40)

    # åˆ›å»ºå¤„ç†å™¨
    processor = create_pdf_processor()
    splitter = create_pdf_splitter()

    # æµ‹è¯•æ•°å­¦PDF
    math_pdf = 'data/textbooks/æ•°å­¦ 3 ä¸Š.pdf'
    if not os.path.exists(math_pdf):
        print(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {math_pdf}")
        exit(1)

    print(f"æ­£åœ¨æµ‹è¯•: {math_pdf}")

    # å¤„ç†PDF
    try:
        pdf_result = processor.extract_text_from_pdf(math_pdf)

        # åˆ†å‰²å†…å®¹
        chunks = splitter.split_pdf_content(pdf_result)

        print(f"âœ… PDFåˆ†å‰²æˆåŠŸ")
        print(f"  - æ€»ç‰‡æ®µæ•°: {len(chunks)}")
        print(f"  - å¹³å‡ç‰‡æ®µé•¿åº¦: {sum(c['text_length'] for c in chunks) / len(chunks):.1f}")
        print(f"  - é«˜è´¨é‡ç‰‡æ®µæ•°: {sum(1 for c in chunks if c['quality_score'] > 0.5)}")

        # æ˜¾ç¤ºå‰3ä¸ªç‰‡æ®µçš„é¢„è§ˆ
        for i, chunk in enumerate(chunks[:3]):
            print(f"\n--- ç‰‡æ®µ {i+1} é¢„è§ˆ ---")
            print(f"ID: {chunk['id']}")
            print(f"é¡µé¢: {chunk['page_number']}")
            print(f"é•¿åº¦: {chunk['text_length']} å­—ç¬¦")
            print(f"è´¨é‡è¯„åˆ†: {chunk['quality_score']:.2f}")
            preview = chunk['content'][:200] + '...' if len(chunk['content']) > 200 else chunk['content']
            print(f"å†…å®¹: {preview}")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")