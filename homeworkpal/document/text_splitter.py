"""
æ–‡æ¡£åˆ†æ®µå™¨
Document Splitter for Homework Pal

é’ˆå¯¹PDFæ–‡æ¡£å†…å®¹çš„æ™ºèƒ½åˆ†æ®µç­–ç•¥
"""

import re
from typing import List, Dict, Any, Optional
# ç®€åŒ–çš„Documentç±»
class Document:
    """ç®€åŒ–çš„æ–‡æ¡£ç±»ï¼Œæ›¿ä»£LangChainçš„Document"""
    def __init__(self, page_content: str, metadata: Dict[str, Any] = None):
        self.page_content = page_content
        self.metadata = metadata or {}

# ç®€åŒ–çš„æ–‡æœ¬åˆ†å‰²å™¨
class SimpleRecursiveTextSplitter:
    """ç®€åŒ–çš„é€’å½’æ–‡æœ¬åˆ†å‰²å™¨"""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200,
                 length_function=len, separators: List[str] = None):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.length_function = length_function
        self.separators = separators or ["\n\n", "\n", " ", ""]

    def create_documents(self, texts: List[str]) -> List[Document]:
        """åˆ›å»ºæ–‡æ¡£åˆ—è¡¨"""
        documents = []
        for text in texts:
            chunks = self._split_text(text)
            for chunk in chunks:
                documents.append(Document(page_content=chunk))
        return documents

    def _split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬"""
        if self.length_function(text) <= self.chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            # æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹
            end = start + self.chunk_size
            if end > len(text):
                end = len(text)

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ®µï¼Œå°è¯•åœ¨åˆ†éš”ç¬¦å¤„åˆ†å‰²
            if end < len(text):
                best_split = end
                for separator in self.separators:
                    split_pos = text.rfind(separator, start, end)
                    if split_pos > start:
                        best_split = split_pos + len(separator)
                        break

                end = best_split

            chunk = text[start:end]
            if chunk.strip():
                chunks.append(chunk.strip())

            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = max(start + 1, end - self.chunk_overlap)

        return chunks
import logging

logger = logging.getLogger(__name__)


class PDFTextSplitter:
    """PDFæ–‡æ¡£æ™ºèƒ½åˆ†æ®µå™¨"""

    def __init__(self,
                 chunk_size: int = 1500,
                 chunk_overlap: int = 200,
                 respect_sentence_endings: bool = True,
                 respect_paragraph_breaks: bool = True):
        """
        åˆå§‹åŒ–PDFæ–‡æ¡£åˆ†æ®µå™¨

        Args:
            chunk_size: åˆ†æ®µå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: åˆ†æ®µé‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            respect_sentence_endings: æ˜¯å¦å°Šé‡å¥å­ç»“å°¾
            respect_paragraph_breaks: æ˜¯å¦å°Šé‡æ®µè½åˆ†éš”
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.respect_sentence_endings = respect_sentence_endings
        self.respect_paragraph_breaks = respect_paragraph_breaks

        # åˆ›å»ºç®€åŒ–çš„æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = SimpleRecursiveTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=self._get_pdf_separators()
        )

    def _get_pdf_separators(self) -> List[str]:
        """è·å–PDFæ–‡æ¡£çš„åˆ†éš”ç¬¦åˆ—è¡¨"""
        separators = []

        if self.respect_paragraph_breaks:
            # æ®µè½åˆ†éš”ç¬¦ï¼ˆåŒæ¢è¡Œä»¥ä¸Šï¼‰
            separators.append("\n\n\n")

        if self.respect_sentence_endings:
            # å¥å­åˆ†éš”ç¬¦
            separators.extend([
                "ã€‚\n",    # å¥å·+æ¢è¡Œ
                "ï¼\n",    # æ„Ÿå¹å·+æ¢è¡Œ
                "ï¼Ÿ\n",    # é—®å·+æ¢è¡Œ
                "ï¼›\n",    # åˆ†å·+æ¢è¡Œ
                "ï¼š\n",    # å†’å·+æ¢è¡Œ
            ])

        # å¸¸è§„åˆ†éš”ç¬¦
        separators.extend([
            "\n",         # å•ä¸ªæ¢è¡Œ
            "ã€‚",         # å¥å·
            "ï¼",         # æ„Ÿå¹å·
            "ï¼Ÿ",         # é—®å·
            "ï¼›",         # åˆ†å·
            "ï¼š",         # å†’å·
            "ï¼Œ",         # é€—å·
            " ",          # ç©ºæ ¼
        ])

        return separators

    def split_pdf_content(self, pdf_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        åˆ†å‰²PDFæ–‡æ¡£å†…å®¹

        Args:
            pdf_result: PDFå¤„ç†ç»“æœ

        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹åˆ†å‰²PDFæ–‡æ¡£: {pdf_result['file_name']}")

            # è·å–PDFçš„é¡µé¢ä¿¡æ¯
            pages = pdf_result.get('pages', [])

            if not pages:
                logger.warning("PDFæ²¡æœ‰å¯å¤„ç†çš„é¡µé¢")
                return []

            chunks = []

            for page in pages:
                page_chunks = self._split_page_content(page, pdf_result)
                chunks.extend(page_chunks)

            logger.info(f"PDFåˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")
            return chunks

        except Exception as e:
            logger.error(f"åˆ†å‰²PDFå†…å®¹æ—¶å‡ºé”™: {e}")
            raise

    def _split_page_content(self, page: Dict[str, Any], pdf_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†å‰²å•ä¸ªé¡µé¢çš„å†…å®¹"""
        page_text = page.get('text', '')
        page_number = page.get('page_number', 0)

        if not page_text.strip():
            return []

        # é¢„å¤„ç†é¡µé¢æ–‡æœ¬
        cleaned_text = self._preprocess_page_text(page_text)

        # ä½¿ç”¨LangChainè¿›è¡Œæ–‡æœ¬åˆ†å‰²
        documents = self.text_splitter.create_documents([cleaned_text])

        chunks = []
        for i, doc in enumerate(documents):
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
            chunk_id = f"{pdf_result['file_name']}_page_{page_number}_chunk_{i+1}"

            # æ£€æŸ¥ç‰‡æ®µè´¨é‡
            text_quality = self._assess_text_quality(doc.page_content)

            if text_quality['is_meaningful']:
                chunk = {
                    'id': chunk_id,
                    'content': doc.page_content.strip(),
                    'page_number': page_number,
                    'chunk_index': i,
                    'total_chunks': len(documents),
                    'text_length': len(doc.page_content),
                    'word_count': len(doc.page_content.split()),
                    'quality_score': text_quality['score'],
                    'content_type': self._identify_content_type(doc.page_content),
                    'metadata': {
                        'pdf_file': pdf_result['file_name'],
                        'subject': pdf_result['education_metadata'].get('subject', 'æœªè¯†åˆ«'),
                        'grade': pdf_result['education_metadata'].get('grade', 'æœªè¯†åˆ«'),
                        'page_number': page_number,
                        'total_pages': pdf_result.get('total_pages', 0),
                        'processed_date': pdf_result.get('processed_date', ''),
                        'content_type': self._identify_content_type(doc.page_content),
                        'has_images': len(page.get('images', [])) > 0
                    }
                }

                chunks.append(chunk)

        return chunks

    def _preprocess_page_text(self, text: str) -> str:
        """é¢„å¤„ç†é¡µé¢æ–‡æœ¬"""
        if not text:
            return text

        # ç§»é™¤é¡µé¢å¤´éƒ¨å’Œå°¾éƒ¨çš„é¡µç ç­‰
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()

            # è·³è¿‡é¡µç è¡Œ
            if self._is_page_number_line(line):
                continue

            # è·³è¿‡ç©ºè¡Œï¼ˆä¿ç•™ç»“æ„ï¼‰
            if line or cleaned_lines:  # å¦‚æœæ˜¯ç¬¬ä¸€è¡Œæˆ–è€…å·²ç»æœ‰å†…å®¹ï¼Œä¿ç•™ç©ºè¡Œ
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    def _is_page_number_line(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé¡µç è¡Œ"""
        line = line.strip()

        # å¸¸è§çš„é¡µç æ¨¡å¼
        page_patterns = [
            r'^\d+$',                      # çº¯æ•°å­—
            r'^ç¬¬\d+é¡µ$',                   # ç¬¬Xé¡µ
            r'^Page\s*\d+$',               # Page X
            r'^\d+\s*/\s*\d+$',            # 1/50
            r'^-\s*\d+\s*-$',               # - 5 -
            r'^\[\s*\d+\s*\]$',              # [1]
            r'^\(\s*\d+\s*\)$',              # (1)
        ]

        for pattern in page_patterns:
            if re.match(pattern, line):
                return True

        return False

    def _assess_text_quality(self, text: str) -> Dict[str, Any]:
        """è¯„ä¼°æ–‡æœ¬è´¨é‡"""
        text = text.strip()

        if not text:
            return {'is_meaningful': False, 'score': 0.0}

        score = 1.0  # åŸºç¡€åˆ†æ•°

        # æ–‡æœ¬é•¿åº¦è¯„åˆ†ï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿éƒ½æ‰£åˆ†ï¼‰
        length = len(text)
        if length < 50:
            score -= 0.5
        elif length > 2000:
            score -= 0.3

        # åŒ…å«ä¸­æ–‡å†…å®¹çš„è¯„åˆ†
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        if chinese_chars == 0:
            score -= 0.5
        else:
            chinese_ratio = chinese_chars / length
            score += chinese_ratio * 0.3

        # æ•™è‚²å†…å®¹å…³é”®è¯è¯„åˆ†
        education_keywords = [
            'ç»ƒä¹ ', 'ä¾‹é¢˜', 'ç­”æ¡ˆ', 'çŸ¥è¯†ç‚¹', 'å­¦ä¹ ', 'æ€è€ƒ', 'è®¨è®º',
            'æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­', 'è¿ç®—', 'æ¦‚å¿µ', 'æ–¹æ³•', 'æŠ€å·§',
            'ä¹˜æ³•è¡¨', 'åŠ æ³•', 'å‡æ³•', 'åº”ç”¨é¢˜', 'ç»¼åˆé¢˜'
        ]

        keyword_count = sum(1 for keyword in education_keywords if keyword in text)
        score += min(keyword_count * 0.1, 0.5)

        # ç»“æ„åŒ–å†…å®¹è¯„åˆ†ï¼ˆåŒ…å«æ ‡é¢˜ã€åˆ—è¡¨ç­‰ï¼‰
        if re.search(r'^(ç¬¬.*[ï¼š:])', text):
            score += 0.2
        if re.search(r'^\d+[ã€.]', text):
            score += 0.2
        if re.search(r'^\*|^-', text):
            score += 0.1

        # æœ€ç»ˆè¯„åˆ†
        score = max(0.0, min(1.0, score))

        return {
            'is_meaningful': score > 0.3,  # æœ€ä½0.3åˆ†æ‰è®¤ä¸ºæœ‰æ„ä¹‰
            'score': score,
            'length': length,
            'chinese_chars': chinese_chars,
            'keyword_count': keyword_count
        }

    def _identify_content_type(self, text: str) -> str:
        """è¯†åˆ«å†…å®¹ç±»å‹"""
        text_lower = text.lower()

        # æ•™å­¦å†…å®¹ç±»å‹
        if re.search(r'ä¾‹é¢˜|ç»ƒä¹ |æµ‹è¯•|ä½œä¸š|è€ƒè¯•', text_lower):
            return 'ç»ƒä¹ é¢˜'
        elif re.search(r'æ¦‚å¿µ|å®šä¹‰|è§£é‡Š|è¯´æ˜', text_lower):
            return 'æ¦‚å¿µè®²è§£'
        elif re.search(r'æ­¥éª¤|è¿‡ç¨‹|æ–¹æ³•', text_lower):
            return 'æ–¹æ³•æ­¥éª¤'
        elif re.search(r'å…¬å¼|å®šç†|å®šå¾‹', text_lower):
            return 'å…¬å¼å®šç†'
        elif re.search(r'å›¾ç‰‡|æ’å›¾|å›¾è¡¨', text_lower):
            return 'å›¾ç¤ºè¯´æ˜'
        elif re.search(r'æ€»ç»“|å°ç»“|å›é¡¾', text_lower):
            return 'æ€»ç»“å¤ä¹ '
        else:
            return 'æ­£æ–‡å†…å®¹'

    def get_splitting_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†æ®µç»Ÿè®¡ä¿¡æ¯"""
        return {
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'respect_sentence_endings': self.respect_sentence_endings,
            'respect_paragraph_breaks': self.respect_paragraph_breaks,
            'separators_count': len(self._get_pdf_separators())
        }


class ChineseTextbookSplitter(PDFTextSplitter):
    """è¯­æ–‡æ•™æä¸“ç”¨æ–‡æœ¬åˆ†å‰²å™¨"""

    def __init__(self,
                 chunk_size: int = 1200,          # è¯­æ–‡å†…å®¹ç¨å°ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
                 chunk_overlap: int = 150,
                 respect_sentence_endings: bool = True,
                 respect_paragraph_breaks: bool = True):
        """
        åˆå§‹åŒ–è¯­æ–‡æ•™ææ–‡æœ¬åˆ†å‰²å™¨

        Args:
            chunk_size: åˆ†æ®µå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: åˆ†æ®µé‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            respect_sentence_endings: æ˜¯å¦å°Šé‡å¥å­ç»“å°¾
            respect_paragraph_breaks: æ˜¯å¦å°Šé‡æ®µè½åˆ†éš”
        """
        super().__init__(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            respect_sentence_endings=respect_sentence_endings,
            respect_paragraph_breaks=respect_paragraph_breaks
        )

        # åˆ›å»ºè¯­æ–‡æ•™æä¸“ç”¨çš„æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = SimpleRecursiveTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=self._get_chinese_separators()
        )

    def _get_chinese_separators(self) -> List[str]:
        """è·å–è¯­æ–‡æ•™æä¸“ç”¨åˆ†éš”ç¬¦åˆ—è¡¨"""
        separators = []

        # è¯­æ–‡æ•™æç»“æ„åˆ†éš”ç¬¦ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
        separators.extend([
            r'\nç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+è¯¾',        # è¯¾æ–‡åˆ†éš”
            r'\nç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+å•å…ƒ',      # å•å…ƒåˆ†éš”
            r'\n\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\s*ã€',     # ç¼–å·åˆ—è¡¨
            r'\n\s*ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ï¼‰',     # æ‹¬å·ç¼–å·
            r'\n\s*[1-9]\d*\s*[ã€\.]',             # æ•°å­—ç¼–å·
            r'\n\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',             # åœ†åœˆæ•°å­—
            r'\n\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',               # åœ†åœˆæ•°å­—ï¼ˆå¦ä¸€ç§æ ¼å¼ï¼‰
            r'\n\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',               # åœ†åœˆæ•°å­—ï¼ˆé‡å¤ç¡®ä¿åŒ¹é…ï¼‰
            r'\n\s*[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',               # åœ†åœˆæ•°å­—ï¼ˆæœ€åä¸€æ¬¡ï¼‰
        ])

        # æ®µè½åˆ†éš”ç¬¦
        if self.respect_paragraph_breaks:
            separators.append("\n\n\n")
            separators.append("\n\n")

        # å¥å­åˆ†éš”ç¬¦ï¼ˆè¯­æ–‡æ•™æå¸¸ç”¨ï¼‰
        if self.respect_sentence_endings:
            separators.extend([
                "ã€‚\n",    # å¥å·+æ¢è¡Œ
                "ï¼\n",    # æ„Ÿå¹å·+æ¢è¡Œ
                "ï¼Ÿ\n",    # é—®å·+æ¢è¡Œ
                "ï¼›\n",    # åˆ†å·+æ¢è¡Œ
                "ï¼š\n",    # å†’å·+æ¢è¡Œ
            ])

        # æ ‡ç‚¹ç¬¦å·åˆ†éš”ç¬¦
        separators.extend([
            "ã€‚",         # å¥å·
            "ï¼",         # æ„Ÿå¹å·
            "ï¼Ÿ",         # é—®å·
            "ï¼›",         # åˆ†å·
            "ï¼š",         # å†’å·
            "ï¼Œ",         # é€—å·
            "ã€",         # é¡¿å·
            "\n",         # å•ä¸ªæ¢è¡Œ
            " ",          # ç©ºæ ¼
        ])

        return separators

    def _identify_chinese_content_type(self, text: str, structure_info: Dict[str, Any] = None) -> str:
        """
        è¯†åˆ«è¯­æ–‡æ•™æå†…å®¹ç±»å‹

        Args:
            text: æ–‡æœ¬å†…å®¹
            structure_info: ç»“æ„è¯†åˆ«ä¿¡æ¯

        Returns:
            å†…å®¹ç±»å‹å­—ç¬¦ä¸²
        """
        # å¦‚æœæœ‰ç»“æ„ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨
        if structure_info and structure_info.get('content_type') != 'æœªè¯†åˆ«':
            return structure_info['content_type']

        text_lower = text.lower()

        # è¯­æ–‡æ•™æç‰¹æœ‰å†…å®¹ç±»å‹
        if re.search(r'ç”Ÿå­—\s*è¯|ç”Ÿå­—\s*è¡¨|è¯è¯­\s*ç›˜ç‚¹|æ‹¼éŸ³ä¹å›­', text_lower):
            return 'ç”Ÿå­—è¯'
        elif re.search(r'è¯¾åç»ƒä¹ |ç»ƒä¹ \s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+|åŸºç¡€\s*ç»ƒä¹ ', text_lower):
            return 'ç»ƒä¹ é¢˜'
        elif re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+è¯¾.*[ã€Š].*[ã€‹]', text):
            return 'è¯¾æ–‡'
        elif re.search(r'å•å…ƒ\s*å¤ä¹ |è¯­æ–‡\s*å›­åœ°|å£è¯­\s*äº¤é™…', text_lower):
            return 'å•å…ƒå¤ä¹ '
        elif re.search(r'ä¹ ä½œ|å†™ä½œ|çœ‹å›¾\s*å†™è¯', text_lower):
            return 'å†™ä½œæŒ‡å¯¼'
        elif re.search(r'å¤è¯—|æ—¥ç§¯æœˆç´¯|å¤è¯—è¯', text_lower):
            return 'å¤è¯—è¯'
        elif re.search(r'è¯†å­—|å†™å­—', text_lower):
            return 'è¯†å­—'
        elif re.search(r'é˜…è¯»\s*æç¤º|ç²¾è¯»\s*æŒ‡å¯¼', text_lower):
            return 'é˜…è¯»æŒ‡å¯¼'
        elif re.search(r'æ‹¼éŸ³|å£°æ¯|éŸµæ¯', text_lower):
            return 'æ‹¼éŸ³å­¦ä¹ '
        elif re.search(r'è¯è¯­\s*è§£é‡Š|è¿‘ä¹‰è¯|åä¹‰è¯', text_lower):
            return 'è¯è¯­å­¦ä¹ '
        elif re.search(r'å¥å­\s*ç»ƒä¹ |å¥å¼|é€ å¥', text_lower):
            return 'å¥å­ç»ƒä¹ '
        else:
            return 'è¯­æ–‡å†…å®¹'

    def _assess_chinese_text_quality(self, text: str) -> Dict[str, Any]:
        """
        è¯„ä¼°è¯­æ–‡æ•™ææ–‡æœ¬è´¨é‡

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            è´¨é‡è¯„ä¼°ç»“æœ
        """
        text = text.strip()

        if not text:
            return {'is_meaningful': False, 'score': 0.0}

        score = 1.0  # åŸºç¡€åˆ†æ•°

        # æ–‡æœ¬é•¿åº¦è¯„åˆ†ï¼ˆè¯­æ–‡å†…å®¹ç‰¹ç‚¹ï¼‰
        length = len(text)
        if length < 30:
            score -= 0.6
        elif length > 1500:
            score -= 0.2

        # ä¸­æ–‡å†…å®¹è¯„åˆ†
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        if chinese_chars == 0:
            score -= 0.8
        else:
            chinese_ratio = chinese_chars / length
            score += chinese_ratio * 0.4

        # è¯­æ–‡æ•™æå…³é”®è¯è¯„åˆ†
        chinese_keywords = [
            'è¯¾æ–‡', 'ç”Ÿå­—', 'è¯è¯­', 'ç»ƒä¹ ', 'é˜…è¯»', 'å†™ä½œ', 'å£è¯­', 'äº¤é™…',
            'æ‹¼éŸ³', 'è¯†å­—', 'å†™å­—', 'å¤è¯—', 'æ—¥ç§¯æœˆç´¯', 'è¯­æ–‡å›­åœ°',
            'è¿‘ä¹‰è¯', 'åä¹‰è¯', 'é€ å¥', 'ç†è§£', 'èƒŒè¯µ', 'æœ—è¯»',
            'ç¬¬è¯¾', 'å•å…ƒ', 'å¤ä¹ ', 'ä¹ ä½œ', 'çœ‹å›¾å†™è¯'
        ]

        keyword_count = sum(1 for keyword in chinese_keywords if keyword in text)
        score += min(keyword_count * 0.08, 0.6)

        # æ•™è‚²ç»“æ„è¯„åˆ†
        if re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+è¯¾', text):
            score += 0.3
        if re.search(r'^\d+[ã€.]', text, re.MULTILINE):
            score += 0.2
        if re.search(r'[ã€Š].*[ã€‹]', text):  # ä¹¦åå·
            score += 0.2

        # å†…å®¹ç±»å‹è¯„åˆ†
        content_type = self._identify_chinese_content_type(text)
        if content_type in ['è¯¾æ–‡', 'å¤è¯—', 'ç”Ÿå­—è¯', 'ç»ƒä¹ é¢˜']:
            score += 0.2
        elif content_type in ['å†™ä½œæŒ‡å¯¼', 'é˜…è¯»æŒ‡å¯¼', 'å•å…ƒå¤ä¹ ']:
            score += 0.1

        # æ’é™¤å™ªéŸ³å†…å®¹
        noise_patterns = [
            r'^\d+$',                      # çº¯æ•°å­—
            r'^ç¬¬\d+é¡µ$',                   # é¡µç 
            r'^page\s*\d+$',               # è‹±æ–‡é¡µç 
            r'^-*$',                       # çº¯æ¨ªçº¿
            r'^\s*$',                      # ç©ºç™½
            r'^[^\u4e00-\u9fff]*$',        # æ— ä¸­æ–‡
        ]

        for pattern in noise_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                score -= 0.5

        # æœ€ç»ˆè¯„åˆ†
        score = max(0.0, min(1.0, score))

        return {
            'is_meaningful': score > 0.4,  # è¯­æ–‡å†…å®¹é˜ˆå€¼ç¨é«˜
            'score': score,
            'length': length,
            'chinese_chars': chinese_chars,
            'chinese_ratio': chinese_chars / length if length > 0 else 0,
            'keyword_count': keyword_count,
            'content_type': content_type
        }

    def split_page_content(self, page: Dict[str, Any], pdf_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        åˆ†å‰²è¯­æ–‡æ•™æå•ä¸ªé¡µé¢çš„å†…å®¹

        Args:
            page: é¡µé¢ä¿¡æ¯
            pdf_result: PDFå¤„ç†ç»“æœ

        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        page_text = page.get('text', '')
        page_number = page.get('page_number', 0)
        structure_info = page.get('structure_info', {})

        if not page_text.strip():
            return []

        # é¢„å¤„ç†é¡µé¢æ–‡æœ¬
        cleaned_text = self._preprocess_page_text(page_text)

        # ä½¿ç”¨è¯­æ–‡æ•™æä¸“ç”¨åˆ†å‰²å™¨
        documents = self.text_splitter.create_documents([cleaned_text])

        chunks = []
        for i, doc in enumerate(documents):
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
            chunk_id = f"{pdf_result['file_name']}_page_{page_number}_chunk_{i+1}"

            # è¯­æ–‡æ•™æä¸“ç”¨è´¨é‡è¯„ä¼°
            text_quality = self._assess_chinese_text_quality(doc.page_content)

            if text_quality['is_meaningful']:
                # è¯†åˆ«å†…å®¹ç±»å‹
                content_type = self._identify_chinese_content_type(doc.page_content, structure_info)

                chunk = {
                    'id': chunk_id,
                    'content': doc.page_content.strip(),
                    'page_number': page_number,
                    'chunk_index': i,
                    'total_chunks': len(documents),
                    'text_length': len(doc.page_content),
                    'word_count': len(doc.page_content.split()),
                    'quality_score': text_quality['score'],
                    'content_type': content_type,
                    'metadata': {
                        'pdf_file': pdf_result['file_name'],
                        'subject': pdf_result['education_metadata'].get('subject', 'è¯­æ–‡'),
                        'grade': pdf_result['education_metadata'].get('grade', 'ä¸‰å¹´çº§'),
                        'page_number': page_number,
                        'total_pages': pdf_result.get('total_pages', 0),
                        'processed_date': pdf_result.get('processed_date', ''),
                        'content_type': content_type,
                        'has_images': len(page.get('images', [])) > 0,
                        'language_focus': structure_info.get('language_focus', 'ç»¼åˆå­¦ä¹ '),
                        'difficulty_level': structure_info.get('difficulty_level', 1),
                        'section_type': structure_info.get('section_type', content_type),
                        'chinese_ratio': text_quality.get('chinese_ratio', 0),
                        'is_lesson_content': content_type in ['è¯¾æ–‡', 'å¤è¯—', 'é˜…è¯»æŒ‡å¯¼'],
                        'is_exercise': content_type in ['ç»ƒä¹ é¢˜', 'é€ å¥', 'å¥å­ç»ƒä¹ '],
                        'is_vocabulary': content_type in ['ç”Ÿå­—è¯', 'è¯è¯­å­¦ä¹ '],
                        'is_writing': content_type in ['å†™ä½œæŒ‡å¯¼', 'çœ‹å›¾å†™è¯']
                    }
                }

                chunks.append(chunk)

        return chunks


def create_pdf_splitter(chunk_size: int = 1500,
                      chunk_overlap: int = 200,
                      subject: str = None) -> PDFTextSplitter:
    """
    åˆ›å»ºPDFåˆ†æ®µå™¨çš„å·¥å‚å‡½æ•°

    Args:
        chunk_size: åˆ†æ®µå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        chunk_overlap: åˆ†æ®µé‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        subject: å­¦ç§‘ç±»å‹ï¼Œå¦‚æœæ˜¯'è¯­æ–‡'åˆ™ä½¿ç”¨è¯­æ–‡æ•™æä¸“ç”¨åˆ†å‰²å™¨

    Returns:
        PDFåˆ†å‰²å™¨å®ä¾‹
    """
    if subject == 'è¯­æ–‡':
        return ChineseTextbookSplitter(
            chunk_size=1200,    # è¯­æ–‡å†…å®¹ä½¿ç”¨è¾ƒå°ç‰‡æ®µ
            chunk_overlap=150,
            respect_sentence_endings=True,
            respect_paragraph_breaks=True
        )
    else:
        return PDFTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            respect_sentence_endings=True,
            respect_paragraph_breaks=True
        )


if __name__ == "__main__":
    # æµ‹è¯•PDFåˆ†æ®µå™¨
    from homeworkpal.document.pdf_processor import create_pdf_processor
    import os

    print("ğŸ”§ æµ‹è¯•PDFæ–‡æ¡£åˆ†æ®µå™¨")
    print("=" * 40)

    # åˆ›å»ºå¤„ç†å™¨
    processor = create_pdf_processor()
    splitter = create_pdf_splitter()

    # æµ‹è¯•æ•°å­¦PDF
    math_pdf = 'data/textbooks/æ•°å­¦ 3 ä¸Š.pdf'
    if not os.path.exists(math_pdf):
        print(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {math_pdf}")
        exit(1)

    print(f"æ­£åœ¨æµ‹è¯•: {math_pdf}")

    # å¤„ç†PDF
    try:
        pdf_result = processor.extract_text_from_pdf(math_pdf)

        # åˆ†å‰²å†…å®¹
        chunks = splitter.split_pdf_content(pdf_result)

        print(f"âœ… PDFåˆ†å‰²æˆåŠŸ")
        print(f"  - æ€»ç‰‡æ®µæ•°: {len(chunks)}")
        print(f"  - å¹³å‡ç‰‡æ®µé•¿åº¦: {sum(c['text_length'] for c in chunks) / len(chunks):.1f}")
        print(f"  - é«˜è´¨é‡ç‰‡æ®µæ•°: {sum(1 for c in chunks if c['quality_score'] > 0.5)}")

        # æ˜¾ç¤ºå‰3ä¸ªç‰‡æ®µçš„é¢„è§ˆ
        for i, chunk in enumerate(chunks[:3]):
            print(f"\n--- ç‰‡æ®µ {i+1} é¢„è§ˆ ---")
            print(f"ID: {chunk['id']}")
            print(f"é¡µé¢: {chunk['page_number']}")
            print(f"é•¿åº¦: {chunk['text_length']} å­—ç¬¦")
            print(f"è´¨é‡è¯„åˆ†: {chunk['quality_score']:.2f}")
            preview = chunk['content'][:200] + '...' if len(chunk['content']) > 200 else chunk['content']
            print(f"å†…å®¹: {preview}")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")